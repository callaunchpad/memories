{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xlrd\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from numba import decorators\n",
    "import librosa\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import Dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read in the data file\n",
    "# Give the location of the file \n",
    "\n",
    "df = pd.read_excel(r'data.xlsx', sheet_name='reduced totals')\n",
    "# print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "## LOADING IN DATASETS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = Path.cwd().joinpath(\"SongEmotionDataset\")\n",
    "\n",
    "train_song = []\n",
    "test_song = []\n",
    "train_emotion = []\n",
    "test_emotion = []\n",
    "\n",
    "for i in range(1, 401):\n",
    "    if i % 5 == 0:\n",
    "        test_song.append(dataset.joinpath(\"{}.mp3\".format(i)))\n",
    "        emotion_arr = []\n",
    "        for j in range(5):\n",
    "            emotion_arr.append(sheet.cell_value(i, 2 + j))\n",
    "        test_emotion.append(torch.tensor(emotion_arr, device=device).float())\n",
    "    else:\n",
    "        train_song.append(dataset.joinpath(\"{}.mp3\".format(i)))\n",
    "        emotion_arr = []\n",
    "        for j in range(5):\n",
    "            emotion_arr.append(sheet.cell_value(i, 2 + j))\n",
    "        train_emotion.append(torch.tensor(emotion_arr))\n",
    "\n",
    "print(len(train_song), len(test_song))\n",
    "print(len(train_emotion), len(test_emotion))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SongEmotionDataset/249.mp3\n",
      "SongEmotionDataset/37.mp3\n",
      "SongEmotionDataset/120.mp3\n",
      "SongEmotionDataset/221.mp3\n",
      "SongEmotionDataset/347.mp3\n",
      "SongEmotionDataset/247.mp3\n",
      "SongEmotionDataset/183.mp3\n",
      "SongEmotionDataset/214.mp3\n",
      "SongEmotionDataset/45.mp3\n",
      "SongEmotionDataset/390.mp3\n",
      "SongEmotionDataset/275.mp3\n",
      "SongEmotionDataset/121.mp3\n",
      "SongEmotionDataset/262.mp3\n",
      "SongEmotionDataset/182.mp3\n",
      "SongEmotionDataset/386.mp3\n",
      "SongEmotionDataset/235.mp3\n",
      "SongEmotionDataset/126.mp3\n",
      "SongEmotionDataset/372.mp3\n",
      "SongEmotionDataset/35.mp3\n",
      "SongEmotionDataset/283.mp3\n",
      "SongEmotionDataset/14.mp3\n",
      "SongEmotionDataset/333.mp3\n",
      "SongEmotionDataset/265.mp3\n",
      "SongEmotionDataset/62.mp3\n",
      "SongEmotionDataset/211.mp3\n",
      "SongEmotionDataset/79.mp3\n",
      "SongEmotionDataset/171.mp3\n",
      "SongEmotionDataset/60.mp3\n",
      "SongEmotionDataset/61.mp3\n",
      "SongEmotionDataset/293.mp3\n",
      "SongEmotionDataset/357.mp3\n",
      "SongEmotionDataset/209.mp3\n",
      "SongEmotionDataset/396.mp3\n",
      "SongEmotionDataset/328.mp3\n",
      "SongEmotionDataset/308.mp3\n",
      "SongEmotionDataset/201.mp3\n",
      "SongEmotionDataset/173.mp3\n",
      "SongEmotionDataset/374.mp3\n",
      "SongEmotionDataset/13.mp3\n",
      "SongEmotionDataset/252.mp3\n",
      "SongEmotionDataset/318.mp3\n",
      "SongEmotionDataset/4.mp3\n",
      "SongEmotionDataset/236.mp3\n",
      "SongEmotionDataset/123.mp3\n",
      "SongEmotionDataset/97.mp3\n",
      "SongEmotionDataset/138.mp3\n",
      "SongEmotionDataset/71.mp3\n",
      "SongEmotionDataset/310.mp3\n",
      "SongEmotionDataset/312.mp3\n",
      "SongEmotionDataset/151.mp3\n",
      "SongEmotionDataset/238.mp3\n",
      "SongEmotionDataset/256.mp3\n",
      "SongEmotionDataset/311.mp3\n",
      "SongEmotionDataset/42.mp3\n",
      "SongEmotionDataset/39.mp3\n",
      "SongEmotionDataset/383.mp3\n",
      "SongEmotionDataset/381.mp3\n",
      "SongEmotionDataset/135.mp3\n",
      "SongEmotionDataset/22.mp3\n",
      "SongEmotionDataset/392.mp3\n",
      "SongEmotionDataset/223.mp3\n",
      "SongEmotionDataset/264.mp3\n",
      "SongEmotionDataset/232.mp3\n",
      "SongEmotionDataset/105.mp3\n",
      "SongEmotionDataset/8.mp3\n",
      "SongEmotionDataset/338.mp3\n",
      "SongEmotionDataset/116.mp3\n",
      "SongEmotionDataset/176.mp3\n",
      "SongEmotionDataset/359.mp3\n",
      "SongEmotionDataset/259.mp3\n",
      "SongEmotionDataset/366.mp3\n",
      "SongEmotionDataset/398.mp3\n",
      "SongEmotionDataset/288.mp3\n",
      "SongEmotionDataset/242.mp3\n",
      "SongEmotionDataset/155.mp3\n",
      "SongEmotionDataset/140.mp3\n",
      "SongEmotionDataset/145.mp3\n",
      "SongEmotionDataset/393.mp3\n",
      "SongEmotionDataset/55.mp3\n",
      "SongEmotionDataset/44.mp3\n",
      "SongEmotionDataset/95.mp3\n",
      "SongEmotionDataset/24.mp3\n",
      "SongEmotionDataset/237.mp3\n",
      "SongEmotionDataset/364.mp3\n",
      "SongEmotionDataset/159.mp3\n",
      "SongEmotionDataset/99.mp3\n",
      "SongEmotionDataset/342.mp3\n",
      "SongEmotionDataset/18.mp3\n",
      "SongEmotionDataset/226.mp3\n",
      "SongEmotionDataset/194.mp3\n",
      "SongEmotionDataset/216.mp3\n",
      "SongEmotionDataset/66.mp3\n",
      "SongEmotionDataset/246.mp3\n",
      "SongEmotionDataset/149.mp3\n",
      "SongEmotionDataset/26.mp3\n",
      "SongEmotionDataset/184.mp3\n",
      "SongEmotionDataset/78.mp3\n",
      "SongEmotionDataset/325.mp3\n",
      "SongEmotionDataset/331.mp3\n",
      "SongEmotionDataset/257.mp3\n",
      "SongEmotionDataset/203.mp3\n",
      "SongEmotionDataset/73.mp3\n",
      "SongEmotionDataset/46.mp3\n",
      "SongEmotionDataset/119.mp3\n",
      "SongEmotionDataset/178.mp3\n",
      "SongEmotionDataset/156.mp3\n",
      "SongEmotionDataset/147.mp3\n",
      "SongEmotionDataset/314.mp3\n",
      "SongEmotionDataset/320.mp3\n",
      "SongEmotionDataset/379.mp3\n",
      "SongEmotionDataset/261.mp3\n",
      "SongEmotionDataset/9.mp3\n",
      "SongEmotionDataset/48.mp3\n",
      "SongEmotionDataset/7.mp3\n",
      "SongEmotionDataset/96.mp3\n",
      "SongEmotionDataset/94.mp3\n",
      "SongEmotionDataset/108.mp3\n",
      "SongEmotionDataset/64.mp3\n",
      "SongEmotionDataset/321.mp3\n",
      "SongEmotionDataset/297.mp3\n",
      "SongEmotionDataset/348.mp3\n",
      "SongEmotionDataset/186.mp3\n",
      "SongEmotionDataset/212.mp3\n",
      "SongEmotionDataset/85.mp3\n",
      "SongEmotionDataset/65.mp3\n",
      "SongEmotionDataset/133.mp3\n",
      "SongEmotionDataset/30.mp3\n",
      "SongEmotionDataset/222.mp3\n",
      "SongEmotionDataset/17.mp3\n",
      "SongEmotionDataset/219.mp3\n",
      "SongEmotionDataset/197.mp3\n",
      "SongEmotionDataset/32.mp3\n",
      "SongEmotionDataset/340.mp3\n",
      "SongEmotionDataset/326.mp3\n",
      "SongEmotionDataset/289.mp3\n",
      "SongEmotionDataset/377.mp3\n",
      "SongEmotionDataset/113.mp3\n",
      "SongEmotionDataset/224.mp3\n",
      "SongEmotionDataset/144.mp3\n",
      "SongEmotionDataset/174.mp3\n",
      "SongEmotionDataset/387.mp3\n",
      "SongEmotionDataset/382.mp3\n",
      "SongEmotionDataset/148.mp3\n",
      "SongEmotionDataset/50.mp3\n",
      "SongEmotionDataset/251.mp3\n",
      "SongEmotionDataset/128.mp3\n",
      "SongEmotionDataset/83.mp3\n",
      "SongEmotionDataset/124.mp3\n",
      "SongEmotionDataset/384.mp3\n",
      "SongEmotionDataset/150.mp3\n",
      "SongEmotionDataset/58.mp3\n",
      "SongEmotionDataset/53.mp3\n",
      "SongEmotionDataset/29.mp3\n",
      "SongEmotionDataset/263.mp3\n",
      "SongEmotionDataset/316.mp3\n",
      "SongEmotionDataset/371.mp3\n",
      "SongEmotionDataset/276.mp3\n",
      "SongEmotionDataset/228.mp3\n",
      "SongEmotionDataset/286.mp3\n",
      "SongEmotionDataset/370.mp3\n",
      "SongEmotionDataset/239.mp3\n",
      "SongEmotionDataset/335.mp3\n",
      "SongEmotionDataset/207.mp3\n",
      "SongEmotionDataset/245.mp3\n",
      "SongEmotionDataset/266.mp3\n",
      "SongEmotionDataset/196.mp3\n",
      "SongEmotionDataset/198.mp3\n",
      "SongEmotionDataset/309.mp3\n",
      "SongEmotionDataset/272.mp3\n",
      "SongEmotionDataset/114.mp3\n",
      "SongEmotionDataset/250.mp3\n",
      "SongEmotionDataset/92.mp3\n",
      "SongEmotionDataset/295.mp3\n",
      "SongEmotionDataset/15.mp3\n",
      "SongEmotionDataset/170.mp3\n",
      "SongEmotionDataset/391.mp3\n",
      "SongEmotionDataset/254.mp3\n",
      "SongEmotionDataset/102.mp3\n",
      "SongEmotionDataset/89.mp3\n",
      "SongEmotionDataset/185.mp3\n",
      "SongEmotionDataset/158.mp3\n",
      "SongEmotionDataset/248.mp3\n",
      "SongEmotionDataset/271.mp3\n",
      "SongEmotionDataset/75.mp3\n",
      "SongEmotionDataset/378.mp3\n",
      "SongEmotionDataset/356.mp3\n",
      "SongEmotionDataset/389.mp3\n",
      "SongEmotionDataset/327.mp3\n",
      "SongEmotionDataset/137.mp3\n",
      "SongEmotionDataset/345.mp3\n",
      "SongEmotionDataset/16.mp3\n",
      "SongEmotionDataset/12.mp3\n",
      "SongEmotionDataset/260.mp3\n",
      "SongEmotionDataset/306.mp3\n",
      "SongEmotionDataset/217.mp3\n",
      "SongEmotionDataset/177.mp3\n",
      "SongEmotionDataset/350.mp3\n",
      "SongEmotionDataset/72.mp3\n",
      "SongEmotionDataset/360.mp3\n",
      "SongEmotionDataset/373.mp3\n",
      "SongEmotionDataset/103.mp3\n",
      "SongEmotionDataset/351.mp3\n",
      "SongEmotionDataset/231.mp3\n",
      "SongEmotionDataset/229.mp3\n",
      "SongEmotionDataset/319.mp3\n",
      "SongEmotionDataset/74.mp3\n",
      "SongEmotionDataset/167.mp3\n",
      "SongEmotionDataset/187.mp3\n",
      "SongEmotionDataset/70.mp3\n",
      "SongEmotionDataset/84.mp3\n",
      "SongEmotionDataset/21.mp3\n",
      "SongEmotionDataset/290.mp3\n",
      "SongEmotionDataset/188.mp3\n",
      "SongEmotionDataset/361.mp3\n",
      "SongEmotionDataset/139.mp3\n",
      "SongEmotionDataset/303.mp3\n",
      "SongEmotionDataset/298.mp3\n",
      "SongEmotionDataset/394.mp3\n",
      "SongEmotionDataset/52.mp3\n",
      "SongEmotionDataset/40.mp3\n",
      "SongEmotionDataset/11.mp3\n",
      "SongEmotionDataset/349.mp3\n",
      "SongEmotionDataset/341.mp3\n",
      "SongEmotionDataset/27.mp3\n",
      "SongEmotionDataset/93.mp3\n",
      "SongEmotionDataset/181.mp3\n",
      "SongEmotionDataset/282.mp3\n",
      "SongEmotionDataset/213.mp3\n",
      "SongEmotionDataset/104.mp3\n",
      "SongEmotionDataset/367.mp3\n",
      "SongEmotionDataset/136.mp3\n",
      "SongEmotionDataset/31.mp3\n",
      "SongEmotionDataset/294.mp3\n",
      "SongEmotionDataset/172.mp3\n",
      "SongEmotionDataset/146.mp3\n",
      "SongEmotionDataset/168.mp3\n",
      "SongEmotionDataset/109.mp3\n",
      "SongEmotionDataset/164.mp3\n",
      "SongEmotionDataset/28.mp3\n",
      "SongEmotionDataset/273.mp3\n",
      "SongEmotionDataset/49.mp3\n",
      "SongEmotionDataset/220.mp3\n",
      "SongEmotionDataset/192.mp3\n",
      "SongEmotionDataset/117.mp3\n",
      "SongEmotionDataset/129.mp3\n",
      "SongEmotionDataset/59.mp3\n",
      "SongEmotionDataset/322.mp3\n",
      "SongEmotionDataset/195.mp3\n",
      "SongEmotionDataset/76.mp3\n",
      "SongEmotionDataset/134.mp3\n",
      "SongEmotionDataset/299.mp3\n",
      "SongEmotionDataset/41.mp3\n",
      "SongEmotionDataset/143.mp3\n",
      "SongEmotionDataset/284.mp3\n",
      "SongEmotionDataset/106.mp3\n",
      "SongEmotionDataset/193.mp3\n",
      "SongEmotionDataset/122.mp3\n",
      "SongEmotionDataset/162.mp3\n",
      "SongEmotionDataset/199.mp3\n",
      "SongEmotionDataset/110.mp3\n",
      "SongEmotionDataset/23.mp3\n",
      "SongEmotionDataset/225.mp3\n",
      "SongEmotionDataset/301.mp3\n",
      "SongEmotionDataset/179.mp3\n",
      "SongEmotionDataset/352.mp3\n",
      "SongEmotionDataset/368.mp3\n",
      "SongEmotionDataset/180.mp3\n",
      "SongEmotionDataset/244.mp3\n",
      "SongEmotionDataset/230.mp3\n",
      "SongEmotionDataset/380.mp3\n",
      "SongEmotionDataset/191.mp3\n",
      "SongEmotionDataset/270.mp3\n",
      "SongEmotionDataset/300.mp3\n",
      "SongEmotionDataset/175.mp3\n",
      "SongEmotionDataset/344.mp3\n",
      "SongEmotionDataset/38.mp3\n",
      "SongEmotionDataset/385.mp3\n",
      "SongEmotionDataset/346.mp3\n",
      "SongEmotionDataset/86.mp3\n",
      "SongEmotionDataset/165.mp3\n",
      "SongEmotionDataset/253.mp3\n",
      "SongEmotionDataset/33.mp3\n",
      "SongEmotionDataset/142.mp3\n",
      "SongEmotionDataset/337.mp3\n",
      "SongEmotionDataset/400.mp3\n",
      "SongEmotionDataset/1.mp3\n",
      "SongEmotionDataset/130.mp3\n",
      "SongEmotionDataset/132.mp3\n",
      "SongEmotionDataset/291.mp3\n",
      "SongEmotionDataset/362.mp3\n",
      "SongEmotionDataset/269.mp3\n",
      "SongEmotionDataset/292.mp3\n",
      "SongEmotionDataset/25.mp3\n",
      "SongEmotionDataset/241.mp3\n",
      "SongEmotionDataset/354.mp3\n",
      "SongEmotionDataset/101.mp3\n",
      "SongEmotionDataset/267.mp3\n",
      "SongEmotionDataset/205.mp3\n",
      "SongEmotionDataset/154.mp3\n",
      "SongEmotionDataset/332.mp3\n",
      "SongEmotionDataset/233.mp3\n",
      "SongEmotionDataset/258.mp3\n",
      "SongEmotionDataset/82.mp3\n",
      "SongEmotionDataset/19.mp3\n",
      "SongEmotionDataset/6.mp3\n",
      "SongEmotionDataset/218.mp3\n",
      "SongEmotionDataset/268.mp3\n",
      "SongEmotionDataset/153.mp3\n",
      "SongEmotionDataset/200.mp3\n",
      "SongEmotionDataset/202.mp3\n",
      "SongEmotionDataset/69.mp3\n",
      "SongEmotionDataset/208.mp3\n",
      "SongEmotionDataset/339.mp3\n",
      "SongEmotionDataset/189.mp3\n",
      "SongEmotionDataset/317.mp3\n",
      "SongEmotionDataset/278.mp3\n",
      "SongEmotionDataset/80.mp3\n",
      "SongEmotionDataset/279.mp3\n",
      "SongEmotionDataset/118.mp3\n",
      "SongEmotionDataset/206.mp3\n",
      "SongEmotionDataset/51.mp3\n",
      "SongEmotionDataset/10.mp3\n",
      "SongEmotionDataset/2.mp3\n",
      "SongEmotionDataset/315.mp3\n",
      "SongEmotionDataset/329.mp3\n",
      "SongEmotionDataset/399.mp3\n",
      "SongEmotionDataset/190.mp3\n",
      "SongEmotionDataset/330.mp3\n",
      "SongEmotionDataset/227.mp3\n",
      "SongEmotionDataset/163.mp3\n",
      "SongEmotionDataset/141.mp3\n",
      "SongEmotionDataset/210.mp3\n",
      "SongEmotionDataset/397.mp3\n",
      "SongEmotionDataset/54.mp3\n",
      "SongEmotionDataset/67.mp3\n",
      "SongEmotionDataset/204.mp3\n",
      "SongEmotionDataset/280.mp3\n",
      "SongEmotionDataset/343.mp3\n",
      "SongEmotionDataset/243.mp3\n",
      "SongEmotionDataset/376.mp3\n",
      "SongEmotionDataset/77.mp3\n",
      "SongEmotionDataset/375.mp3\n",
      "SongEmotionDataset/81.mp3\n",
      "SongEmotionDataset/336.mp3\n",
      "SongEmotionDataset/5.mp3\n",
      "SongEmotionDataset/240.mp3\n",
      "SongEmotionDataset/234.mp3\n",
      "SongEmotionDataset/296.mp3\n",
      "SongEmotionDataset/112.mp3\n",
      "SongEmotionDataset/287.mp3\n",
      "SongEmotionDataset/111.mp3\n",
      "SongEmotionDataset/161.mp3\n",
      "SongEmotionDataset/90.mp3\n",
      "SongEmotionDataset/20.mp3\n",
      "SongEmotionDataset/324.mp3\n",
      "SongEmotionDataset/313.mp3\n",
      "SongEmotionDataset/285.mp3\n",
      "SongEmotionDataset/363.mp3\n",
      "SongEmotionDataset/305.mp3\n",
      "SongEmotionDataset/160.mp3\n",
      "SongEmotionDataset/36.mp3\n",
      "SongEmotionDataset/88.mp3\n",
      "SongEmotionDataset/127.mp3\n",
      "SongEmotionDataset/274.mp3\n",
      "SongEmotionDataset/215.mp3\n",
      "SongEmotionDataset/63.mp3\n",
      "SongEmotionDataset/277.mp3\n",
      "SongEmotionDataset/355.mp3\n",
      "SongEmotionDataset/395.mp3\n",
      "SongEmotionDataset/3.mp3\n",
      "SongEmotionDataset/365.mp3\n",
      "SongEmotionDataset/169.mp3\n",
      "SongEmotionDataset/47.mp3\n",
      "SongEmotionDataset/302.mp3\n",
      "SongEmotionDataset/34.mp3\n",
      "SongEmotionDataset/56.mp3\n",
      "SongEmotionDataset/98.mp3\n",
      "SongEmotionDataset/307.mp3\n",
      "SongEmotionDataset/107.mp3\n",
      "SongEmotionDataset/152.mp3\n",
      "SongEmotionDataset/281.mp3\n",
      "SongEmotionDataset/91.mp3\n",
      "SongEmotionDataset/304.mp3\n",
      "SongEmotionDataset/125.mp3\n",
      "SongEmotionDataset/323.mp3\n",
      "SongEmotionDataset/388.mp3\n",
      "SongEmotionDataset/87.mp3\n",
      "SongEmotionDataset/131.mp3\n",
      "SongEmotionDataset/369.mp3\n",
      "SongEmotionDataset/157.mp3\n",
      "SongEmotionDataset/57.mp3\n",
      "SongEmotionDataset/358.mp3\n",
      "SongEmotionDataset/353.mp3\n",
      "SongEmotionDataset/166.mp3\n",
      "SongEmotionDataset/100.mp3\n",
      "SongEmotionDataset/43.mp3\n",
      "SongEmotionDataset/334.mp3\n",
      "SongEmotionDataset/255.mp3\n",
      "SongEmotionDataset/68.mp3\n",
      "SongEmotionDataset/115.mp3\n"
     ]
    }
   ],
   "source": [
    "for sound_file in data_path.iterdir():\n",
    "    if \".mp3\" in str(sound_file):\n",
    "        print(sound_file)\n",
    "        audio, sample_rate = librosa.load(str(sound_file), res_type='kaiser_fast')\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/czf/memories/venv/lib/python3.6/site-packages/librosa/core/audio.py:161: UserWarning: PySoundFile failed. Trying audioread instead.\n",
      "  warnings.warn('PySoundFile failed. Trying audioread instead.')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(40, 2586) (1323648,)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[-5.30341797e+02, -4.07741577e+02, -3.27536621e+02, ...,\n",
       "        -2.39811523e+02, -1.96744080e+02, -1.44711777e+02],\n",
       "       [ 5.81265569e-01,  1.03006027e+02,  1.29354553e+02, ...,\n",
       "         1.48707626e+02,  1.45873001e+02,  1.28202530e+02],\n",
       "       [ 4.58764762e-01,  7.53921986e+00, -1.18814125e+01, ...,\n",
       "        -2.51551704e+01, -1.92207527e+01, -1.79366188e+01],\n",
       "       ...,\n",
       "       [ 3.11299562e-01, -1.29907084e+00,  1.18818974e+00, ...,\n",
       "        -6.58579540e+00, -3.34302998e+00, -4.75482178e+00],\n",
       "       [ 2.23848164e-01, -3.19489312e+00, -2.78556681e+00, ...,\n",
       "        -1.36089420e+01, -6.40699673e+00, -5.27228928e+00],\n",
       "       [ 8.67742151e-02,  1.31472754e+00, -1.41885233e+00, ...,\n",
       "         3.34440261e-01,  1.14392626e+00, -3.62402201e-02]], dtype=float32)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "audio, sample_rate = librosa.load(\"SongEmotionDataset/1.mp3\", res_type='kaiser_fast')\n",
    "# [print(x) for x in audio]\n",
    "\n",
    "#convert audio into 2d array\n",
    "mfccs = librosa.feature.mfcc(y=audio, sr=sample_rate, n_mfcc=40)\n",
    "# mfccsscaled = np.mean(mfccs.T,axis=0)\n",
    "print(mfccs.shape, audio.shape)\n",
    "mfccs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1323648])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "audio_tensor = torch.tensor(audio)\n",
    "audio_tensor\n",
    "audio_tensor.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Net(\n",
      "  (conv1): Conv1d(1, 128, kernel_size=(80,), stride=(4,))\n",
      "  (bn1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (pool1): MaxPool1d(kernel_size=4, stride=4, padding=0, dilation=1, ceil_mode=False)\n",
      "  (conv2): Conv1d(128, 128, kernel_size=(3,), stride=(1,))\n",
      "  (bn2): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (pool2): MaxPool1d(kernel_size=4, stride=4, padding=0, dilation=1, ceil_mode=False)\n",
      "  (conv3): Conv1d(128, 256, kernel_size=(3,), stride=(1,))\n",
      "  (bn3): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (pool3): MaxPool1d(kernel_size=4, stride=4, padding=0, dilation=1, ceil_mode=False)\n",
      "  (conv4): Conv1d(256, 512, kernel_size=(3,), stride=(1,))\n",
      "  (bn4): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (pool4): MaxPool1d(kernel_size=4, stride=4, padding=0, dilation=1, ceil_mode=False)\n",
      "  (avgPool): AvgPool1d(kernel_size=(30,), stride=(30,), padding=(0,))\n",
      "  (fc1): Linear(in_features=512, out_features=5, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.conv1 = nn.Conv1d(1, 128, 80, 4)\n",
    "        self.bn1 = nn.BatchNorm1d(128)\n",
    "        self.pool1 = nn.MaxPool1d(4)\n",
    "        self.conv2 = nn.Conv1d(128, 128, 3)\n",
    "        self.bn2 = nn.BatchNorm1d(128)\n",
    "        self.pool2 = nn.MaxPool1d(4)\n",
    "        self.conv3 = nn.Conv1d(128, 256, 3)\n",
    "        self.bn3 = nn.BatchNorm1d(256)\n",
    "        self.pool3 = nn.MaxPool1d(4)\n",
    "        self.conv4 = nn.Conv1d(256, 512, 3)\n",
    "        self.bn4 = nn.BatchNorm1d(512)\n",
    "        self.pool4 = nn.MaxPool1d(4)\n",
    "        self.avgPool = nn.AvgPool1d(30) #input should be 512x30 so this outputs a 512x1\n",
    "        self.fc1 = nn.Linear(512, 5)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = F.relu(self.bn1(x))\n",
    "        x = self.pool1(x)\n",
    "        x = self.conv2(x)\n",
    "        x = F.relu(self.bn2(x))\n",
    "        x = self.pool2(x)\n",
    "        x = self.conv3(x)\n",
    "        x = F.relu(self.bn3(x))\n",
    "        x = self.pool3(x)\n",
    "        x = self.conv4(x)\n",
    "        x = F.relu(self.bn4(x))\n",
    "        x = self.pool4(x)\n",
    "        x = self.avgPool(x)\n",
    "        x = x.permute(0, 2, 1) #change the 512x1 to 1x512\n",
    "        x = self.fc1(x)\n",
    "        return F.log_softmax(x, dim = 2)\n",
    "\n",
    "model = Net()\n",
    "model.to(device)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = torch.utils.Dataloader()\n",
    "test_loader = torch.utils.DataLoader()\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr = 0.01, weight_decay = 0.0001)\n",
    "scheduler = optim.lr_scheduler.StepLR(optimizer, step_size = 20, gamma = 0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train():\n",
    "    model.train()\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        optimizer.zero_grad()\n",
    "        data.unsqueeze_(1)\n",
    "        data = data.requires_grad_() #set requires_grad to True for training\n",
    "        output = model(data)\n",
    "        output = output.view(-1, len(emotions))\n",
    "#         print(output.shape, target.shape)\n",
    "#         print(output, target)\n",
    "        loss = F.kl_div(output, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if batch_idx % log_interval == 0: #print training stats\n",
    "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                epoch, batch_idx * len(data), len(train_loader.dataset),\n",
    "                100. * batch_idx / len(train_loader), loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(model, epoch):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    for data, target in test_loader:\n",
    "        data.unsqueeze_(1)\n",
    "        output = model(data)\n",
    "        output = output.permute(1, 0, 2)\n",
    "        pred = output.max(2)[1] # get the index of the max log-probability\n",
    "        correct += pred.eq(target.max(1)[1]).cpu().sum().item()\n",
    "    print('\\nTest set: Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
    "        correct, len(test_loader.dataset),\n",
    "        100. * correct / len(test_loader.dataset)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
