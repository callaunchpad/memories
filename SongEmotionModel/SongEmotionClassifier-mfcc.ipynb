{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xlrd\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from numba import decorators\n",
    "import librosa\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import Dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read in the data file\n",
    "# Give the location of the file \n",
    "\n",
    "df = pd.read_excel(r'data/data.xlsx', sheet_name='reduced totals')\n",
    "# print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "## LOADING IN DATASETS\n",
    "\n",
    "dataset = Path.cwd().joinpath(\"SongEmotionDataset\")\n",
    "datasheet = Path.cwd().joinpath(\"data\") # for csua\n",
    "\n",
    "#emotion labels\n",
    "label_loc = datasheet.joinpath(\"data.xlsx\")\n",
    "wb = xlrd.open_workbook(label_loc) \n",
    "sheet = wb.sheet_by_index(2)\n",
    "\n",
    "#emotion arr\n",
    "emotions = [\"amazement\", \"calmness\", \"power\", \"joyful activation\", \"sadness\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 11.4343, 157.9053,  55.5535, 120.1634,  54.9434], device='cuda:0')\n",
      "tensor([3.8678, 9.1242, 8.9825, 8.8231, 8.2024], device='cuda:0')\n",
      "[1, 10, 10, 11, 7]\n"
     ]
    }
   ],
   "source": [
    "train_percentage = 0.8\n",
    "allowed_exceedance = 0\n",
    "\n",
    "train_song = []\n",
    "test_song = []\n",
    "train_emotion = []\n",
    "test_emotion = []\n",
    "\n",
    "row_indexes = np.arange(1,401)\n",
    "np.random.shuffle(row_indexes)\n",
    "\n",
    "# train_indexes = [row_indexes[i] for i in range(len(row_indexes)) if i < len(row_indexes)*train_percentage]\n",
    "# test_indexes = [row_indexes[i] for i in range(len(row_indexes)) if i >= len(row_indexes)*train_percentage]\n",
    "\n",
    "def get_data(indexes):\n",
    "    song = []\n",
    "    emotion = []\n",
    "    \n",
    "    totals = torch.zeros(len(emotions), device=device).float()\n",
    "    for x in indexes:    \n",
    "        row = torch.tensor([sheet.cell_value(x, 2 + j) for j in range(5)], device=device).float()\n",
    "        totals += F.softmax(row)\n",
    "\n",
    "    min_total = torch.min(totals)\n",
    "    print(totals)\n",
    "    \n",
    "    totals = torch.zeros(len(emotions), device=device).float()\n",
    "    for x in indexes:\n",
    "        row = torch.tensor([sheet.cell_value(x, 2 + j) for j in range(5)], device=device).float()\n",
    "        \n",
    "        if torch.max(totals + row) < min_total*(1 + allowed_exceedance):\n",
    "            song.append(dataset.joinpath(\"{}.mp3\".format(x)))\n",
    "            emotion.append(row)\n",
    "            totals += F.softmax(row)\n",
    "            \n",
    "    print(totals)\n",
    "    return song, emotion\n",
    "    \n",
    "song, emotion = get_data(row_indexes)\n",
    "# test_song, test_emotion = get_data(test_indexes)\n",
    "\n",
    "train_song = [song[i] for i in range(len(song)) if i < len(song)*train_percentage]\n",
    "test_song = [song[i] for i in range(len(song)) if i >= len(song)*train_percentage]\n",
    "\n",
    "train_emotion = [emotion[i] for i in range(len(emotion)) if i < len(emotion)*train_percentage]\n",
    "test_emotion = [emotion[i] for i in range(len(emotion)) if i >= len(emotion)*train_percentage]\n",
    "\n",
    "num_maxes = [0 for _ in emotions]\n",
    "\n",
    "for row in emotion:\n",
    "    i = torch.argmax(row)\n",
    "    num_maxes[i] += 1\n",
    "    \n",
    "print(num_maxes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_emotion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SongEmotionDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Song Emotion Dataset. Uses librosa to process mp3 files.\n",
    "    Takes first 20 seconds, and samples every 10 to get processed audio tensor.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, mp3, labels, transform=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            mp3: list of paths to mp3 files\n",
    "            labels: list of labels\n",
    "        \"\"\"\n",
    "        self.labels = labels\n",
    "        self.mp3 = mp3\n",
    "        self.cache = {}\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        if index not in self.cache.keys():\n",
    "#             print(\"index of \" + str(index) + \" was cached!\")\n",
    "            data, rate = librosa.load(self.mp3[index], sr=16000, duration=10)\n",
    "            mfccs = librosa.feature.mfcc(y=data, sr=rate, n_mfcc=40)\n",
    "            assert rate == 16000\n",
    "            sample_tensor = torch.tensor(mfccs, device=device).float()\n",
    "            downsampled_tensor = sample_tensor[::10]\n",
    "    #         print(mfccs.shape, data.shape)\n",
    "\n",
    "            self.cache[index] = (downsampled_tensor, F.softmax(self.labels[index]))\n",
    "#         else:\n",
    "#             print(\"index was cached! index of \" + str(index))\n",
    "        return self.cache[index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Net(\n",
      "  (conv1): Conv2d(1, 32, kernel_size=(2, 2), stride=(1, 1))\n",
      "  (conv2): Conv2d(32, 64, kernel_size=(2, 2), stride=(1, 1))\n",
      "  (pool1): MaxPool2d(kernel_size=(2, 2), stride=(2, 2), padding=0, dilation=1, ceil_mode=False)\n",
      "  (drop1): Dropout(p=0.25, inplace=False)\n",
      "  (flat1): Flatten()\n",
      "  (dense1): Linear(in_features=9920, out_features=128, bias=True)\n",
      "  (drop2): Dropout(p=0.5, inplace=False)\n",
      "  (dense2): Linear(in_features=128, out_features=5, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 32, (2,2))\n",
    "        self.conv2 = nn.Conv2d(32, 64, (2,2))\n",
    "        self.pool1 = nn.MaxPool2d((2,2))\n",
    "        self.drop1 = nn.Dropout(p=0.25)\n",
    "        self.flat1 = nn.Flatten()\n",
    "        self.dense1 = nn.Linear(9920, 128)\n",
    "        self.drop2 = nn.Dropout(p=0.5)\n",
    "        self.dense2 = nn.Linear(128, 5)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.conv2(x)\n",
    "        x = F.relu(x)\n",
    "        \n",
    "        x = self.pool1(x)\n",
    "        x = self.drop1(x)\n",
    "        x = self.flat1(x)\n",
    "        x = self.dense1(x)\n",
    "        x = F.relu(x)\n",
    "        \n",
    "        x = self.drop2(x)\n",
    "        x = self.dense2(x)\n",
    "        x = F.relu(x)\n",
    "        \n",
    "        return F.log_softmax(x, dim = 1)\n",
    "\n",
    "model = Net()\n",
    "model.to(device)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train set size: 67\n",
      "Test set size: 16\n"
     ]
    }
   ],
   "source": [
    "train_set = SongEmotionDataset(train_song, train_emotion)\n",
    "test_set = SongEmotionDataset(test_song, test_emotion)\n",
    "print(\"Train set size: \" + str(len(train_set)))\n",
    "print(\"Test set size: \" + str(len(test_set)))\n",
    "\n",
    "kwargs = {'num_workers': 1, 'pin_memory': True} if device == 'cuda' else {} #needed for using datasets on gpu\n",
    "train_loader = torch.utils.data.DataLoader(train_set, batch_size = 8, shuffle = True, **kwargs)\n",
    "test_loader = torch.utils.data.DataLoader(test_set, batch_size = 8, shuffle = True, **kwargs)\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr = 0.01, weight_decay = 0.0001)\n",
    "scheduler = optim.lr_scheduler.StepLR(optimizer, step_size = 20, gamma = 0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, epoch):\n",
    "    model.train()\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        optimizer.zero_grad()\n",
    "        data.unsqueeze_(1)\n",
    "        data = data.requires_grad_() #set requires_grad to True for training\n",
    "        output = model(data)\n",
    "#         output = output.view(-1, len(emotions))\n",
    "#         print(output.shape, target.shape)\n",
    "#         print(output, target)\n",
    "#         loss = F.kl_div(output, target)\n",
    "        kl = nn.KLDivLoss()\n",
    "        loss = kl(output, target)\n",
    "#         loss = F.cross_entropy(output, target)\n",
    "#         loss = nn.CrossEntropyLoss(output, target)\n",
    "#         loss = F.nll_loss(output, target)\n",
    "        loss.backward()\n",
    "        print(loss)\n",
    "        optimizer.step()\n",
    "#         scheduler.step()\n",
    "        if batch_idx % log_interval == 0: #print training stats\n",
    "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                epoch, batch_idx * len(data), len(train_loader.dataset),\n",
    "                100. * batch_idx / len(train_loader), loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(model, epoch):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    for data, target in test_loader:\n",
    "        data. unsqueeze_(1)\n",
    "        output = model(data)\n",
    "        print(output)\n",
    "        print(target)\n",
    "        print(\"\\n\")\n",
    "#         output = output.permute(1, 0, 2)\n",
    "        pred = output.max(1)[1] # get the index of the max log-probability \n",
    "        correct += pred.eq(target.max(1)[1]).cpu().sum().item()\n",
    "    print('\\nTest set: Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
    "        correct, len(test_loader.dataset),\n",
    "        100. * correct / len(test_loader.dataset)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training epoch 1\n",
      "tensor(0.2435, device='cuda:0', grad_fn=<KlDivBackward>)\n",
      "Train Epoch: 1 [0/67 (0%)]\tLoss: 0.243493\n",
      "tensor(0.1910, device='cuda:0', grad_fn=<KlDivBackward>)\n",
      "tensor(0.2449, device='cuda:0', grad_fn=<KlDivBackward>)\n",
      "tensor(0.1865, device='cuda:0', grad_fn=<KlDivBackward>)\n",
      "tensor(0.2535, device='cuda:0', grad_fn=<KlDivBackward>)\n",
      "tensor(0.2018, device='cuda:0', grad_fn=<KlDivBackward>)\n",
      "Train Epoch: 1 [40/67 (56%)]\tLoss: 0.201776\n",
      "tensor(0.2140, device='cuda:0', grad_fn=<KlDivBackward>)\n",
      "tensor(0.1921, device='cuda:0', grad_fn=<KlDivBackward>)\n",
      "tensor(0.1567, device='cuda:0', grad_fn=<KlDivBackward>)\n",
      "tensor([[-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094]], device='cuda:0',\n",
      "       grad_fn=<LogSoftmaxBackward>)\n",
      "tensor([[0.0047, 0.0345, 0.6931, 0.0127, 0.2550],\n",
      "        [0.0388, 0.0388, 0.7784, 0.1053, 0.0388],\n",
      "        [0.1441, 0.3917, 0.0530, 0.0195, 0.3917],\n",
      "        [0.0317, 0.0861, 0.0117, 0.2341, 0.6364],\n",
      "        [0.0161, 0.0438, 0.0438, 0.0161, 0.8801],\n",
      "        [0.0202, 0.1491, 0.4053, 0.0202, 0.4053],\n",
      "        [0.0146, 0.0397, 0.7979, 0.1080, 0.0397],\n",
      "        [0.1643, 0.0604, 0.4466, 0.1643, 0.1643]], device='cuda:0')\n",
      "\n",
      "\n",
      "tensor([[-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094]], device='cuda:0',\n",
      "       grad_fn=<LogSoftmaxBackward>)\n",
      "tensor([[0.6869, 0.0930, 0.0930, 0.0342, 0.0930],\n",
      "        [0.1044, 0.0141, 0.0052, 0.1044, 0.7718],\n",
      "        [0.0161, 0.0438, 0.8801, 0.0438, 0.0161],\n",
      "        [0.1915, 0.0705, 0.5206, 0.0259, 0.1915],\n",
      "        [0.0408, 0.0150, 0.8185, 0.0150, 0.1108],\n",
      "        [0.3483, 0.3483, 0.0471, 0.1281, 0.1281],\n",
      "        [0.0157, 0.0426, 0.0426, 0.0426, 0.8564],\n",
      "        [0.0117, 0.0861, 0.6364, 0.2341, 0.0317]], device='cuda:0')\n",
      "\n",
      "\n",
      "\n",
      "Test set: Accuracy: 2/16 (12%)\n",
      "\n",
      "training epoch 2\n",
      "tensor(0.2562, device='cuda:0', grad_fn=<KlDivBackward>)\n",
      "Train Epoch: 2 [0/67 (0%)]\tLoss: 0.256153\n",
      "tensor(0.2495, device='cuda:0', grad_fn=<KlDivBackward>)\n",
      "tensor(0.1956, device='cuda:0', grad_fn=<KlDivBackward>)\n",
      "tensor(0.1833, device='cuda:0', grad_fn=<KlDivBackward>)\n",
      "tensor(0.1547, device='cuda:0', grad_fn=<KlDivBackward>)\n",
      "tensor(0.1857, device='cuda:0', grad_fn=<KlDivBackward>)\n",
      "Train Epoch: 2 [40/67 (56%)]\tLoss: 0.185701\n",
      "tensor(0.2112, device='cuda:0', grad_fn=<KlDivBackward>)\n",
      "tensor(0.2374, device='cuda:0', grad_fn=<KlDivBackward>)\n",
      "tensor(0.2999, device='cuda:0', grad_fn=<KlDivBackward>)\n",
      "tensor([[-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094]], device='cuda:0',\n",
      "       grad_fn=<LogSoftmaxBackward>)\n",
      "tensor([[0.0388, 0.0388, 0.7784, 0.1053, 0.0388],\n",
      "        [0.0157, 0.0426, 0.0426, 0.0426, 0.8564],\n",
      "        [0.0161, 0.0438, 0.0438, 0.0161, 0.8801],\n",
      "        [0.0202, 0.1491, 0.4053, 0.0202, 0.4053],\n",
      "        [0.0146, 0.0397, 0.7979, 0.1080, 0.0397],\n",
      "        [0.0161, 0.0438, 0.8801, 0.0438, 0.0161],\n",
      "        [0.3483, 0.3483, 0.0471, 0.1281, 0.1281],\n",
      "        [0.1915, 0.0705, 0.5206, 0.0259, 0.1915]], device='cuda:0')\n",
      "\n",
      "\n",
      "tensor([[-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094]], device='cuda:0',\n",
      "       grad_fn=<LogSoftmaxBackward>)\n",
      "tensor([[0.6869, 0.0930, 0.0930, 0.0342, 0.0930],\n",
      "        [0.0317, 0.0861, 0.0117, 0.2341, 0.6364],\n",
      "        [0.1441, 0.3917, 0.0530, 0.0195, 0.3917],\n",
      "        [0.0117, 0.0861, 0.6364, 0.2341, 0.0317],\n",
      "        [0.1643, 0.0604, 0.4466, 0.1643, 0.1643],\n",
      "        [0.0047, 0.0345, 0.6931, 0.0127, 0.2550],\n",
      "        [0.1044, 0.0141, 0.0052, 0.1044, 0.7718],\n",
      "        [0.0408, 0.0150, 0.8185, 0.0150, 0.1108]], device='cuda:0')\n",
      "\n",
      "\n",
      "\n",
      "Test set: Accuracy: 2/16 (12%)\n",
      "\n",
      "training epoch 3\n",
      "tensor(0.2308, device='cuda:0', grad_fn=<KlDivBackward>)\n",
      "Train Epoch: 3 [0/67 (0%)]\tLoss: 0.230822\n",
      "tensor(0.2109, device='cuda:0', grad_fn=<KlDivBackward>)\n",
      "tensor(0.2225, device='cuda:0', grad_fn=<KlDivBackward>)\n",
      "tensor(0.1906, device='cuda:0', grad_fn=<KlDivBackward>)\n",
      "tensor(0.2653, device='cuda:0', grad_fn=<KlDivBackward>)\n",
      "tensor(0.1971, device='cuda:0', grad_fn=<KlDivBackward>)\n",
      "Train Epoch: 3 [40/67 (56%)]\tLoss: 0.197117\n",
      "tensor(0.1720, device='cuda:0', grad_fn=<KlDivBackward>)\n",
      "tensor(0.1809, device='cuda:0', grad_fn=<KlDivBackward>)\n",
      "tensor(0.3089, device='cuda:0', grad_fn=<KlDivBackward>)\n",
      "tensor([[-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094]], device='cuda:0',\n",
      "       grad_fn=<LogSoftmaxBackward>)\n",
      "tensor([[0.0146, 0.0397, 0.7979, 0.1080, 0.0397],\n",
      "        [0.1441, 0.3917, 0.0530, 0.0195, 0.3917],\n",
      "        [0.6869, 0.0930, 0.0930, 0.0342, 0.0930],\n",
      "        [0.0408, 0.0150, 0.8185, 0.0150, 0.1108],\n",
      "        [0.0161, 0.0438, 0.8801, 0.0438, 0.0161],\n",
      "        [0.0157, 0.0426, 0.0426, 0.0426, 0.8564],\n",
      "        [0.1915, 0.0705, 0.5206, 0.0259, 0.1915],\n",
      "        [0.3483, 0.3483, 0.0471, 0.1281, 0.1281]], device='cuda:0')\n",
      "\n",
      "\n",
      "tensor([[-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094]], device='cuda:0',\n",
      "       grad_fn=<LogSoftmaxBackward>)\n",
      "tensor([[0.0047, 0.0345, 0.6931, 0.0127, 0.2550],\n",
      "        [0.0161, 0.0438, 0.0438, 0.0161, 0.8801],\n",
      "        [0.0202, 0.1491, 0.4053, 0.0202, 0.4053],\n",
      "        [0.1044, 0.0141, 0.0052, 0.1044, 0.7718],\n",
      "        [0.1643, 0.0604, 0.4466, 0.1643, 0.1643],\n",
      "        [0.0117, 0.0861, 0.6364, 0.2341, 0.0317],\n",
      "        [0.0317, 0.0861, 0.0117, 0.2341, 0.6364],\n",
      "        [0.0388, 0.0388, 0.7784, 0.1053, 0.0388]], device='cuda:0')\n",
      "\n",
      "\n",
      "\n",
      "Test set: Accuracy: 2/16 (12%)\n",
      "\n",
      "training epoch 4\n",
      "tensor(0.1667, device='cuda:0', grad_fn=<KlDivBackward>)\n",
      "Train Epoch: 4 [0/67 (0%)]\tLoss: 0.166682\n",
      "tensor(0.2253, device='cuda:0', grad_fn=<KlDivBackward>)\n",
      "tensor(0.1859, device='cuda:0', grad_fn=<KlDivBackward>)\n",
      "tensor(0.2412, device='cuda:0', grad_fn=<KlDivBackward>)\n",
      "tensor(0.2108, device='cuda:0', grad_fn=<KlDivBackward>)\n",
      "tensor(0.2057, device='cuda:0', grad_fn=<KlDivBackward>)\n",
      "Train Epoch: 4 [40/67 (56%)]\tLoss: 0.205677\n",
      "tensor(0.1866, device='cuda:0', grad_fn=<KlDivBackward>)\n",
      "tensor(0.2629, device='cuda:0', grad_fn=<KlDivBackward>)\n",
      "tensor(0.2692, device='cuda:0', grad_fn=<KlDivBackward>)\n",
      "tensor([[-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094]], device='cuda:0',\n",
      "       grad_fn=<LogSoftmaxBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.0157, 0.0426, 0.0426, 0.0426, 0.8564],\n",
      "        [0.0408, 0.0150, 0.8185, 0.0150, 0.1108],\n",
      "        [0.1643, 0.0604, 0.4466, 0.1643, 0.1643],\n",
      "        [0.0117, 0.0861, 0.6364, 0.2341, 0.0317],\n",
      "        [0.0388, 0.0388, 0.7784, 0.1053, 0.0388],\n",
      "        [0.3483, 0.3483, 0.0471, 0.1281, 0.1281],\n",
      "        [0.1441, 0.3917, 0.0530, 0.0195, 0.3917],\n",
      "        [0.0317, 0.0861, 0.0117, 0.2341, 0.6364]], device='cuda:0')\n",
      "\n",
      "\n",
      "tensor([[-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094]], device='cuda:0',\n",
      "       grad_fn=<LogSoftmaxBackward>)\n",
      "tensor([[0.0146, 0.0397, 0.7979, 0.1080, 0.0397],\n",
      "        [0.0202, 0.1491, 0.4053, 0.0202, 0.4053],\n",
      "        [0.1915, 0.0705, 0.5206, 0.0259, 0.1915],\n",
      "        [0.6869, 0.0930, 0.0930, 0.0342, 0.0930],\n",
      "        [0.0161, 0.0438, 0.8801, 0.0438, 0.0161],\n",
      "        [0.0047, 0.0345, 0.6931, 0.0127, 0.2550],\n",
      "        [0.0161, 0.0438, 0.0438, 0.0161, 0.8801],\n",
      "        [0.1044, 0.0141, 0.0052, 0.1044, 0.7718]], device='cuda:0')\n",
      "\n",
      "\n",
      "\n",
      "Test set: Accuracy: 2/16 (12%)\n",
      "\n",
      "training epoch 5\n",
      "tensor(0.2013, device='cuda:0', grad_fn=<KlDivBackward>)\n",
      "Train Epoch: 5 [0/67 (0%)]\tLoss: 0.201285\n",
      "tensor(0.2623, device='cuda:0', grad_fn=<KlDivBackward>)\n",
      "tensor(0.2075, device='cuda:0', grad_fn=<KlDivBackward>)\n",
      "tensor(0.1730, device='cuda:0', grad_fn=<KlDivBackward>)\n",
      "tensor(0.2480, device='cuda:0', grad_fn=<KlDivBackward>)\n",
      "tensor(0.2455, device='cuda:0', grad_fn=<KlDivBackward>)\n",
      "Train Epoch: 5 [40/67 (56%)]\tLoss: 0.245534\n",
      "tensor(0.1767, device='cuda:0', grad_fn=<KlDivBackward>)\n",
      "tensor(0.1917, device='cuda:0', grad_fn=<KlDivBackward>)\n",
      "tensor(0.2131, device='cuda:0', grad_fn=<KlDivBackward>)\n",
      "tensor([[-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094]], device='cuda:0',\n",
      "       grad_fn=<LogSoftmaxBackward>)\n",
      "tensor([[0.0388, 0.0388, 0.7784, 0.1053, 0.0388],\n",
      "        [0.1044, 0.0141, 0.0052, 0.1044, 0.7718],\n",
      "        [0.0047, 0.0345, 0.6931, 0.0127, 0.2550],\n",
      "        [0.0202, 0.1491, 0.4053, 0.0202, 0.4053],\n",
      "        [0.6869, 0.0930, 0.0930, 0.0342, 0.0930],\n",
      "        [0.0161, 0.0438, 0.8801, 0.0438, 0.0161],\n",
      "        [0.0146, 0.0397, 0.7979, 0.1080, 0.0397],\n",
      "        [0.0117, 0.0861, 0.6364, 0.2341, 0.0317]], device='cuda:0')\n",
      "\n",
      "\n",
      "tensor([[-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094]], device='cuda:0',\n",
      "       grad_fn=<LogSoftmaxBackward>)\n",
      "tensor([[0.0408, 0.0150, 0.8185, 0.0150, 0.1108],\n",
      "        [0.1441, 0.3917, 0.0530, 0.0195, 0.3917],\n",
      "        [0.1643, 0.0604, 0.4466, 0.1643, 0.1643],\n",
      "        [0.1915, 0.0705, 0.5206, 0.0259, 0.1915],\n",
      "        [0.0157, 0.0426, 0.0426, 0.0426, 0.8564],\n",
      "        [0.0317, 0.0861, 0.0117, 0.2341, 0.6364],\n",
      "        [0.0161, 0.0438, 0.0438, 0.0161, 0.8801],\n",
      "        [0.3483, 0.3483, 0.0471, 0.1281, 0.1281]], device='cuda:0')\n",
      "\n",
      "\n",
      "\n",
      "Test set: Accuracy: 2/16 (12%)\n",
      "\n",
      "training epoch 6\n",
      "tensor(0.1873, device='cuda:0', grad_fn=<KlDivBackward>)\n",
      "Train Epoch: 6 [0/67 (0%)]\tLoss: 0.187296\n",
      "tensor(0.2318, device='cuda:0', grad_fn=<KlDivBackward>)\n",
      "tensor(0.2514, device='cuda:0', grad_fn=<KlDivBackward>)\n",
      "tensor(0.1967, device='cuda:0', grad_fn=<KlDivBackward>)\n",
      "tensor(0.2093, device='cuda:0', grad_fn=<KlDivBackward>)\n",
      "tensor(0.1830, device='cuda:0', grad_fn=<KlDivBackward>)\n",
      "Train Epoch: 6 [40/67 (56%)]\tLoss: 0.183048\n",
      "tensor(0.1916, device='cuda:0', grad_fn=<KlDivBackward>)\n",
      "tensor(0.2293, device='cuda:0', grad_fn=<KlDivBackward>)\n",
      "tensor(0.2812, device='cuda:0', grad_fn=<KlDivBackward>)\n",
      "tensor([[-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094]], device='cuda:0',\n",
      "       grad_fn=<LogSoftmaxBackward>)\n",
      "tensor([[0.1915, 0.0705, 0.5206, 0.0259, 0.1915],\n",
      "        [0.1643, 0.0604, 0.4466, 0.1643, 0.1643],\n",
      "        [0.0388, 0.0388, 0.7784, 0.1053, 0.0388],\n",
      "        [0.0317, 0.0861, 0.0117, 0.2341, 0.6364],\n",
      "        [0.0202, 0.1491, 0.4053, 0.0202, 0.4053],\n",
      "        [0.0408, 0.0150, 0.8185, 0.0150, 0.1108],\n",
      "        [0.1044, 0.0141, 0.0052, 0.1044, 0.7718],\n",
      "        [0.0161, 0.0438, 0.0438, 0.0161, 0.8801]], device='cuda:0')\n",
      "\n",
      "\n",
      "tensor([[-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094]], device='cuda:0',\n",
      "       grad_fn=<LogSoftmaxBackward>)\n",
      "tensor([[0.6869, 0.0930, 0.0930, 0.0342, 0.0930],\n",
      "        [0.3483, 0.3483, 0.0471, 0.1281, 0.1281],\n",
      "        [0.0146, 0.0397, 0.7979, 0.1080, 0.0397],\n",
      "        [0.0157, 0.0426, 0.0426, 0.0426, 0.8564],\n",
      "        [0.0161, 0.0438, 0.8801, 0.0438, 0.0161],\n",
      "        [0.0117, 0.0861, 0.6364, 0.2341, 0.0317],\n",
      "        [0.0047, 0.0345, 0.6931, 0.0127, 0.2550],\n",
      "        [0.1441, 0.3917, 0.0530, 0.0195, 0.3917]], device='cuda:0')\n",
      "\n",
      "\n",
      "\n",
      "Test set: Accuracy: 2/16 (12%)\n",
      "\n",
      "training epoch 7\n",
      "tensor(0.1903, device='cuda:0', grad_fn=<KlDivBackward>)\n",
      "Train Epoch: 7 [0/67 (0%)]\tLoss: 0.190341\n",
      "tensor(0.1706, device='cuda:0', grad_fn=<KlDivBackward>)\n",
      "tensor(0.2199, device='cuda:0', grad_fn=<KlDivBackward>)\n",
      "tensor(0.2443, device='cuda:0', grad_fn=<KlDivBackward>)\n",
      "tensor(0.2392, device='cuda:0', grad_fn=<KlDivBackward>)\n",
      "tensor(0.2268, device='cuda:0', grad_fn=<KlDivBackward>)\n",
      "Train Epoch: 7 [40/67 (56%)]\tLoss: 0.226801\n",
      "tensor(0.1782, device='cuda:0', grad_fn=<KlDivBackward>)\n",
      "tensor(0.2105, device='cuda:0', grad_fn=<KlDivBackward>)\n",
      "tensor(0.2826, device='cuda:0', grad_fn=<KlDivBackward>)\n",
      "tensor([[-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094]], device='cuda:0',\n",
      "       grad_fn=<LogSoftmaxBackward>)\n",
      "tensor([[0.0161, 0.0438, 0.0438, 0.0161, 0.8801],\n",
      "        [0.0047, 0.0345, 0.6931, 0.0127, 0.2550],\n",
      "        [0.0388, 0.0388, 0.7784, 0.1053, 0.0388],\n",
      "        [0.0117, 0.0861, 0.6364, 0.2341, 0.0317],\n",
      "        [0.1044, 0.0141, 0.0052, 0.1044, 0.7718],\n",
      "        [0.0146, 0.0397, 0.7979, 0.1080, 0.0397],\n",
      "        [0.0161, 0.0438, 0.8801, 0.0438, 0.0161],\n",
      "        [0.0317, 0.0861, 0.0117, 0.2341, 0.6364]], device='cuda:0')\n",
      "\n",
      "\n",
      "tensor([[-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094]], device='cuda:0',\n",
      "       grad_fn=<LogSoftmaxBackward>)\n",
      "tensor([[0.0408, 0.0150, 0.8185, 0.0150, 0.1108],\n",
      "        [0.3483, 0.3483, 0.0471, 0.1281, 0.1281],\n",
      "        [0.1643, 0.0604, 0.4466, 0.1643, 0.1643],\n",
      "        [0.6869, 0.0930, 0.0930, 0.0342, 0.0930],\n",
      "        [0.0202, 0.1491, 0.4053, 0.0202, 0.4053],\n",
      "        [0.1915, 0.0705, 0.5206, 0.0259, 0.1915],\n",
      "        [0.1441, 0.3917, 0.0530, 0.0195, 0.3917],\n",
      "        [0.0157, 0.0426, 0.0426, 0.0426, 0.8564]], device='cuda:0')\n",
      "\n",
      "\n",
      "\n",
      "Test set: Accuracy: 2/16 (12%)\n",
      "\n",
      "training epoch 8\n",
      "tensor(0.2097, device='cuda:0', grad_fn=<KlDivBackward>)\n",
      "Train Epoch: 8 [0/67 (0%)]\tLoss: 0.209740\n",
      "tensor(0.2415, device='cuda:0', grad_fn=<KlDivBackward>)\n",
      "tensor(0.2141, device='cuda:0', grad_fn=<KlDivBackward>)\n",
      "tensor(0.2207, device='cuda:0', grad_fn=<KlDivBackward>)\n",
      "tensor(0.2168, device='cuda:0', grad_fn=<KlDivBackward>)\n",
      "tensor(0.1797, device='cuda:0', grad_fn=<KlDivBackward>)\n",
      "Train Epoch: 8 [40/67 (56%)]\tLoss: 0.179718\n",
      "tensor(0.2220, device='cuda:0', grad_fn=<KlDivBackward>)\n",
      "tensor(0.1834, device='cuda:0', grad_fn=<KlDivBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.2612, device='cuda:0', grad_fn=<KlDivBackward>)\n",
      "tensor([[-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094]], device='cuda:0',\n",
      "       grad_fn=<LogSoftmaxBackward>)\n",
      "tensor([[0.1441, 0.3917, 0.0530, 0.0195, 0.3917],\n",
      "        [0.0047, 0.0345, 0.6931, 0.0127, 0.2550],\n",
      "        [0.0388, 0.0388, 0.7784, 0.1053, 0.0388],\n",
      "        [0.1643, 0.0604, 0.4466, 0.1643, 0.1643],\n",
      "        [0.0146, 0.0397, 0.7979, 0.1080, 0.0397],\n",
      "        [0.0408, 0.0150, 0.8185, 0.0150, 0.1108],\n",
      "        [0.1915, 0.0705, 0.5206, 0.0259, 0.1915],\n",
      "        [0.1044, 0.0141, 0.0052, 0.1044, 0.7718]], device='cuda:0')\n",
      "\n",
      "\n",
      "tensor([[-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094]], device='cuda:0',\n",
      "       grad_fn=<LogSoftmaxBackward>)\n",
      "tensor([[0.0161, 0.0438, 0.0438, 0.0161, 0.8801],\n",
      "        [0.0161, 0.0438, 0.8801, 0.0438, 0.0161],\n",
      "        [0.0117, 0.0861, 0.6364, 0.2341, 0.0317],\n",
      "        [0.6869, 0.0930, 0.0930, 0.0342, 0.0930],\n",
      "        [0.3483, 0.3483, 0.0471, 0.1281, 0.1281],\n",
      "        [0.0157, 0.0426, 0.0426, 0.0426, 0.8564],\n",
      "        [0.0202, 0.1491, 0.4053, 0.0202, 0.4053],\n",
      "        [0.0317, 0.0861, 0.0117, 0.2341, 0.6364]], device='cuda:0')\n",
      "\n",
      "\n",
      "\n",
      "Test set: Accuracy: 2/16 (12%)\n",
      "\n",
      "training epoch 9\n",
      "tensor(0.2071, device='cuda:0', grad_fn=<KlDivBackward>)\n",
      "Train Epoch: 9 [0/67 (0%)]\tLoss: 0.207077\n",
      "tensor(0.2302, device='cuda:0', grad_fn=<KlDivBackward>)\n",
      "tensor(0.2188, device='cuda:0', grad_fn=<KlDivBackward>)\n",
      "tensor(0.1867, device='cuda:0', grad_fn=<KlDivBackward>)\n",
      "tensor(0.1886, device='cuda:0', grad_fn=<KlDivBackward>)\n",
      "tensor(0.1881, device='cuda:0', grad_fn=<KlDivBackward>)\n",
      "Train Epoch: 9 [40/67 (56%)]\tLoss: 0.188116\n",
      "tensor(0.2445, device='cuda:0', grad_fn=<KlDivBackward>)\n",
      "tensor(0.2537, device='cuda:0', grad_fn=<KlDivBackward>)\n",
      "tensor(0.1822, device='cuda:0', grad_fn=<KlDivBackward>)\n",
      "tensor([[-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094]], device='cuda:0',\n",
      "       grad_fn=<LogSoftmaxBackward>)\n",
      "tensor([[0.0117, 0.0861, 0.6364, 0.2341, 0.0317],\n",
      "        [0.0161, 0.0438, 0.8801, 0.0438, 0.0161],\n",
      "        [0.1441, 0.3917, 0.0530, 0.0195, 0.3917],\n",
      "        [0.6869, 0.0930, 0.0930, 0.0342, 0.0930],\n",
      "        [0.1643, 0.0604, 0.4466, 0.1643, 0.1643],\n",
      "        [0.3483, 0.3483, 0.0471, 0.1281, 0.1281],\n",
      "        [0.1915, 0.0705, 0.5206, 0.0259, 0.1915],\n",
      "        [0.0202, 0.1491, 0.4053, 0.0202, 0.4053]], device='cuda:0')\n",
      "\n",
      "\n",
      "tensor([[-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094]], device='cuda:0',\n",
      "       grad_fn=<LogSoftmaxBackward>)\n",
      "tensor([[0.0317, 0.0861, 0.0117, 0.2341, 0.6364],\n",
      "        [0.1044, 0.0141, 0.0052, 0.1044, 0.7718],\n",
      "        [0.0146, 0.0397, 0.7979, 0.1080, 0.0397],\n",
      "        [0.0408, 0.0150, 0.8185, 0.0150, 0.1108],\n",
      "        [0.0047, 0.0345, 0.6931, 0.0127, 0.2550],\n",
      "        [0.0388, 0.0388, 0.7784, 0.1053, 0.0388],\n",
      "        [0.0161, 0.0438, 0.0438, 0.0161, 0.8801],\n",
      "        [0.0157, 0.0426, 0.0426, 0.0426, 0.8564]], device='cuda:0')\n",
      "\n",
      "\n",
      "\n",
      "Test set: Accuracy: 2/16 (12%)\n",
      "\n",
      "training epoch 10\n",
      "tensor(0.2281, device='cuda:0', grad_fn=<KlDivBackward>)\n",
      "Train Epoch: 10 [0/67 (0%)]\tLoss: 0.228088\n",
      "tensor(0.2244, device='cuda:0', grad_fn=<KlDivBackward>)\n",
      "tensor(0.2411, device='cuda:0', grad_fn=<KlDivBackward>)\n",
      "tensor(0.1478, device='cuda:0', grad_fn=<KlDivBackward>)\n",
      "tensor(0.2146, device='cuda:0', grad_fn=<KlDivBackward>)\n",
      "tensor(0.2343, device='cuda:0', grad_fn=<KlDivBackward>)\n",
      "Train Epoch: 10 [40/67 (56%)]\tLoss: 0.234283\n",
      "tensor(0.1864, device='cuda:0', grad_fn=<KlDivBackward>)\n",
      "tensor(0.2192, device='cuda:0', grad_fn=<KlDivBackward>)\n",
      "tensor(0.2405, device='cuda:0', grad_fn=<KlDivBackward>)\n",
      "tensor([[-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094]], device='cuda:0',\n",
      "       grad_fn=<LogSoftmaxBackward>)\n",
      "tensor([[0.0161, 0.0438, 0.8801, 0.0438, 0.0161],\n",
      "        [0.0161, 0.0438, 0.0438, 0.0161, 0.8801],\n",
      "        [0.0117, 0.0861, 0.6364, 0.2341, 0.0317],\n",
      "        [0.0146, 0.0397, 0.7979, 0.1080, 0.0397],\n",
      "        [0.0388, 0.0388, 0.7784, 0.1053, 0.0388],\n",
      "        [0.1044, 0.0141, 0.0052, 0.1044, 0.7718],\n",
      "        [0.0202, 0.1491, 0.4053, 0.0202, 0.4053],\n",
      "        [0.1441, 0.3917, 0.0530, 0.0195, 0.3917]], device='cuda:0')\n",
      "\n",
      "\n",
      "tensor([[-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094]], device='cuda:0',\n",
      "       grad_fn=<LogSoftmaxBackward>)\n",
      "tensor([[0.0317, 0.0861, 0.0117, 0.2341, 0.6364],\n",
      "        [0.0157, 0.0426, 0.0426, 0.0426, 0.8564],\n",
      "        [0.1643, 0.0604, 0.4466, 0.1643, 0.1643],\n",
      "        [0.6869, 0.0930, 0.0930, 0.0342, 0.0930],\n",
      "        [0.1915, 0.0705, 0.5206, 0.0259, 0.1915],\n",
      "        [0.0047, 0.0345, 0.6931, 0.0127, 0.2550],\n",
      "        [0.0408, 0.0150, 0.8185, 0.0150, 0.1108],\n",
      "        [0.3483, 0.3483, 0.0471, 0.1281, 0.1281]], device='cuda:0')\n",
      "\n",
      "\n",
      "\n",
      "Test set: Accuracy: 2/16 (12%)\n",
      "\n",
      "training epoch 11\n",
      "tensor(0.2424, device='cuda:0', grad_fn=<KlDivBackward>)\n",
      "Train Epoch: 11 [0/67 (0%)]\tLoss: 0.242391\n",
      "tensor(0.2281, device='cuda:0', grad_fn=<KlDivBackward>)\n",
      "tensor(0.2231, device='cuda:0', grad_fn=<KlDivBackward>)\n",
      "tensor(0.2468, device='cuda:0', grad_fn=<KlDivBackward>)\n",
      "tensor(0.1933, device='cuda:0', grad_fn=<KlDivBackward>)\n",
      "tensor(0.2126, device='cuda:0', grad_fn=<KlDivBackward>)\n",
      "Train Epoch: 11 [40/67 (56%)]\tLoss: 0.212637\n",
      "tensor(0.1774, device='cuda:0', grad_fn=<KlDivBackward>)\n",
      "tensor(0.2116, device='cuda:0', grad_fn=<KlDivBackward>)\n",
      "tensor(0.1352, device='cuda:0', grad_fn=<KlDivBackward>)\n",
      "tensor([[-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094]], device='cuda:0',\n",
      "       grad_fn=<LogSoftmaxBackward>)\n",
      "tensor([[0.6869, 0.0930, 0.0930, 0.0342, 0.0930],\n",
      "        [0.0388, 0.0388, 0.7784, 0.1053, 0.0388],\n",
      "        [0.0202, 0.1491, 0.4053, 0.0202, 0.4053],\n",
      "        [0.1441, 0.3917, 0.0530, 0.0195, 0.3917],\n",
      "        [0.0146, 0.0397, 0.7979, 0.1080, 0.0397],\n",
      "        [0.1643, 0.0604, 0.4466, 0.1643, 0.1643],\n",
      "        [0.1044, 0.0141, 0.0052, 0.1044, 0.7718],\n",
      "        [0.3483, 0.3483, 0.0471, 0.1281, 0.1281]], device='cuda:0')\n",
      "\n",
      "\n",
      "tensor([[-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094]], device='cuda:0',\n",
      "       grad_fn=<LogSoftmaxBackward>)\n",
      "tensor([[0.1915, 0.0705, 0.5206, 0.0259, 0.1915],\n",
      "        [0.0317, 0.0861, 0.0117, 0.2341, 0.6364],\n",
      "        [0.0161, 0.0438, 0.0438, 0.0161, 0.8801],\n",
      "        [0.0161, 0.0438, 0.8801, 0.0438, 0.0161],\n",
      "        [0.0047, 0.0345, 0.6931, 0.0127, 0.2550],\n",
      "        [0.0408, 0.0150, 0.8185, 0.0150, 0.1108],\n",
      "        [0.0157, 0.0426, 0.0426, 0.0426, 0.8564],\n",
      "        [0.0117, 0.0861, 0.6364, 0.2341, 0.0317]], device='cuda:0')\n",
      "\n",
      "\n",
      "\n",
      "Test set: Accuracy: 2/16 (12%)\n",
      "\n",
      "training epoch 12\n",
      "tensor(0.1539, device='cuda:0', grad_fn=<KlDivBackward>)\n",
      "Train Epoch: 12 [0/67 (0%)]\tLoss: 0.153863\n",
      "tensor(0.2533, device='cuda:0', grad_fn=<KlDivBackward>)\n",
      "tensor(0.2136, device='cuda:0', grad_fn=<KlDivBackward>)\n",
      "tensor(0.1992, device='cuda:0', grad_fn=<KlDivBackward>)\n",
      "tensor(0.2192, device='cuda:0', grad_fn=<KlDivBackward>)\n",
      "tensor(0.1857, device='cuda:0', grad_fn=<KlDivBackward>)\n",
      "Train Epoch: 12 [40/67 (56%)]\tLoss: 0.185676\n",
      "tensor(0.2193, device='cuda:0', grad_fn=<KlDivBackward>)\n",
      "tensor(0.2340, device='cuda:0', grad_fn=<KlDivBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.2871, device='cuda:0', grad_fn=<KlDivBackward>)\n",
      "tensor([[-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094]], device='cuda:0',\n",
      "       grad_fn=<LogSoftmaxBackward>)\n",
      "tensor([[0.3483, 0.3483, 0.0471, 0.1281, 0.1281],\n",
      "        [0.1044, 0.0141, 0.0052, 0.1044, 0.7718],\n",
      "        [0.0388, 0.0388, 0.7784, 0.1053, 0.0388],\n",
      "        [0.1643, 0.0604, 0.4466, 0.1643, 0.1643],\n",
      "        [0.6869, 0.0930, 0.0930, 0.0342, 0.0930],\n",
      "        [0.0408, 0.0150, 0.8185, 0.0150, 0.1108],\n",
      "        [0.0161, 0.0438, 0.0438, 0.0161, 0.8801],\n",
      "        [0.0202, 0.1491, 0.4053, 0.0202, 0.4053]], device='cuda:0')\n",
      "\n",
      "\n",
      "tensor([[-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094]], device='cuda:0',\n",
      "       grad_fn=<LogSoftmaxBackward>)\n",
      "tensor([[0.0161, 0.0438, 0.8801, 0.0438, 0.0161],\n",
      "        [0.1915, 0.0705, 0.5206, 0.0259, 0.1915],\n",
      "        [0.0117, 0.0861, 0.6364, 0.2341, 0.0317],\n",
      "        [0.0157, 0.0426, 0.0426, 0.0426, 0.8564],\n",
      "        [0.1441, 0.3917, 0.0530, 0.0195, 0.3917],\n",
      "        [0.0317, 0.0861, 0.0117, 0.2341, 0.6364],\n",
      "        [0.0146, 0.0397, 0.7979, 0.1080, 0.0397],\n",
      "        [0.0047, 0.0345, 0.6931, 0.0127, 0.2550]], device='cuda:0')\n",
      "\n",
      "\n",
      "\n",
      "Test set: Accuracy: 2/16 (12%)\n",
      "\n",
      "training epoch 13\n",
      "tensor(0.1870, device='cuda:0', grad_fn=<KlDivBackward>)\n",
      "Train Epoch: 13 [0/67 (0%)]\tLoss: 0.187048\n",
      "tensor(0.2181, device='cuda:0', grad_fn=<KlDivBackward>)\n",
      "tensor(0.2386, device='cuda:0', grad_fn=<KlDivBackward>)\n",
      "tensor(0.1989, device='cuda:0', grad_fn=<KlDivBackward>)\n",
      "tensor(0.1774, device='cuda:0', grad_fn=<KlDivBackward>)\n",
      "tensor(0.2255, device='cuda:0', grad_fn=<KlDivBackward>)\n",
      "Train Epoch: 13 [40/67 (56%)]\tLoss: 0.225550\n",
      "tensor(0.2202, device='cuda:0', grad_fn=<KlDivBackward>)\n",
      "tensor(0.2186, device='cuda:0', grad_fn=<KlDivBackward>)\n",
      "tensor(0.2708, device='cuda:0', grad_fn=<KlDivBackward>)\n",
      "tensor([[-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094]], device='cuda:0',\n",
      "       grad_fn=<LogSoftmaxBackward>)\n",
      "tensor([[0.0388, 0.0388, 0.7784, 0.1053, 0.0388],\n",
      "        [0.6869, 0.0930, 0.0930, 0.0342, 0.0930],\n",
      "        [0.0117, 0.0861, 0.6364, 0.2341, 0.0317],\n",
      "        [0.0161, 0.0438, 0.8801, 0.0438, 0.0161],\n",
      "        [0.0317, 0.0861, 0.0117, 0.2341, 0.6364],\n",
      "        [0.1044, 0.0141, 0.0052, 0.1044, 0.7718],\n",
      "        [0.0146, 0.0397, 0.7979, 0.1080, 0.0397],\n",
      "        [0.0202, 0.1491, 0.4053, 0.0202, 0.4053]], device='cuda:0')\n",
      "\n",
      "\n",
      "tensor([[-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094]], device='cuda:0',\n",
      "       grad_fn=<LogSoftmaxBackward>)\n",
      "tensor([[0.0161, 0.0438, 0.0438, 0.0161, 0.8801],\n",
      "        [0.0408, 0.0150, 0.8185, 0.0150, 0.1108],\n",
      "        [0.3483, 0.3483, 0.0471, 0.1281, 0.1281],\n",
      "        [0.1915, 0.0705, 0.5206, 0.0259, 0.1915],\n",
      "        [0.1441, 0.3917, 0.0530, 0.0195, 0.3917],\n",
      "        [0.0047, 0.0345, 0.6931, 0.0127, 0.2550],\n",
      "        [0.1643, 0.0604, 0.4466, 0.1643, 0.1643],\n",
      "        [0.0157, 0.0426, 0.0426, 0.0426, 0.8564]], device='cuda:0')\n",
      "\n",
      "\n",
      "\n",
      "Test set: Accuracy: 2/16 (12%)\n",
      "\n",
      "training epoch 14\n",
      "tensor(0.2650, device='cuda:0', grad_fn=<KlDivBackward>)\n",
      "Train Epoch: 14 [0/67 (0%)]\tLoss: 0.264997\n",
      "tensor(0.1721, device='cuda:0', grad_fn=<KlDivBackward>)\n",
      "tensor(0.1587, device='cuda:0', grad_fn=<KlDivBackward>)\n",
      "tensor(0.2583, device='cuda:0', grad_fn=<KlDivBackward>)\n",
      "tensor(0.1888, device='cuda:0', grad_fn=<KlDivBackward>)\n",
      "tensor(0.2208, device='cuda:0', grad_fn=<KlDivBackward>)\n",
      "Train Epoch: 14 [40/67 (56%)]\tLoss: 0.220799\n",
      "tensor(0.2006, device='cuda:0', grad_fn=<KlDivBackward>)\n",
      "tensor(0.2218, device='cuda:0', grad_fn=<KlDivBackward>)\n",
      "tensor(0.2665, device='cuda:0', grad_fn=<KlDivBackward>)\n",
      "tensor([[-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094]], device='cuda:0',\n",
      "       grad_fn=<LogSoftmaxBackward>)\n",
      "tensor([[0.0161, 0.0438, 0.0438, 0.0161, 0.8801],\n",
      "        [0.1441, 0.3917, 0.0530, 0.0195, 0.3917],\n",
      "        [0.0117, 0.0861, 0.6364, 0.2341, 0.0317],\n",
      "        [0.0047, 0.0345, 0.6931, 0.0127, 0.2550],\n",
      "        [0.0157, 0.0426, 0.0426, 0.0426, 0.8564],\n",
      "        [0.0388, 0.0388, 0.7784, 0.1053, 0.0388],\n",
      "        [0.0202, 0.1491, 0.4053, 0.0202, 0.4053],\n",
      "        [0.1915, 0.0705, 0.5206, 0.0259, 0.1915]], device='cuda:0')\n",
      "\n",
      "\n",
      "tensor([[-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094]], device='cuda:0',\n",
      "       grad_fn=<LogSoftmaxBackward>)\n",
      "tensor([[0.0146, 0.0397, 0.7979, 0.1080, 0.0397],\n",
      "        [0.0408, 0.0150, 0.8185, 0.0150, 0.1108],\n",
      "        [0.6869, 0.0930, 0.0930, 0.0342, 0.0930],\n",
      "        [0.1643, 0.0604, 0.4466, 0.1643, 0.1643],\n",
      "        [0.1044, 0.0141, 0.0052, 0.1044, 0.7718],\n",
      "        [0.0317, 0.0861, 0.0117, 0.2341, 0.6364],\n",
      "        [0.3483, 0.3483, 0.0471, 0.1281, 0.1281],\n",
      "        [0.0161, 0.0438, 0.8801, 0.0438, 0.0161]], device='cuda:0')\n",
      "\n",
      "\n",
      "\n",
      "Test set: Accuracy: 2/16 (12%)\n",
      "\n",
      "training epoch 15\n",
      "tensor(0.2169, device='cuda:0', grad_fn=<KlDivBackward>)\n",
      "Train Epoch: 15 [0/67 (0%)]\tLoss: 0.216890\n",
      "tensor(0.2269, device='cuda:0', grad_fn=<KlDivBackward>)\n",
      "tensor(0.1738, device='cuda:0', grad_fn=<KlDivBackward>)\n",
      "tensor(0.2107, device='cuda:0', grad_fn=<KlDivBackward>)\n",
      "tensor(0.1766, device='cuda:0', grad_fn=<KlDivBackward>)\n",
      "tensor(0.2445, device='cuda:0', grad_fn=<KlDivBackward>)\n",
      "Train Epoch: 15 [40/67 (56%)]\tLoss: 0.244491\n",
      "tensor(0.1787, device='cuda:0', grad_fn=<KlDivBackward>)\n",
      "tensor(0.2725, device='cuda:0', grad_fn=<KlDivBackward>)\n",
      "tensor(0.2275, device='cuda:0', grad_fn=<KlDivBackward>)\n",
      "tensor([[-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094]], device='cuda:0',\n",
      "       grad_fn=<LogSoftmaxBackward>)\n",
      "tensor([[0.1643, 0.0604, 0.4466, 0.1643, 0.1643],\n",
      "        [0.1044, 0.0141, 0.0052, 0.1044, 0.7718],\n",
      "        [0.0317, 0.0861, 0.0117, 0.2341, 0.6364],\n",
      "        [0.3483, 0.3483, 0.0471, 0.1281, 0.1281],\n",
      "        [0.0146, 0.0397, 0.7979, 0.1080, 0.0397],\n",
      "        [0.0161, 0.0438, 0.8801, 0.0438, 0.0161],\n",
      "        [0.0408, 0.0150, 0.8185, 0.0150, 0.1108],\n",
      "        [0.1441, 0.3917, 0.0530, 0.0195, 0.3917]], device='cuda:0')\n",
      "\n",
      "\n",
      "tensor([[-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094]], device='cuda:0',\n",
      "       grad_fn=<LogSoftmaxBackward>)\n",
      "tensor([[0.0047, 0.0345, 0.6931, 0.0127, 0.2550],\n",
      "        [0.1915, 0.0705, 0.5206, 0.0259, 0.1915],\n",
      "        [0.6869, 0.0930, 0.0930, 0.0342, 0.0930],\n",
      "        [0.0161, 0.0438, 0.0438, 0.0161, 0.8801],\n",
      "        [0.0117, 0.0861, 0.6364, 0.2341, 0.0317],\n",
      "        [0.0388, 0.0388, 0.7784, 0.1053, 0.0388],\n",
      "        [0.0157, 0.0426, 0.0426, 0.0426, 0.8564],\n",
      "        [0.0202, 0.1491, 0.4053, 0.0202, 0.4053]], device='cuda:0')\n",
      "\n",
      "\n",
      "\n",
      "Test set: Accuracy: 2/16 (12%)\n",
      "\n",
      "training epoch 16\n",
      "tensor(0.2102, device='cuda:0', grad_fn=<KlDivBackward>)\n",
      "Train Epoch: 16 [0/67 (0%)]\tLoss: 0.210184\n",
      "tensor(0.2272, device='cuda:0', grad_fn=<KlDivBackward>)\n",
      "tensor(0.2244, device='cuda:0', grad_fn=<KlDivBackward>)\n",
      "tensor(0.1855, device='cuda:0', grad_fn=<KlDivBackward>)\n",
      "tensor(0.2132, device='cuda:0', grad_fn=<KlDivBackward>)\n",
      "tensor(0.2212, device='cuda:0', grad_fn=<KlDivBackward>)\n",
      "Train Epoch: 16 [40/67 (56%)]\tLoss: 0.221202\n",
      "tensor(0.2642, device='cuda:0', grad_fn=<KlDivBackward>)\n",
      "tensor(0.1881, device='cuda:0', grad_fn=<KlDivBackward>)\n",
      "tensor(0.1388, device='cuda:0', grad_fn=<KlDivBackward>)\n",
      "tensor([[-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094]], device='cuda:0',\n",
      "       grad_fn=<LogSoftmaxBackward>)\n",
      "tensor([[0.0161, 0.0438, 0.8801, 0.0438, 0.0161],\n",
      "        [0.0146, 0.0397, 0.7979, 0.1080, 0.0397],\n",
      "        [0.0047, 0.0345, 0.6931, 0.0127, 0.2550],\n",
      "        [0.0157, 0.0426, 0.0426, 0.0426, 0.8564],\n",
      "        [0.0161, 0.0438, 0.0438, 0.0161, 0.8801],\n",
      "        [0.0202, 0.1491, 0.4053, 0.0202, 0.4053],\n",
      "        [0.1643, 0.0604, 0.4466, 0.1643, 0.1643],\n",
      "        [0.0317, 0.0861, 0.0117, 0.2341, 0.6364]], device='cuda:0')\n",
      "\n",
      "\n",
      "tensor([[-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094]], device='cuda:0',\n",
      "       grad_fn=<LogSoftmaxBackward>)\n",
      "tensor([[0.0117, 0.0861, 0.6364, 0.2341, 0.0317],\n",
      "        [0.0388, 0.0388, 0.7784, 0.1053, 0.0388],\n",
      "        [0.6869, 0.0930, 0.0930, 0.0342, 0.0930],\n",
      "        [0.3483, 0.3483, 0.0471, 0.1281, 0.1281],\n",
      "        [0.1915, 0.0705, 0.5206, 0.0259, 0.1915],\n",
      "        [0.0408, 0.0150, 0.8185, 0.0150, 0.1108],\n",
      "        [0.1441, 0.3917, 0.0530, 0.0195, 0.3917],\n",
      "        [0.1044, 0.0141, 0.0052, 0.1044, 0.7718]], device='cuda:0')\n",
      "\n",
      "\n",
      "\n",
      "Test set: Accuracy: 2/16 (12%)\n",
      "\n",
      "training epoch 17\n",
      "tensor(0.1922, device='cuda:0', grad_fn=<KlDivBackward>)\n",
      "Train Epoch: 17 [0/67 (0%)]\tLoss: 0.192248\n",
      "tensor(0.1944, device='cuda:0', grad_fn=<KlDivBackward>)\n",
      "tensor(0.1930, device='cuda:0', grad_fn=<KlDivBackward>)\n",
      "tensor(0.2242, device='cuda:0', grad_fn=<KlDivBackward>)\n",
      "tensor(0.1771, device='cuda:0', grad_fn=<KlDivBackward>)\n",
      "tensor(0.2296, device='cuda:0', grad_fn=<KlDivBackward>)\n",
      "Train Epoch: 17 [40/67 (56%)]\tLoss: 0.229599\n",
      "tensor(0.2566, device='cuda:0', grad_fn=<KlDivBackward>)\n",
      "tensor(0.2145, device='cuda:0', grad_fn=<KlDivBackward>)\n",
      "tensor(0.2781, device='cuda:0', grad_fn=<KlDivBackward>)\n",
      "tensor([[-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094]], device='cuda:0',\n",
      "       grad_fn=<LogSoftmaxBackward>)\n",
      "tensor([[0.0388, 0.0388, 0.7784, 0.1053, 0.0388],\n",
      "        [0.1044, 0.0141, 0.0052, 0.1044, 0.7718],\n",
      "        [0.0408, 0.0150, 0.8185, 0.0150, 0.1108],\n",
      "        [0.0146, 0.0397, 0.7979, 0.1080, 0.0397],\n",
      "        [0.6869, 0.0930, 0.0930, 0.0342, 0.0930],\n",
      "        [0.1643, 0.0604, 0.4466, 0.1643, 0.1643],\n",
      "        [0.0202, 0.1491, 0.4053, 0.0202, 0.4053],\n",
      "        [0.0161, 0.0438, 0.8801, 0.0438, 0.0161]], device='cuda:0')\n",
      "\n",
      "\n",
      "tensor([[-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094]], device='cuda:0',\n",
      "       grad_fn=<LogSoftmaxBackward>)\n",
      "tensor([[0.0117, 0.0861, 0.6364, 0.2341, 0.0317],\n",
      "        [0.0317, 0.0861, 0.0117, 0.2341, 0.6364],\n",
      "        [0.0161, 0.0438, 0.0438, 0.0161, 0.8801],\n",
      "        [0.0157, 0.0426, 0.0426, 0.0426, 0.8564],\n",
      "        [0.1915, 0.0705, 0.5206, 0.0259, 0.1915],\n",
      "        [0.1441, 0.3917, 0.0530, 0.0195, 0.3917],\n",
      "        [0.0047, 0.0345, 0.6931, 0.0127, 0.2550],\n",
      "        [0.3483, 0.3483, 0.0471, 0.1281, 0.1281]], device='cuda:0')\n",
      "\n",
      "\n",
      "\n",
      "Test set: Accuracy: 2/16 (12%)\n",
      "\n",
      "training epoch 18\n",
      "tensor(0.2357, device='cuda:0', grad_fn=<KlDivBackward>)\n",
      "Train Epoch: 18 [0/67 (0%)]\tLoss: 0.235656\n",
      "tensor(0.2051, device='cuda:0', grad_fn=<KlDivBackward>)\n",
      "tensor(0.2315, device='cuda:0', grad_fn=<KlDivBackward>)\n",
      "tensor(0.1829, device='cuda:0', grad_fn=<KlDivBackward>)\n",
      "tensor(0.2009, device='cuda:0', grad_fn=<KlDivBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.2292, device='cuda:0', grad_fn=<KlDivBackward>)\n",
      "Train Epoch: 18 [40/67 (56%)]\tLoss: 0.229240\n",
      "tensor(0.2662, device='cuda:0', grad_fn=<KlDivBackward>)\n",
      "tensor(0.1612, device='cuda:0', grad_fn=<KlDivBackward>)\n",
      "tensor(0.1955, device='cuda:0', grad_fn=<KlDivBackward>)\n",
      "tensor([[-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094]], device='cuda:0',\n",
      "       grad_fn=<LogSoftmaxBackward>)\n",
      "tensor([[0.0317, 0.0861, 0.0117, 0.2341, 0.6364],\n",
      "        [0.1643, 0.0604, 0.4466, 0.1643, 0.1643],\n",
      "        [0.0408, 0.0150, 0.8185, 0.0150, 0.1108],\n",
      "        [0.0202, 0.1491, 0.4053, 0.0202, 0.4053],\n",
      "        [0.0161, 0.0438, 0.8801, 0.0438, 0.0161],\n",
      "        [0.0047, 0.0345, 0.6931, 0.0127, 0.2550],\n",
      "        [0.0117, 0.0861, 0.6364, 0.2341, 0.0317],\n",
      "        [0.0146, 0.0397, 0.7979, 0.1080, 0.0397]], device='cuda:0')\n",
      "\n",
      "\n",
      "tensor([[-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094]], device='cuda:0',\n",
      "       grad_fn=<LogSoftmaxBackward>)\n",
      "tensor([[0.1044, 0.0141, 0.0052, 0.1044, 0.7718],\n",
      "        [0.0157, 0.0426, 0.0426, 0.0426, 0.8564],\n",
      "        [0.1915, 0.0705, 0.5206, 0.0259, 0.1915],\n",
      "        [0.0161, 0.0438, 0.0438, 0.0161, 0.8801],\n",
      "        [0.6869, 0.0930, 0.0930, 0.0342, 0.0930],\n",
      "        [0.3483, 0.3483, 0.0471, 0.1281, 0.1281],\n",
      "        [0.1441, 0.3917, 0.0530, 0.0195, 0.3917],\n",
      "        [0.0388, 0.0388, 0.7784, 0.1053, 0.0388]], device='cuda:0')\n",
      "\n",
      "\n",
      "\n",
      "Test set: Accuracy: 2/16 (12%)\n",
      "\n",
      "training epoch 19\n",
      "tensor(0.2322, device='cuda:0', grad_fn=<KlDivBackward>)\n",
      "Train Epoch: 19 [0/67 (0%)]\tLoss: 0.232161\n",
      "tensor(0.2450, device='cuda:0', grad_fn=<KlDivBackward>)\n",
      "tensor(0.1878, device='cuda:0', grad_fn=<KlDivBackward>)\n",
      "tensor(0.2311, device='cuda:0', grad_fn=<KlDivBackward>)\n",
      "tensor(0.1982, device='cuda:0', grad_fn=<KlDivBackward>)\n",
      "tensor(0.2110, device='cuda:0', grad_fn=<KlDivBackward>)\n",
      "Train Epoch: 19 [40/67 (56%)]\tLoss: 0.211016\n",
      "tensor(0.2335, device='cuda:0', grad_fn=<KlDivBackward>)\n",
      "tensor(0.2053, device='cuda:0', grad_fn=<KlDivBackward>)\n",
      "tensor(0.1115, device='cuda:0', grad_fn=<KlDivBackward>)\n",
      "tensor([[-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094]], device='cuda:0',\n",
      "       grad_fn=<LogSoftmaxBackward>)\n",
      "tensor([[0.0388, 0.0388, 0.7784, 0.1053, 0.0388],\n",
      "        [0.3483, 0.3483, 0.0471, 0.1281, 0.1281],\n",
      "        [0.0047, 0.0345, 0.6931, 0.0127, 0.2550],\n",
      "        [0.0202, 0.1491, 0.4053, 0.0202, 0.4053],\n",
      "        [0.0161, 0.0438, 0.0438, 0.0161, 0.8801],\n",
      "        [0.0408, 0.0150, 0.8185, 0.0150, 0.1108],\n",
      "        [0.1441, 0.3917, 0.0530, 0.0195, 0.3917],\n",
      "        [0.0157, 0.0426, 0.0426, 0.0426, 0.8564]], device='cuda:0')\n",
      "\n",
      "\n",
      "tensor([[-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094]], device='cuda:0',\n",
      "       grad_fn=<LogSoftmaxBackward>)\n",
      "tensor([[0.6869, 0.0930, 0.0930, 0.0342, 0.0930],\n",
      "        [0.0117, 0.0861, 0.6364, 0.2341, 0.0317],\n",
      "        [0.0161, 0.0438, 0.8801, 0.0438, 0.0161],\n",
      "        [0.0146, 0.0397, 0.7979, 0.1080, 0.0397],\n",
      "        [0.1643, 0.0604, 0.4466, 0.1643, 0.1643],\n",
      "        [0.0317, 0.0861, 0.0117, 0.2341, 0.6364],\n",
      "        [0.1044, 0.0141, 0.0052, 0.1044, 0.7718],\n",
      "        [0.1915, 0.0705, 0.5206, 0.0259, 0.1915]], device='cuda:0')\n",
      "\n",
      "\n",
      "\n",
      "Test set: Accuracy: 2/16 (12%)\n",
      "\n",
      "training epoch 20\n",
      "tensor(0.1787, device='cuda:0', grad_fn=<KlDivBackward>)\n",
      "Train Epoch: 20 [0/67 (0%)]\tLoss: 0.178669\n",
      "tensor(0.2671, device='cuda:0', grad_fn=<KlDivBackward>)\n",
      "tensor(0.2397, device='cuda:0', grad_fn=<KlDivBackward>)\n",
      "tensor(0.2520, device='cuda:0', grad_fn=<KlDivBackward>)\n",
      "tensor(0.1973, device='cuda:0', grad_fn=<KlDivBackward>)\n",
      "tensor(0.1718, device='cuda:0', grad_fn=<KlDivBackward>)\n",
      "Train Epoch: 20 [40/67 (56%)]\tLoss: 0.171783\n",
      "tensor(0.2058, device='cuda:0', grad_fn=<KlDivBackward>)\n",
      "tensor(0.2156, device='cuda:0', grad_fn=<KlDivBackward>)\n",
      "tensor(0.1548, device='cuda:0', grad_fn=<KlDivBackward>)\n",
      "tensor([[-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094]], device='cuda:0',\n",
      "       grad_fn=<LogSoftmaxBackward>)\n",
      "tensor([[0.0317, 0.0861, 0.0117, 0.2341, 0.6364],\n",
      "        [0.1044, 0.0141, 0.0052, 0.1044, 0.7718],\n",
      "        [0.0157, 0.0426, 0.0426, 0.0426, 0.8564],\n",
      "        [0.0146, 0.0397, 0.7979, 0.1080, 0.0397],\n",
      "        [0.0161, 0.0438, 0.0438, 0.0161, 0.8801],\n",
      "        [0.1643, 0.0604, 0.4466, 0.1643, 0.1643],\n",
      "        [0.0047, 0.0345, 0.6931, 0.0127, 0.2550],\n",
      "        [0.6869, 0.0930, 0.0930, 0.0342, 0.0930]], device='cuda:0')\n",
      "\n",
      "\n",
      "tensor([[-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094]], device='cuda:0',\n",
      "       grad_fn=<LogSoftmaxBackward>)\n",
      "tensor([[0.0388, 0.0388, 0.7784, 0.1053, 0.0388],\n",
      "        [0.3483, 0.3483, 0.0471, 0.1281, 0.1281],\n",
      "        [0.1441, 0.3917, 0.0530, 0.0195, 0.3917],\n",
      "        [0.0202, 0.1491, 0.4053, 0.0202, 0.4053],\n",
      "        [0.0117, 0.0861, 0.6364, 0.2341, 0.0317],\n",
      "        [0.1915, 0.0705, 0.5206, 0.0259, 0.1915],\n",
      "        [0.0161, 0.0438, 0.8801, 0.0438, 0.0161],\n",
      "        [0.0408, 0.0150, 0.8185, 0.0150, 0.1108]], device='cuda:0')\n",
      "\n",
      "\n",
      "\n",
      "Test set: Accuracy: 2/16 (12%)\n",
      "\n",
      "training epoch 21\n",
      "tensor(0.1903, device='cuda:0', grad_fn=<KlDivBackward>)\n",
      "Train Epoch: 21 [0/67 (0%)]\tLoss: 0.190285\n",
      "tensor(0.2145, device='cuda:0', grad_fn=<KlDivBackward>)\n",
      "tensor(0.2256, device='cuda:0', grad_fn=<KlDivBackward>)\n",
      "tensor(0.1774, device='cuda:0', grad_fn=<KlDivBackward>)\n",
      "tensor(0.2224, device='cuda:0', grad_fn=<KlDivBackward>)\n",
      "tensor(0.2520, device='cuda:0', grad_fn=<KlDivBackward>)\n",
      "Train Epoch: 21 [40/67 (56%)]\tLoss: 0.251958\n",
      "tensor(0.1294, device='cuda:0', grad_fn=<KlDivBackward>)\n",
      "tensor(0.2763, device='cuda:0', grad_fn=<KlDivBackward>)\n",
      "tensor(0.2614, device='cuda:0', grad_fn=<KlDivBackward>)\n",
      "tensor([[-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094]], device='cuda:0',\n",
      "       grad_fn=<LogSoftmaxBackward>)\n",
      "tensor([[0.0317, 0.0861, 0.0117, 0.2341, 0.6364],\n",
      "        [0.0388, 0.0388, 0.7784, 0.1053, 0.0388],\n",
      "        [0.6869, 0.0930, 0.0930, 0.0342, 0.0930],\n",
      "        [0.0202, 0.1491, 0.4053, 0.0202, 0.4053],\n",
      "        [0.0117, 0.0861, 0.6364, 0.2341, 0.0317],\n",
      "        [0.0146, 0.0397, 0.7979, 0.1080, 0.0397],\n",
      "        [0.1044, 0.0141, 0.0052, 0.1044, 0.7718],\n",
      "        [0.3483, 0.3483, 0.0471, 0.1281, 0.1281]], device='cuda:0')\n",
      "\n",
      "\n",
      "tensor([[-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094]], device='cuda:0',\n",
      "       grad_fn=<LogSoftmaxBackward>)\n",
      "tensor([[0.0161, 0.0438, 0.0438, 0.0161, 0.8801],\n",
      "        [0.0157, 0.0426, 0.0426, 0.0426, 0.8564],\n",
      "        [0.1441, 0.3917, 0.0530, 0.0195, 0.3917],\n",
      "        [0.0161, 0.0438, 0.8801, 0.0438, 0.0161],\n",
      "        [0.1643, 0.0604, 0.4466, 0.1643, 0.1643],\n",
      "        [0.0408, 0.0150, 0.8185, 0.0150, 0.1108],\n",
      "        [0.0047, 0.0345, 0.6931, 0.0127, 0.2550],\n",
      "        [0.1915, 0.0705, 0.5206, 0.0259, 0.1915]], device='cuda:0')\n",
      "\n",
      "\n",
      "\n",
      "Test set: Accuracy: 2/16 (12%)\n",
      "\n",
      "training epoch 22\n",
      "tensor(0.2306, device='cuda:0', grad_fn=<KlDivBackward>)\n",
      "Train Epoch: 22 [0/67 (0%)]\tLoss: 0.230573\n",
      "tensor(0.1575, device='cuda:0', grad_fn=<KlDivBackward>)\n",
      "tensor(0.2038, device='cuda:0', grad_fn=<KlDivBackward>)\n",
      "tensor(0.2060, device='cuda:0', grad_fn=<KlDivBackward>)\n",
      "tensor(0.2392, device='cuda:0', grad_fn=<KlDivBackward>)\n",
      "tensor(0.2139, device='cuda:0', grad_fn=<KlDivBackward>)\n",
      "Train Epoch: 22 [40/67 (56%)]\tLoss: 0.213928\n",
      "tensor(0.1983, device='cuda:0', grad_fn=<KlDivBackward>)\n",
      "tensor(0.2648, device='cuda:0', grad_fn=<KlDivBackward>)\n",
      "tensor(0.1915, device='cuda:0', grad_fn=<KlDivBackward>)\n",
      "tensor([[-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094]], device='cuda:0',\n",
      "       grad_fn=<LogSoftmaxBackward>)\n",
      "tensor([[0.0047, 0.0345, 0.6931, 0.0127, 0.2550],\n",
      "        [0.0161, 0.0438, 0.8801, 0.0438, 0.0161],\n",
      "        [0.6869, 0.0930, 0.0930, 0.0342, 0.0930],\n",
      "        [0.1441, 0.3917, 0.0530, 0.0195, 0.3917],\n",
      "        [0.1643, 0.0604, 0.4466, 0.1643, 0.1643],\n",
      "        [0.0157, 0.0426, 0.0426, 0.0426, 0.8564],\n",
      "        [0.3483, 0.3483, 0.0471, 0.1281, 0.1281],\n",
      "        [0.0317, 0.0861, 0.0117, 0.2341, 0.6364]], device='cuda:0')\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094]], device='cuda:0',\n",
      "       grad_fn=<LogSoftmaxBackward>)\n",
      "tensor([[0.0202, 0.1491, 0.4053, 0.0202, 0.4053],\n",
      "        [0.0146, 0.0397, 0.7979, 0.1080, 0.0397],\n",
      "        [0.0161, 0.0438, 0.0438, 0.0161, 0.8801],\n",
      "        [0.0408, 0.0150, 0.8185, 0.0150, 0.1108],\n",
      "        [0.0388, 0.0388, 0.7784, 0.1053, 0.0388],\n",
      "        [0.1915, 0.0705, 0.5206, 0.0259, 0.1915],\n",
      "        [0.0117, 0.0861, 0.6364, 0.2341, 0.0317],\n",
      "        [0.1044, 0.0141, 0.0052, 0.1044, 0.7718]], device='cuda:0')\n",
      "\n",
      "\n",
      "\n",
      "Test set: Accuracy: 2/16 (12%)\n",
      "\n",
      "training epoch 23\n",
      "tensor(0.1939, device='cuda:0', grad_fn=<KlDivBackward>)\n",
      "Train Epoch: 23 [0/67 (0%)]\tLoss: 0.193912\n",
      "tensor(0.1982, device='cuda:0', grad_fn=<KlDivBackward>)\n",
      "tensor(0.2139, device='cuda:0', grad_fn=<KlDivBackward>)\n",
      "tensor(0.2194, device='cuda:0', grad_fn=<KlDivBackward>)\n",
      "tensor(0.2381, device='cuda:0', grad_fn=<KlDivBackward>)\n",
      "tensor(0.2019, device='cuda:0', grad_fn=<KlDivBackward>)\n",
      "Train Epoch: 23 [40/67 (56%)]\tLoss: 0.201920\n",
      "tensor(0.2367, device='cuda:0', grad_fn=<KlDivBackward>)\n",
      "tensor(0.1996, device='cuda:0', grad_fn=<KlDivBackward>)\n",
      "tensor(0.2243, device='cuda:0', grad_fn=<KlDivBackward>)\n",
      "tensor([[-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094]], device='cuda:0',\n",
      "       grad_fn=<LogSoftmaxBackward>)\n",
      "tensor([[0.1643, 0.0604, 0.4466, 0.1643, 0.1643],\n",
      "        [0.3483, 0.3483, 0.0471, 0.1281, 0.1281],\n",
      "        [0.0146, 0.0397, 0.7979, 0.1080, 0.0397],\n",
      "        [0.0161, 0.0438, 0.8801, 0.0438, 0.0161],\n",
      "        [0.1441, 0.3917, 0.0530, 0.0195, 0.3917],\n",
      "        [0.6869, 0.0930, 0.0930, 0.0342, 0.0930],\n",
      "        [0.0388, 0.0388, 0.7784, 0.1053, 0.0388],\n",
      "        [0.0161, 0.0438, 0.0438, 0.0161, 0.8801]], device='cuda:0')\n",
      "\n",
      "\n",
      "tensor([[-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094]], device='cuda:0',\n",
      "       grad_fn=<LogSoftmaxBackward>)\n",
      "tensor([[0.0157, 0.0426, 0.0426, 0.0426, 0.8564],\n",
      "        [0.1915, 0.0705, 0.5206, 0.0259, 0.1915],\n",
      "        [0.0317, 0.0861, 0.0117, 0.2341, 0.6364],\n",
      "        [0.0047, 0.0345, 0.6931, 0.0127, 0.2550],\n",
      "        [0.1044, 0.0141, 0.0052, 0.1044, 0.7718],\n",
      "        [0.0202, 0.1491, 0.4053, 0.0202, 0.4053],\n",
      "        [0.0408, 0.0150, 0.8185, 0.0150, 0.1108],\n",
      "        [0.0117, 0.0861, 0.6364, 0.2341, 0.0317]], device='cuda:0')\n",
      "\n",
      "\n",
      "\n",
      "Test set: Accuracy: 2/16 (12%)\n",
      "\n",
      "training epoch 24\n",
      "tensor(0.2374, device='cuda:0', grad_fn=<KlDivBackward>)\n",
      "Train Epoch: 24 [0/67 (0%)]\tLoss: 0.237419\n",
      "tensor(0.2337, device='cuda:0', grad_fn=<KlDivBackward>)\n",
      "tensor(0.2274, device='cuda:0', grad_fn=<KlDivBackward>)\n",
      "tensor(0.1911, device='cuda:0', grad_fn=<KlDivBackward>)\n",
      "tensor(0.2300, device='cuda:0', grad_fn=<KlDivBackward>)\n",
      "tensor(0.1699, device='cuda:0', grad_fn=<KlDivBackward>)\n",
      "Train Epoch: 24 [40/67 (56%)]\tLoss: 0.169904\n",
      "tensor(0.2391, device='cuda:0', grad_fn=<KlDivBackward>)\n",
      "tensor(0.1964, device='cuda:0', grad_fn=<KlDivBackward>)\n",
      "tensor(0.1625, device='cuda:0', grad_fn=<KlDivBackward>)\n",
      "tensor([[-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094]], device='cuda:0',\n",
      "       grad_fn=<LogSoftmaxBackward>)\n",
      "tensor([[0.0161, 0.0438, 0.8801, 0.0438, 0.0161],\n",
      "        [0.0408, 0.0150, 0.8185, 0.0150, 0.1108],\n",
      "        [0.0047, 0.0345, 0.6931, 0.0127, 0.2550],\n",
      "        [0.1643, 0.0604, 0.4466, 0.1643, 0.1643],\n",
      "        [0.1044, 0.0141, 0.0052, 0.1044, 0.7718],\n",
      "        [0.0388, 0.0388, 0.7784, 0.1053, 0.0388],\n",
      "        [0.6869, 0.0930, 0.0930, 0.0342, 0.0930],\n",
      "        [0.0117, 0.0861, 0.6364, 0.2341, 0.0317]], device='cuda:0')\n",
      "\n",
      "\n",
      "tensor([[-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094]], device='cuda:0',\n",
      "       grad_fn=<LogSoftmaxBackward>)\n",
      "tensor([[0.0157, 0.0426, 0.0426, 0.0426, 0.8564],\n",
      "        [0.1441, 0.3917, 0.0530, 0.0195, 0.3917],\n",
      "        [0.0161, 0.0438, 0.0438, 0.0161, 0.8801],\n",
      "        [0.0146, 0.0397, 0.7979, 0.1080, 0.0397],\n",
      "        [0.0202, 0.1491, 0.4053, 0.0202, 0.4053],\n",
      "        [0.1915, 0.0705, 0.5206, 0.0259, 0.1915],\n",
      "        [0.3483, 0.3483, 0.0471, 0.1281, 0.1281],\n",
      "        [0.0317, 0.0861, 0.0117, 0.2341, 0.6364]], device='cuda:0')\n",
      "\n",
      "\n",
      "\n",
      "Test set: Accuracy: 2/16 (12%)\n",
      "\n",
      "training epoch 25\n",
      "tensor(0.1694, device='cuda:0', grad_fn=<KlDivBackward>)\n",
      "Train Epoch: 25 [0/67 (0%)]\tLoss: 0.169436\n",
      "tensor(0.2129, device='cuda:0', grad_fn=<KlDivBackward>)\n",
      "tensor(0.1686, device='cuda:0', grad_fn=<KlDivBackward>)\n",
      "tensor(0.1932, device='cuda:0', grad_fn=<KlDivBackward>)\n",
      "tensor(0.2169, device='cuda:0', grad_fn=<KlDivBackward>)\n",
      "tensor(0.2334, device='cuda:0', grad_fn=<KlDivBackward>)\n",
      "Train Epoch: 25 [40/67 (56%)]\tLoss: 0.233409\n",
      "tensor(0.2556, device='cuda:0', grad_fn=<KlDivBackward>)\n",
      "tensor(0.2478, device='cuda:0', grad_fn=<KlDivBackward>)\n",
      "tensor(0.2347, device='cuda:0', grad_fn=<KlDivBackward>)\n",
      "tensor([[-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094]], device='cuda:0',\n",
      "       grad_fn=<LogSoftmaxBackward>)\n",
      "tensor([[0.0388, 0.0388, 0.7784, 0.1053, 0.0388],\n",
      "        [0.0047, 0.0345, 0.6931, 0.0127, 0.2550],\n",
      "        [0.0202, 0.1491, 0.4053, 0.0202, 0.4053],\n",
      "        [0.0408, 0.0150, 0.8185, 0.0150, 0.1108],\n",
      "        [0.1441, 0.3917, 0.0530, 0.0195, 0.3917],\n",
      "        [0.0161, 0.0438, 0.8801, 0.0438, 0.0161],\n",
      "        [0.3483, 0.3483, 0.0471, 0.1281, 0.1281],\n",
      "        [0.1915, 0.0705, 0.5206, 0.0259, 0.1915]], device='cuda:0')\n",
      "\n",
      "\n",
      "tensor([[-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094]], device='cuda:0',\n",
      "       grad_fn=<LogSoftmaxBackward>)\n",
      "tensor([[0.0157, 0.0426, 0.0426, 0.0426, 0.8564],\n",
      "        [0.1044, 0.0141, 0.0052, 0.1044, 0.7718],\n",
      "        [0.1643, 0.0604, 0.4466, 0.1643, 0.1643],\n",
      "        [0.0317, 0.0861, 0.0117, 0.2341, 0.6364],\n",
      "        [0.0161, 0.0438, 0.0438, 0.0161, 0.8801],\n",
      "        [0.0117, 0.0861, 0.6364, 0.2341, 0.0317],\n",
      "        [0.6869, 0.0930, 0.0930, 0.0342, 0.0930],\n",
      "        [0.0146, 0.0397, 0.7979, 0.1080, 0.0397]], device='cuda:0')\n",
      "\n",
      "\n",
      "\n",
      "Test set: Accuracy: 2/16 (12%)\n",
      "\n",
      "training epoch 26\n",
      "tensor(0.2243, device='cuda:0', grad_fn=<KlDivBackward>)\n",
      "Train Epoch: 26 [0/67 (0%)]\tLoss: 0.224327\n",
      "tensor(0.2369, device='cuda:0', grad_fn=<KlDivBackward>)\n",
      "tensor(0.1662, device='cuda:0', grad_fn=<KlDivBackward>)\n",
      "tensor(0.2307, device='cuda:0', grad_fn=<KlDivBackward>)\n",
      "tensor(0.2093, device='cuda:0', grad_fn=<KlDivBackward>)\n",
      "tensor(0.1737, device='cuda:0', grad_fn=<KlDivBackward>)\n",
      "Train Epoch: 26 [40/67 (56%)]\tLoss: 0.173712\n",
      "tensor(0.2198, device='cuda:0', grad_fn=<KlDivBackward>)\n",
      "tensor(0.2313, device='cuda:0', grad_fn=<KlDivBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.2498, device='cuda:0', grad_fn=<KlDivBackward>)\n",
      "tensor([[-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094]], device='cuda:0',\n",
      "       grad_fn=<LogSoftmaxBackward>)\n",
      "tensor([[0.1643, 0.0604, 0.4466, 0.1643, 0.1643],\n",
      "        [0.1044, 0.0141, 0.0052, 0.1044, 0.7718],\n",
      "        [0.0388, 0.0388, 0.7784, 0.1053, 0.0388],\n",
      "        [0.0408, 0.0150, 0.8185, 0.0150, 0.1108],\n",
      "        [0.0202, 0.1491, 0.4053, 0.0202, 0.4053],\n",
      "        [0.6869, 0.0930, 0.0930, 0.0342, 0.0930],\n",
      "        [0.0047, 0.0345, 0.6931, 0.0127, 0.2550],\n",
      "        [0.0146, 0.0397, 0.7979, 0.1080, 0.0397]], device='cuda:0')\n",
      "\n",
      "\n",
      "tensor([[-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094]], device='cuda:0',\n",
      "       grad_fn=<LogSoftmaxBackward>)\n",
      "tensor([[0.1441, 0.3917, 0.0530, 0.0195, 0.3917],\n",
      "        [0.0161, 0.0438, 0.0438, 0.0161, 0.8801],\n",
      "        [0.1915, 0.0705, 0.5206, 0.0259, 0.1915],\n",
      "        [0.0157, 0.0426, 0.0426, 0.0426, 0.8564],\n",
      "        [0.0161, 0.0438, 0.8801, 0.0438, 0.0161],\n",
      "        [0.0117, 0.0861, 0.6364, 0.2341, 0.0317],\n",
      "        [0.0317, 0.0861, 0.0117, 0.2341, 0.6364],\n",
      "        [0.3483, 0.3483, 0.0471, 0.1281, 0.1281]], device='cuda:0')\n",
      "\n",
      "\n",
      "\n",
      "Test set: Accuracy: 2/16 (12%)\n",
      "\n",
      "training epoch 27\n",
      "tensor(0.2219, device='cuda:0', grad_fn=<KlDivBackward>)\n",
      "Train Epoch: 27 [0/67 (0%)]\tLoss: 0.221894\n",
      "tensor(0.2330, device='cuda:0', grad_fn=<KlDivBackward>)\n",
      "tensor(0.2002, device='cuda:0', grad_fn=<KlDivBackward>)\n",
      "tensor(0.2020, device='cuda:0', grad_fn=<KlDivBackward>)\n",
      "tensor(0.2039, device='cuda:0', grad_fn=<KlDivBackward>)\n",
      "tensor(0.2159, device='cuda:0', grad_fn=<KlDivBackward>)\n",
      "Train Epoch: 27 [40/67 (56%)]\tLoss: 0.215865\n",
      "tensor(0.2466, device='cuda:0', grad_fn=<KlDivBackward>)\n",
      "tensor(0.2020, device='cuda:0', grad_fn=<KlDivBackward>)\n",
      "tensor(0.1614, device='cuda:0', grad_fn=<KlDivBackward>)\n",
      "tensor([[-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094]], device='cuda:0',\n",
      "       grad_fn=<LogSoftmaxBackward>)\n",
      "tensor([[0.0117, 0.0861, 0.6364, 0.2341, 0.0317],\n",
      "        [0.0161, 0.0438, 0.0438, 0.0161, 0.8801],\n",
      "        [0.0047, 0.0345, 0.6931, 0.0127, 0.2550],\n",
      "        [0.6869, 0.0930, 0.0930, 0.0342, 0.0930],\n",
      "        [0.3483, 0.3483, 0.0471, 0.1281, 0.1281],\n",
      "        [0.0317, 0.0861, 0.0117, 0.2341, 0.6364],\n",
      "        [0.0161, 0.0438, 0.8801, 0.0438, 0.0161],\n",
      "        [0.1441, 0.3917, 0.0530, 0.0195, 0.3917]], device='cuda:0')\n",
      "\n",
      "\n",
      "tensor([[-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094]], device='cuda:0',\n",
      "       grad_fn=<LogSoftmaxBackward>)\n",
      "tensor([[0.1643, 0.0604, 0.4466, 0.1643, 0.1643],\n",
      "        [0.0388, 0.0388, 0.7784, 0.1053, 0.0388],\n",
      "        [0.1915, 0.0705, 0.5206, 0.0259, 0.1915],\n",
      "        [0.0202, 0.1491, 0.4053, 0.0202, 0.4053],\n",
      "        [0.0408, 0.0150, 0.8185, 0.0150, 0.1108],\n",
      "        [0.0157, 0.0426, 0.0426, 0.0426, 0.8564],\n",
      "        [0.1044, 0.0141, 0.0052, 0.1044, 0.7718],\n",
      "        [0.0146, 0.0397, 0.7979, 0.1080, 0.0397]], device='cuda:0')\n",
      "\n",
      "\n",
      "\n",
      "Test set: Accuracy: 2/16 (12%)\n",
      "\n",
      "training epoch 28\n",
      "tensor(0.1901, device='cuda:0', grad_fn=<KlDivBackward>)\n",
      "Train Epoch: 28 [0/67 (0%)]\tLoss: 0.190083\n",
      "tensor(0.2357, device='cuda:0', grad_fn=<KlDivBackward>)\n",
      "tensor(0.2424, device='cuda:0', grad_fn=<KlDivBackward>)\n",
      "tensor(0.2468, device='cuda:0', grad_fn=<KlDivBackward>)\n",
      "tensor(0.2603, device='cuda:0', grad_fn=<KlDivBackward>)\n",
      "tensor(0.1942, device='cuda:0', grad_fn=<KlDivBackward>)\n",
      "Train Epoch: 28 [40/67 (56%)]\tLoss: 0.194203\n",
      "tensor(0.1952, device='cuda:0', grad_fn=<KlDivBackward>)\n",
      "tensor(0.1992, device='cuda:0', grad_fn=<KlDivBackward>)\n",
      "tensor(0.0591, device='cuda:0', grad_fn=<KlDivBackward>)\n",
      "tensor([[-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094]], device='cuda:0',\n",
      "       grad_fn=<LogSoftmaxBackward>)\n",
      "tensor([[0.1915, 0.0705, 0.5206, 0.0259, 0.1915],\n",
      "        [0.1044, 0.0141, 0.0052, 0.1044, 0.7718],\n",
      "        [0.0047, 0.0345, 0.6931, 0.0127, 0.2550],\n",
      "        [0.0157, 0.0426, 0.0426, 0.0426, 0.8564],\n",
      "        [0.0317, 0.0861, 0.0117, 0.2341, 0.6364],\n",
      "        [0.0161, 0.0438, 0.0438, 0.0161, 0.8801],\n",
      "        [0.0408, 0.0150, 0.8185, 0.0150, 0.1108],\n",
      "        [0.0161, 0.0438, 0.8801, 0.0438, 0.0161]], device='cuda:0')\n",
      "\n",
      "\n",
      "tensor([[-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094]], device='cuda:0',\n",
      "       grad_fn=<LogSoftmaxBackward>)\n",
      "tensor([[0.0202, 0.1491, 0.4053, 0.0202, 0.4053],\n",
      "        [0.0388, 0.0388, 0.7784, 0.1053, 0.0388],\n",
      "        [0.0117, 0.0861, 0.6364, 0.2341, 0.0317],\n",
      "        [0.3483, 0.3483, 0.0471, 0.1281, 0.1281],\n",
      "        [0.1643, 0.0604, 0.4466, 0.1643, 0.1643],\n",
      "        [0.1441, 0.3917, 0.0530, 0.0195, 0.3917],\n",
      "        [0.0146, 0.0397, 0.7979, 0.1080, 0.0397],\n",
      "        [0.6869, 0.0930, 0.0930, 0.0342, 0.0930]], device='cuda:0')\n",
      "\n",
      "\n",
      "\n",
      "Test set: Accuracy: 2/16 (12%)\n",
      "\n",
      "training epoch 29\n",
      "tensor(0.2295, device='cuda:0', grad_fn=<KlDivBackward>)\n",
      "Train Epoch: 29 [0/67 (0%)]\tLoss: 0.229497\n",
      "tensor(0.1661, device='cuda:0', grad_fn=<KlDivBackward>)\n",
      "tensor(0.2111, device='cuda:0', grad_fn=<KlDivBackward>)\n",
      "tensor(0.2003, device='cuda:0', grad_fn=<KlDivBackward>)\n",
      "tensor(0.2393, device='cuda:0', grad_fn=<KlDivBackward>)\n",
      "tensor(0.2068, device='cuda:0', grad_fn=<KlDivBackward>)\n",
      "Train Epoch: 29 [40/67 (56%)]\tLoss: 0.206779\n",
      "tensor(0.2063, device='cuda:0', grad_fn=<KlDivBackward>)\n",
      "tensor(0.2237, device='cuda:0', grad_fn=<KlDivBackward>)\n",
      "tensor(0.2742, device='cuda:0', grad_fn=<KlDivBackward>)\n",
      "tensor([[-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094]], device='cuda:0',\n",
      "       grad_fn=<LogSoftmaxBackward>)\n",
      "tensor([[0.3483, 0.3483, 0.0471, 0.1281, 0.1281],\n",
      "        [0.0161, 0.0438, 0.8801, 0.0438, 0.0161],\n",
      "        [0.0117, 0.0861, 0.6364, 0.2341, 0.0317],\n",
      "        [0.0408, 0.0150, 0.8185, 0.0150, 0.1108],\n",
      "        [0.0146, 0.0397, 0.7979, 0.1080, 0.0397],\n",
      "        [0.1441, 0.3917, 0.0530, 0.0195, 0.3917],\n",
      "        [0.0202, 0.1491, 0.4053, 0.0202, 0.4053],\n",
      "        [0.0161, 0.0438, 0.0438, 0.0161, 0.8801]], device='cuda:0')\n",
      "\n",
      "\n",
      "tensor([[-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094]], device='cuda:0',\n",
      "       grad_fn=<LogSoftmaxBackward>)\n",
      "tensor([[0.6869, 0.0930, 0.0930, 0.0342, 0.0930],\n",
      "        [0.1915, 0.0705, 0.5206, 0.0259, 0.1915],\n",
      "        [0.0157, 0.0426, 0.0426, 0.0426, 0.8564],\n",
      "        [0.1044, 0.0141, 0.0052, 0.1044, 0.7718],\n",
      "        [0.1643, 0.0604, 0.4466, 0.1643, 0.1643],\n",
      "        [0.0388, 0.0388, 0.7784, 0.1053, 0.0388],\n",
      "        [0.0047, 0.0345, 0.6931, 0.0127, 0.2550],\n",
      "        [0.0317, 0.0861, 0.0117, 0.2341, 0.6364]], device='cuda:0')\n",
      "\n",
      "\n",
      "\n",
      "Test set: Accuracy: 2/16 (12%)\n",
      "\n",
      "training epoch 30\n",
      "tensor(0.1793, device='cuda:0', grad_fn=<KlDivBackward>)\n",
      "Train Epoch: 30 [0/67 (0%)]\tLoss: 0.179301\n",
      "tensor(0.2657, device='cuda:0', grad_fn=<KlDivBackward>)\n",
      "tensor(0.1817, device='cuda:0', grad_fn=<KlDivBackward>)\n",
      "tensor(0.2383, device='cuda:0', grad_fn=<KlDivBackward>)\n",
      "tensor(0.2043, device='cuda:0', grad_fn=<KlDivBackward>)\n",
      "tensor(0.1791, device='cuda:0', grad_fn=<KlDivBackward>)\n",
      "Train Epoch: 30 [40/67 (56%)]\tLoss: 0.179149\n",
      "tensor(0.1947, device='cuda:0', grad_fn=<KlDivBackward>)\n",
      "tensor(0.2372, device='cuda:0', grad_fn=<KlDivBackward>)\n",
      "tensor(0.2818, device='cuda:0', grad_fn=<KlDivBackward>)\n",
      "tensor([[-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094]], device='cuda:0',\n",
      "       grad_fn=<LogSoftmaxBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.3483, 0.3483, 0.0471, 0.1281, 0.1281],\n",
      "        [0.1915, 0.0705, 0.5206, 0.0259, 0.1915],\n",
      "        [0.0047, 0.0345, 0.6931, 0.0127, 0.2550],\n",
      "        [0.0161, 0.0438, 0.0438, 0.0161, 0.8801],\n",
      "        [0.0388, 0.0388, 0.7784, 0.1053, 0.0388],\n",
      "        [0.0117, 0.0861, 0.6364, 0.2341, 0.0317],\n",
      "        [0.6869, 0.0930, 0.0930, 0.0342, 0.0930],\n",
      "        [0.0202, 0.1491, 0.4053, 0.0202, 0.4053]], device='cuda:0')\n",
      "\n",
      "\n",
      "tensor([[-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094]], device='cuda:0',\n",
      "       grad_fn=<LogSoftmaxBackward>)\n",
      "tensor([[0.0146, 0.0397, 0.7979, 0.1080, 0.0397],\n",
      "        [0.0408, 0.0150, 0.8185, 0.0150, 0.1108],\n",
      "        [0.1643, 0.0604, 0.4466, 0.1643, 0.1643],\n",
      "        [0.0157, 0.0426, 0.0426, 0.0426, 0.8564],\n",
      "        [0.0317, 0.0861, 0.0117, 0.2341, 0.6364],\n",
      "        [0.1044, 0.0141, 0.0052, 0.1044, 0.7718],\n",
      "        [0.0161, 0.0438, 0.8801, 0.0438, 0.0161],\n",
      "        [0.1441, 0.3917, 0.0530, 0.0195, 0.3917]], device='cuda:0')\n",
      "\n",
      "\n",
      "\n",
      "Test set: Accuracy: 2/16 (12%)\n",
      "\n",
      "training epoch 31\n",
      "First round of training complete. Setting learn rate to 0.001.\n",
      "tensor(0.2012, device='cuda:0', grad_fn=<KlDivBackward>)\n",
      "Train Epoch: 31 [0/67 (0%)]\tLoss: 0.201178\n",
      "tensor(0.1757, device='cuda:0', grad_fn=<KlDivBackward>)\n",
      "tensor(0.2099, device='cuda:0', grad_fn=<KlDivBackward>)\n",
      "tensor(0.1850, device='cuda:0', grad_fn=<KlDivBackward>)\n",
      "tensor(0.2402, device='cuda:0', grad_fn=<KlDivBackward>)\n",
      "tensor(0.2466, device='cuda:0', grad_fn=<KlDivBackward>)\n",
      "Train Epoch: 31 [40/67 (56%)]\tLoss: 0.246627\n",
      "tensor(0.2243, device='cuda:0', grad_fn=<KlDivBackward>)\n",
      "tensor(0.1879, device='cuda:0', grad_fn=<KlDivBackward>)\n",
      "tensor(0.3069, device='cuda:0', grad_fn=<KlDivBackward>)\n",
      "tensor([[-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094]], device='cuda:0',\n",
      "       grad_fn=<LogSoftmaxBackward>)\n",
      "tensor([[0.0388, 0.0388, 0.7784, 0.1053, 0.0388],\n",
      "        [0.1044, 0.0141, 0.0052, 0.1044, 0.7718],\n",
      "        [0.1441, 0.3917, 0.0530, 0.0195, 0.3917],\n",
      "        [0.3483, 0.3483, 0.0471, 0.1281, 0.1281],\n",
      "        [0.6869, 0.0930, 0.0930, 0.0342, 0.0930],\n",
      "        [0.0161, 0.0438, 0.0438, 0.0161, 0.8801],\n",
      "        [0.0117, 0.0861, 0.6364, 0.2341, 0.0317],\n",
      "        [0.1643, 0.0604, 0.4466, 0.1643, 0.1643]], device='cuda:0')\n",
      "\n",
      "\n",
      "tensor([[-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094]], device='cuda:0',\n",
      "       grad_fn=<LogSoftmaxBackward>)\n",
      "tensor([[0.1915, 0.0705, 0.5206, 0.0259, 0.1915],\n",
      "        [0.0047, 0.0345, 0.6931, 0.0127, 0.2550],\n",
      "        [0.0161, 0.0438, 0.8801, 0.0438, 0.0161],\n",
      "        [0.0146, 0.0397, 0.7979, 0.1080, 0.0397],\n",
      "        [0.0202, 0.1491, 0.4053, 0.0202, 0.4053],\n",
      "        [0.0408, 0.0150, 0.8185, 0.0150, 0.1108],\n",
      "        [0.0157, 0.0426, 0.0426, 0.0426, 0.8564],\n",
      "        [0.0317, 0.0861, 0.0117, 0.2341, 0.6364]], device='cuda:0')\n",
      "\n",
      "\n",
      "\n",
      "Test set: Accuracy: 2/16 (12%)\n",
      "\n",
      "training epoch 32\n",
      "tensor(0.2537, device='cuda:0', grad_fn=<KlDivBackward>)\n",
      "Train Epoch: 32 [0/67 (0%)]\tLoss: 0.253730\n",
      "tensor(0.2250, device='cuda:0', grad_fn=<KlDivBackward>)\n",
      "tensor(0.2257, device='cuda:0', grad_fn=<KlDivBackward>)\n",
      "tensor(0.2055, device='cuda:0', grad_fn=<KlDivBackward>)\n",
      "tensor(0.2292, device='cuda:0', grad_fn=<KlDivBackward>)\n",
      "tensor(0.2245, device='cuda:0', grad_fn=<KlDivBackward>)\n",
      "Train Epoch: 32 [40/67 (56%)]\tLoss: 0.224488\n",
      "tensor(0.1947, device='cuda:0', grad_fn=<KlDivBackward>)\n",
      "tensor(0.1810, device='cuda:0', grad_fn=<KlDivBackward>)\n",
      "tensor(0.1243, device='cuda:0', grad_fn=<KlDivBackward>)\n",
      "tensor([[-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094]], device='cuda:0',\n",
      "       grad_fn=<LogSoftmaxBackward>)\n",
      "tensor([[0.0157, 0.0426, 0.0426, 0.0426, 0.8564],\n",
      "        [0.0117, 0.0861, 0.6364, 0.2341, 0.0317],\n",
      "        [0.1915, 0.0705, 0.5206, 0.0259, 0.1915],\n",
      "        [0.1044, 0.0141, 0.0052, 0.1044, 0.7718],\n",
      "        [0.0161, 0.0438, 0.0438, 0.0161, 0.8801],\n",
      "        [0.0388, 0.0388, 0.7784, 0.1053, 0.0388],\n",
      "        [0.6869, 0.0930, 0.0930, 0.0342, 0.0930],\n",
      "        [0.0161, 0.0438, 0.8801, 0.0438, 0.0161]], device='cuda:0')\n",
      "\n",
      "\n",
      "tensor([[-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094]], device='cuda:0',\n",
      "       grad_fn=<LogSoftmaxBackward>)\n",
      "tensor([[0.0146, 0.0397, 0.7979, 0.1080, 0.0397],\n",
      "        [0.0047, 0.0345, 0.6931, 0.0127, 0.2550],\n",
      "        [0.1643, 0.0604, 0.4466, 0.1643, 0.1643],\n",
      "        [0.1441, 0.3917, 0.0530, 0.0195, 0.3917],\n",
      "        [0.3483, 0.3483, 0.0471, 0.1281, 0.1281],\n",
      "        [0.0202, 0.1491, 0.4053, 0.0202, 0.4053],\n",
      "        [0.0408, 0.0150, 0.8185, 0.0150, 0.1108],\n",
      "        [0.0317, 0.0861, 0.0117, 0.2341, 0.6364]], device='cuda:0')\n",
      "\n",
      "\n",
      "\n",
      "Test set: Accuracy: 2/16 (12%)\n",
      "\n",
      "training epoch 33\n",
      "tensor(0.2858, device='cuda:0', grad_fn=<KlDivBackward>)\n",
      "Train Epoch: 33 [0/67 (0%)]\tLoss: 0.285821\n",
      "tensor(0.1802, device='cuda:0', grad_fn=<KlDivBackward>)\n",
      "tensor(0.2334, device='cuda:0', grad_fn=<KlDivBackward>)\n",
      "tensor(0.2391, device='cuda:0', grad_fn=<KlDivBackward>)\n",
      "tensor(0.1807, device='cuda:0', grad_fn=<KlDivBackward>)\n",
      "tensor(0.2158, device='cuda:0', grad_fn=<KlDivBackward>)\n",
      "Train Epoch: 33 [40/67 (56%)]\tLoss: 0.215800\n",
      "tensor(0.1756, device='cuda:0', grad_fn=<KlDivBackward>)\n",
      "tensor(0.2229, device='cuda:0', grad_fn=<KlDivBackward>)\n",
      "tensor(0.1399, device='cuda:0', grad_fn=<KlDivBackward>)\n",
      "tensor([[-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094]], device='cuda:0',\n",
      "       grad_fn=<LogSoftmaxBackward>)\n",
      "tensor([[0.0161, 0.0438, 0.8801, 0.0438, 0.0161],\n",
      "        [0.3483, 0.3483, 0.0471, 0.1281, 0.1281],\n",
      "        [0.0117, 0.0861, 0.6364, 0.2341, 0.0317],\n",
      "        [0.0047, 0.0345, 0.6931, 0.0127, 0.2550],\n",
      "        [0.0157, 0.0426, 0.0426, 0.0426, 0.8564],\n",
      "        [0.0146, 0.0397, 0.7979, 0.1080, 0.0397],\n",
      "        [0.0388, 0.0388, 0.7784, 0.1053, 0.0388],\n",
      "        [0.0317, 0.0861, 0.0117, 0.2341, 0.6364]], device='cuda:0')\n",
      "\n",
      "\n",
      "tensor([[-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094]], device='cuda:0',\n",
      "       grad_fn=<LogSoftmaxBackward>)\n",
      "tensor([[0.1915, 0.0705, 0.5206, 0.0259, 0.1915],\n",
      "        [0.6869, 0.0930, 0.0930, 0.0342, 0.0930],\n",
      "        [0.0161, 0.0438, 0.0438, 0.0161, 0.8801],\n",
      "        [0.0202, 0.1491, 0.4053, 0.0202, 0.4053],\n",
      "        [0.1441, 0.3917, 0.0530, 0.0195, 0.3917],\n",
      "        [0.1643, 0.0604, 0.4466, 0.1643, 0.1643],\n",
      "        [0.0408, 0.0150, 0.8185, 0.0150, 0.1108],\n",
      "        [0.1044, 0.0141, 0.0052, 0.1044, 0.7718]], device='cuda:0')\n",
      "\n",
      "\n",
      "\n",
      "Test set: Accuracy: 2/16 (12%)\n",
      "\n",
      "training epoch 34\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.2174, device='cuda:0', grad_fn=<KlDivBackward>)\n",
      "Train Epoch: 34 [0/67 (0%)]\tLoss: 0.217437\n",
      "tensor(0.2160, device='cuda:0', grad_fn=<KlDivBackward>)\n",
      "tensor(0.2311, device='cuda:0', grad_fn=<KlDivBackward>)\n",
      "tensor(0.2402, device='cuda:0', grad_fn=<KlDivBackward>)\n",
      "tensor(0.2256, device='cuda:0', grad_fn=<KlDivBackward>)\n",
      "tensor(0.1850, device='cuda:0', grad_fn=<KlDivBackward>)\n",
      "Train Epoch: 34 [40/67 (56%)]\tLoss: 0.185020\n",
      "tensor(0.1581, device='cuda:0', grad_fn=<KlDivBackward>)\n",
      "tensor(0.2125, device='cuda:0', grad_fn=<KlDivBackward>)\n",
      "tensor(0.2663, device='cuda:0', grad_fn=<KlDivBackward>)\n",
      "tensor([[-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094]], device='cuda:0',\n",
      "       grad_fn=<LogSoftmaxBackward>)\n",
      "tensor([[0.3483, 0.3483, 0.0471, 0.1281, 0.1281],\n",
      "        [0.0117, 0.0861, 0.6364, 0.2341, 0.0317],\n",
      "        [0.0161, 0.0438, 0.8801, 0.0438, 0.0161],\n",
      "        [0.0157, 0.0426, 0.0426, 0.0426, 0.8564],\n",
      "        [0.0317, 0.0861, 0.0117, 0.2341, 0.6364],\n",
      "        [0.1643, 0.0604, 0.4466, 0.1643, 0.1643],\n",
      "        [0.0408, 0.0150, 0.8185, 0.0150, 0.1108],\n",
      "        [0.0047, 0.0345, 0.6931, 0.0127, 0.2550]], device='cuda:0')\n",
      "\n",
      "\n",
      "tensor([[-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094]], device='cuda:0',\n",
      "       grad_fn=<LogSoftmaxBackward>)\n",
      "tensor([[0.0146, 0.0397, 0.7979, 0.1080, 0.0397],\n",
      "        [0.0388, 0.0388, 0.7784, 0.1053, 0.0388],\n",
      "        [0.1441, 0.3917, 0.0530, 0.0195, 0.3917],\n",
      "        [0.0161, 0.0438, 0.0438, 0.0161, 0.8801],\n",
      "        [0.1915, 0.0705, 0.5206, 0.0259, 0.1915],\n",
      "        [0.0202, 0.1491, 0.4053, 0.0202, 0.4053],\n",
      "        [0.1044, 0.0141, 0.0052, 0.1044, 0.7718],\n",
      "        [0.6869, 0.0930, 0.0930, 0.0342, 0.0930]], device='cuda:0')\n",
      "\n",
      "\n",
      "\n",
      "Test set: Accuracy: 2/16 (12%)\n",
      "\n",
      "training epoch 35\n",
      "tensor(0.2473, device='cuda:0', grad_fn=<KlDivBackward>)\n",
      "Train Epoch: 35 [0/67 (0%)]\tLoss: 0.247329\n",
      "tensor(0.2514, device='cuda:0', grad_fn=<KlDivBackward>)\n",
      "tensor(0.2156, device='cuda:0', grad_fn=<KlDivBackward>)\n",
      "tensor(0.2423, device='cuda:0', grad_fn=<KlDivBackward>)\n",
      "tensor(0.1912, device='cuda:0', grad_fn=<KlDivBackward>)\n",
      "tensor(0.1954, device='cuda:0', grad_fn=<KlDivBackward>)\n",
      "Train Epoch: 35 [40/67 (56%)]\tLoss: 0.195366\n",
      "tensor(0.2131, device='cuda:0', grad_fn=<KlDivBackward>)\n",
      "tensor(0.1585, device='cuda:0', grad_fn=<KlDivBackward>)\n",
      "tensor(0.1897, device='cuda:0', grad_fn=<KlDivBackward>)\n",
      "tensor([[-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094]], device='cuda:0',\n",
      "       grad_fn=<LogSoftmaxBackward>)\n",
      "tensor([[0.1643, 0.0604, 0.4466, 0.1643, 0.1643],\n",
      "        [0.0047, 0.0345, 0.6931, 0.0127, 0.2550],\n",
      "        [0.3483, 0.3483, 0.0471, 0.1281, 0.1281],\n",
      "        [0.0202, 0.1491, 0.4053, 0.0202, 0.4053],\n",
      "        [0.6869, 0.0930, 0.0930, 0.0342, 0.0930],\n",
      "        [0.0161, 0.0438, 0.8801, 0.0438, 0.0161],\n",
      "        [0.0408, 0.0150, 0.8185, 0.0150, 0.1108],\n",
      "        [0.1044, 0.0141, 0.0052, 0.1044, 0.7718]], device='cuda:0')\n",
      "\n",
      "\n",
      "tensor([[-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094]], device='cuda:0',\n",
      "       grad_fn=<LogSoftmaxBackward>)\n",
      "tensor([[0.0117, 0.0861, 0.6364, 0.2341, 0.0317],\n",
      "        [0.0317, 0.0861, 0.0117, 0.2341, 0.6364],\n",
      "        [0.0146, 0.0397, 0.7979, 0.1080, 0.0397],\n",
      "        [0.0161, 0.0438, 0.0438, 0.0161, 0.8801],\n",
      "        [0.0157, 0.0426, 0.0426, 0.0426, 0.8564],\n",
      "        [0.1915, 0.0705, 0.5206, 0.0259, 0.1915],\n",
      "        [0.1441, 0.3917, 0.0530, 0.0195, 0.3917],\n",
      "        [0.0388, 0.0388, 0.7784, 0.1053, 0.0388]], device='cuda:0')\n",
      "\n",
      "\n",
      "\n",
      "Test set: Accuracy: 2/16 (12%)\n",
      "\n",
      "training epoch 36\n",
      "tensor(0.2147, device='cuda:0', grad_fn=<KlDivBackward>)\n",
      "Train Epoch: 36 [0/67 (0%)]\tLoss: 0.214705\n",
      "tensor(0.2579, device='cuda:0', grad_fn=<KlDivBackward>)\n",
      "tensor(0.2327, device='cuda:0', grad_fn=<KlDivBackward>)\n",
      "tensor(0.2014, device='cuda:0', grad_fn=<KlDivBackward>)\n",
      "tensor(0.1595, device='cuda:0', grad_fn=<KlDivBackward>)\n",
      "tensor(0.2275, device='cuda:0', grad_fn=<KlDivBackward>)\n",
      "Train Epoch: 36 [40/67 (56%)]\tLoss: 0.227492\n",
      "tensor(0.2066, device='cuda:0', grad_fn=<KlDivBackward>)\n",
      "tensor(0.1998, device='cuda:0', grad_fn=<KlDivBackward>)\n",
      "tensor(0.2288, device='cuda:0', grad_fn=<KlDivBackward>)\n",
      "tensor([[-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094]], device='cuda:0',\n",
      "       grad_fn=<LogSoftmaxBackward>)\n",
      "tensor([[0.3483, 0.3483, 0.0471, 0.1281, 0.1281],\n",
      "        [0.0047, 0.0345, 0.6931, 0.0127, 0.2550],\n",
      "        [0.0146, 0.0397, 0.7979, 0.1080, 0.0397],\n",
      "        [0.0388, 0.0388, 0.7784, 0.1053, 0.0388],\n",
      "        [0.1915, 0.0705, 0.5206, 0.0259, 0.1915],\n",
      "        [0.0408, 0.0150, 0.8185, 0.0150, 0.1108],\n",
      "        [0.1044, 0.0141, 0.0052, 0.1044, 0.7718],\n",
      "        [0.6869, 0.0930, 0.0930, 0.0342, 0.0930]], device='cuda:0')\n",
      "\n",
      "\n",
      "tensor([[-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094]], device='cuda:0',\n",
      "       grad_fn=<LogSoftmaxBackward>)\n",
      "tensor([[0.1441, 0.3917, 0.0530, 0.0195, 0.3917],\n",
      "        [0.0161, 0.0438, 0.8801, 0.0438, 0.0161],\n",
      "        [0.0202, 0.1491, 0.4053, 0.0202, 0.4053],\n",
      "        [0.0157, 0.0426, 0.0426, 0.0426, 0.8564],\n",
      "        [0.0117, 0.0861, 0.6364, 0.2341, 0.0317],\n",
      "        [0.1643, 0.0604, 0.4466, 0.1643, 0.1643],\n",
      "        [0.0161, 0.0438, 0.0438, 0.0161, 0.8801],\n",
      "        [0.0317, 0.0861, 0.0117, 0.2341, 0.6364]], device='cuda:0')\n",
      "\n",
      "\n",
      "\n",
      "Test set: Accuracy: 2/16 (12%)\n",
      "\n",
      "training epoch 37\n",
      "tensor(0.1814, device='cuda:0', grad_fn=<KlDivBackward>)\n",
      "Train Epoch: 37 [0/67 (0%)]\tLoss: 0.181369\n",
      "tensor(0.2093, device='cuda:0', grad_fn=<KlDivBackward>)\n",
      "tensor(0.2554, device='cuda:0', grad_fn=<KlDivBackward>)\n",
      "tensor(0.2548, device='cuda:0', grad_fn=<KlDivBackward>)\n",
      "tensor(0.2540, device='cuda:0', grad_fn=<KlDivBackward>)\n",
      "tensor(0.2050, device='cuda:0', grad_fn=<KlDivBackward>)\n",
      "Train Epoch: 37 [40/67 (56%)]\tLoss: 0.205001\n",
      "tensor(0.2397, device='cuda:0', grad_fn=<KlDivBackward>)\n",
      "tensor(0.1061, device='cuda:0', grad_fn=<KlDivBackward>)\n",
      "tensor(0.2140, device='cuda:0', grad_fn=<KlDivBackward>)\n",
      "tensor([[-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094]], device='cuda:0',\n",
      "       grad_fn=<LogSoftmaxBackward>)\n",
      "tensor([[0.0117, 0.0861, 0.6364, 0.2341, 0.0317],\n",
      "        [0.0388, 0.0388, 0.7784, 0.1053, 0.0388],\n",
      "        [0.1044, 0.0141, 0.0052, 0.1044, 0.7718],\n",
      "        [0.0202, 0.1491, 0.4053, 0.0202, 0.4053],\n",
      "        [0.1915, 0.0705, 0.5206, 0.0259, 0.1915],\n",
      "        [0.0161, 0.0438, 0.0438, 0.0161, 0.8801],\n",
      "        [0.3483, 0.3483, 0.0471, 0.1281, 0.1281],\n",
      "        [0.0317, 0.0861, 0.0117, 0.2341, 0.6364]], device='cuda:0')\n",
      "\n",
      "\n",
      "tensor([[-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094]], device='cuda:0',\n",
      "       grad_fn=<LogSoftmaxBackward>)\n",
      "tensor([[0.1441, 0.3917, 0.0530, 0.0195, 0.3917],\n",
      "        [0.0047, 0.0345, 0.6931, 0.0127, 0.2550],\n",
      "        [0.0157, 0.0426, 0.0426, 0.0426, 0.8564],\n",
      "        [0.6869, 0.0930, 0.0930, 0.0342, 0.0930],\n",
      "        [0.0146, 0.0397, 0.7979, 0.1080, 0.0397],\n",
      "        [0.1643, 0.0604, 0.4466, 0.1643, 0.1643],\n",
      "        [0.0408, 0.0150, 0.8185, 0.0150, 0.1108],\n",
      "        [0.0161, 0.0438, 0.8801, 0.0438, 0.0161]], device='cuda:0')\n",
      "\n",
      "\n",
      "\n",
      "Test set: Accuracy: 2/16 (12%)\n",
      "\n",
      "training epoch 38\n",
      "tensor(0.2220, device='cuda:0', grad_fn=<KlDivBackward>)\n",
      "Train Epoch: 38 [0/67 (0%)]\tLoss: 0.221975\n",
      "tensor(0.2303, device='cuda:0', grad_fn=<KlDivBackward>)\n",
      "tensor(0.2140, device='cuda:0', grad_fn=<KlDivBackward>)\n",
      "tensor(0.1901, device='cuda:0', grad_fn=<KlDivBackward>)\n",
      "tensor(0.1821, device='cuda:0', grad_fn=<KlDivBackward>)\n",
      "tensor(0.2028, device='cuda:0', grad_fn=<KlDivBackward>)\n",
      "Train Epoch: 38 [40/67 (56%)]\tLoss: 0.202752\n",
      "tensor(0.2175, device='cuda:0', grad_fn=<KlDivBackward>)\n",
      "tensor(0.2221, device='cuda:0', grad_fn=<KlDivBackward>)\n",
      "tensor(0.2804, device='cuda:0', grad_fn=<KlDivBackward>)\n",
      "tensor([[-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094]], device='cuda:0',\n",
      "       grad_fn=<LogSoftmaxBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.3483, 0.3483, 0.0471, 0.1281, 0.1281],\n",
      "        [0.0161, 0.0438, 0.0438, 0.0161, 0.8801],\n",
      "        [0.0157, 0.0426, 0.0426, 0.0426, 0.8564],\n",
      "        [0.0161, 0.0438, 0.8801, 0.0438, 0.0161],\n",
      "        [0.0388, 0.0388, 0.7784, 0.1053, 0.0388],\n",
      "        [0.0317, 0.0861, 0.0117, 0.2341, 0.6364],\n",
      "        [0.6869, 0.0930, 0.0930, 0.0342, 0.0930],\n",
      "        [0.1643, 0.0604, 0.4466, 0.1643, 0.1643]], device='cuda:0')\n",
      "\n",
      "\n",
      "tensor([[-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094]], device='cuda:0',\n",
      "       grad_fn=<LogSoftmaxBackward>)\n",
      "tensor([[0.1441, 0.3917, 0.0530, 0.0195, 0.3917],\n",
      "        [0.1044, 0.0141, 0.0052, 0.1044, 0.7718],\n",
      "        [0.0202, 0.1491, 0.4053, 0.0202, 0.4053],\n",
      "        [0.0408, 0.0150, 0.8185, 0.0150, 0.1108],\n",
      "        [0.0146, 0.0397, 0.7979, 0.1080, 0.0397],\n",
      "        [0.1915, 0.0705, 0.5206, 0.0259, 0.1915],\n",
      "        [0.0047, 0.0345, 0.6931, 0.0127, 0.2550],\n",
      "        [0.0117, 0.0861, 0.6364, 0.2341, 0.0317]], device='cuda:0')\n",
      "\n",
      "\n",
      "\n",
      "Test set: Accuracy: 2/16 (12%)\n",
      "\n",
      "training epoch 39\n",
      "tensor(0.1811, device='cuda:0', grad_fn=<KlDivBackward>)\n",
      "Train Epoch: 39 [0/67 (0%)]\tLoss: 0.181144\n",
      "tensor(0.2170, device='cuda:0', grad_fn=<KlDivBackward>)\n",
      "tensor(0.2006, device='cuda:0', grad_fn=<KlDivBackward>)\n",
      "tensor(0.2403, device='cuda:0', grad_fn=<KlDivBackward>)\n",
      "tensor(0.1933, device='cuda:0', grad_fn=<KlDivBackward>)\n",
      "tensor(0.2317, device='cuda:0', grad_fn=<KlDivBackward>)\n",
      "Train Epoch: 39 [40/67 (56%)]\tLoss: 0.231709\n",
      "tensor(0.1882, device='cuda:0', grad_fn=<KlDivBackward>)\n",
      "tensor(0.2359, device='cuda:0', grad_fn=<KlDivBackward>)\n",
      "tensor(0.2608, device='cuda:0', grad_fn=<KlDivBackward>)\n",
      "tensor([[-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094]], device='cuda:0',\n",
      "       grad_fn=<LogSoftmaxBackward>)\n",
      "tensor([[0.6869, 0.0930, 0.0930, 0.0342, 0.0930],\n",
      "        [0.0157, 0.0426, 0.0426, 0.0426, 0.8564],\n",
      "        [0.0161, 0.0438, 0.0438, 0.0161, 0.8801],\n",
      "        [0.1441, 0.3917, 0.0530, 0.0195, 0.3917],\n",
      "        [0.1915, 0.0705, 0.5206, 0.0259, 0.1915],\n",
      "        [0.0408, 0.0150, 0.8185, 0.0150, 0.1108],\n",
      "        [0.0161, 0.0438, 0.8801, 0.0438, 0.0161],\n",
      "        [0.3483, 0.3483, 0.0471, 0.1281, 0.1281]], device='cuda:0')\n",
      "\n",
      "\n",
      "tensor([[-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094]], device='cuda:0',\n",
      "       grad_fn=<LogSoftmaxBackward>)\n",
      "tensor([[0.0146, 0.0397, 0.7979, 0.1080, 0.0397],\n",
      "        [0.0388, 0.0388, 0.7784, 0.1053, 0.0388],\n",
      "        [0.0317, 0.0861, 0.0117, 0.2341, 0.6364],\n",
      "        [0.0047, 0.0345, 0.6931, 0.0127, 0.2550],\n",
      "        [0.0202, 0.1491, 0.4053, 0.0202, 0.4053],\n",
      "        [0.1643, 0.0604, 0.4466, 0.1643, 0.1643],\n",
      "        [0.1044, 0.0141, 0.0052, 0.1044, 0.7718],\n",
      "        [0.0117, 0.0861, 0.6364, 0.2341, 0.0317]], device='cuda:0')\n",
      "\n",
      "\n",
      "\n",
      "Test set: Accuracy: 2/16 (12%)\n",
      "\n",
      "training epoch 40\n",
      "tensor(0.2067, device='cuda:0', grad_fn=<KlDivBackward>)\n",
      "Train Epoch: 40 [0/67 (0%)]\tLoss: 0.206739\n",
      "tensor(0.2455, device='cuda:0', grad_fn=<KlDivBackward>)\n",
      "tensor(0.1528, device='cuda:0', grad_fn=<KlDivBackward>)\n",
      "tensor(0.2433, device='cuda:0', grad_fn=<KlDivBackward>)\n",
      "tensor(0.2287, device='cuda:0', grad_fn=<KlDivBackward>)\n",
      "tensor(0.2289, device='cuda:0', grad_fn=<KlDivBackward>)\n",
      "Train Epoch: 40 [40/67 (56%)]\tLoss: 0.228920\n",
      "tensor(0.2091, device='cuda:0', grad_fn=<KlDivBackward>)\n",
      "tensor(0.1859, device='cuda:0', grad_fn=<KlDivBackward>)\n",
      "tensor(0.2267, device='cuda:0', grad_fn=<KlDivBackward>)\n",
      "tensor([[-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094]], device='cuda:0',\n",
      "       grad_fn=<LogSoftmaxBackward>)\n",
      "tensor([[0.1441, 0.3917, 0.0530, 0.0195, 0.3917],\n",
      "        [0.0157, 0.0426, 0.0426, 0.0426, 0.8564],\n",
      "        [0.0388, 0.0388, 0.7784, 0.1053, 0.0388],\n",
      "        [0.0202, 0.1491, 0.4053, 0.0202, 0.4053],\n",
      "        [0.1044, 0.0141, 0.0052, 0.1044, 0.7718],\n",
      "        [0.0146, 0.0397, 0.7979, 0.1080, 0.0397],\n",
      "        [0.0117, 0.0861, 0.6364, 0.2341, 0.0317],\n",
      "        [0.0047, 0.0345, 0.6931, 0.0127, 0.2550]], device='cuda:0')\n",
      "\n",
      "\n",
      "tensor([[-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094]], device='cuda:0',\n",
      "       grad_fn=<LogSoftmaxBackward>)\n",
      "tensor([[0.1915, 0.0705, 0.5206, 0.0259, 0.1915],\n",
      "        [0.0408, 0.0150, 0.8185, 0.0150, 0.1108],\n",
      "        [0.0161, 0.0438, 0.0438, 0.0161, 0.8801],\n",
      "        [0.0161, 0.0438, 0.8801, 0.0438, 0.0161],\n",
      "        [0.6869, 0.0930, 0.0930, 0.0342, 0.0930],\n",
      "        [0.3483, 0.3483, 0.0471, 0.1281, 0.1281],\n",
      "        [0.0317, 0.0861, 0.0117, 0.2341, 0.6364],\n",
      "        [0.1643, 0.0604, 0.4466, 0.1643, 0.1643]], device='cuda:0')\n",
      "\n",
      "\n",
      "\n",
      "Test set: Accuracy: 2/16 (12%)\n",
      "\n",
      "training epoch 41\n",
      "tensor(0.1837, device='cuda:0', grad_fn=<KlDivBackward>)\n",
      "Train Epoch: 41 [0/67 (0%)]\tLoss: 0.183715\n",
      "tensor(0.2003, device='cuda:0', grad_fn=<KlDivBackward>)\n",
      "tensor(0.2076, device='cuda:0', grad_fn=<KlDivBackward>)\n",
      "tensor(0.2200, device='cuda:0', grad_fn=<KlDivBackward>)\n",
      "tensor(0.2012, device='cuda:0', grad_fn=<KlDivBackward>)\n",
      "tensor(0.2418, device='cuda:0', grad_fn=<KlDivBackward>)\n",
      "Train Epoch: 41 [40/67 (56%)]\tLoss: 0.241781\n",
      "tensor(0.2021, device='cuda:0', grad_fn=<KlDivBackward>)\n",
      "tensor(0.2513, device='cuda:0', grad_fn=<KlDivBackward>)\n",
      "tensor(0.2078, device='cuda:0', grad_fn=<KlDivBackward>)\n",
      "tensor([[-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094]], device='cuda:0',\n",
      "       grad_fn=<LogSoftmaxBackward>)\n",
      "tensor([[0.1915, 0.0705, 0.5206, 0.0259, 0.1915],\n",
      "        [0.0161, 0.0438, 0.0438, 0.0161, 0.8801],\n",
      "        [0.0408, 0.0150, 0.8185, 0.0150, 0.1108],\n",
      "        [0.0157, 0.0426, 0.0426, 0.0426, 0.8564],\n",
      "        [0.0388, 0.0388, 0.7784, 0.1053, 0.0388],\n",
      "        [0.0047, 0.0345, 0.6931, 0.0127, 0.2550],\n",
      "        [0.3483, 0.3483, 0.0471, 0.1281, 0.1281],\n",
      "        [0.0317, 0.0861, 0.0117, 0.2341, 0.6364]], device='cuda:0')\n",
      "\n",
      "\n",
      "tensor([[-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094]], device='cuda:0',\n",
      "       grad_fn=<LogSoftmaxBackward>)\n",
      "tensor([[0.0161, 0.0438, 0.8801, 0.0438, 0.0161],\n",
      "        [0.0202, 0.1491, 0.4053, 0.0202, 0.4053],\n",
      "        [0.0117, 0.0861, 0.6364, 0.2341, 0.0317],\n",
      "        [0.1643, 0.0604, 0.4466, 0.1643, 0.1643],\n",
      "        [0.1441, 0.3917, 0.0530, 0.0195, 0.3917],\n",
      "        [0.0146, 0.0397, 0.7979, 0.1080, 0.0397],\n",
      "        [0.1044, 0.0141, 0.0052, 0.1044, 0.7718],\n",
      "        [0.6869, 0.0930, 0.0930, 0.0342, 0.0930]], device='cuda:0')\n",
      "\n",
      "\n",
      "\n",
      "Test set: Accuracy: 2/16 (12%)\n",
      "\n",
      "training epoch 42\n",
      "tensor(0.2595, device='cuda:0', grad_fn=<KlDivBackward>)\n",
      "Train Epoch: 42 [0/67 (0%)]\tLoss: 0.259490\n",
      "tensor(0.2068, device='cuda:0', grad_fn=<KlDivBackward>)\n",
      "tensor(0.2057, device='cuda:0', grad_fn=<KlDivBackward>)\n",
      "tensor(0.1797, device='cuda:0', grad_fn=<KlDivBackward>)\n",
      "tensor(0.2269, device='cuda:0', grad_fn=<KlDivBackward>)\n",
      "tensor(0.1932, device='cuda:0', grad_fn=<KlDivBackward>)\n",
      "Train Epoch: 42 [40/67 (56%)]\tLoss: 0.193209\n",
      "tensor(0.2376, device='cuda:0', grad_fn=<KlDivBackward>)\n",
      "tensor(0.1854, device='cuda:0', grad_fn=<KlDivBackward>)\n",
      "tensor(0.2433, device='cuda:0', grad_fn=<KlDivBackward>)\n",
      "tensor([[-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094]], device='cuda:0',\n",
      "       grad_fn=<LogSoftmaxBackward>)\n",
      "tensor([[0.0317, 0.0861, 0.0117, 0.2341, 0.6364],\n",
      "        [0.3483, 0.3483, 0.0471, 0.1281, 0.1281],\n",
      "        [0.1441, 0.3917, 0.0530, 0.0195, 0.3917],\n",
      "        [0.0157, 0.0426, 0.0426, 0.0426, 0.8564],\n",
      "        [0.6869, 0.0930, 0.0930, 0.0342, 0.0930],\n",
      "        [0.0161, 0.0438, 0.8801, 0.0438, 0.0161],\n",
      "        [0.0146, 0.0397, 0.7979, 0.1080, 0.0397],\n",
      "        [0.1643, 0.0604, 0.4466, 0.1643, 0.1643]], device='cuda:0')\n",
      "\n",
      "\n",
      "tensor([[-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094]], device='cuda:0',\n",
      "       grad_fn=<LogSoftmaxBackward>)\n",
      "tensor([[0.0388, 0.0388, 0.7784, 0.1053, 0.0388],\n",
      "        [0.1915, 0.0705, 0.5206, 0.0259, 0.1915],\n",
      "        [0.0047, 0.0345, 0.6931, 0.0127, 0.2550],\n",
      "        [0.0202, 0.1491, 0.4053, 0.0202, 0.4053],\n",
      "        [0.1044, 0.0141, 0.0052, 0.1044, 0.7718],\n",
      "        [0.0117, 0.0861, 0.6364, 0.2341, 0.0317],\n",
      "        [0.0408, 0.0150, 0.8185, 0.0150, 0.1108],\n",
      "        [0.0161, 0.0438, 0.0438, 0.0161, 0.8801]], device='cuda:0')\n",
      "\n",
      "\n",
      "\n",
      "Test set: Accuracy: 2/16 (12%)\n",
      "\n",
      "training epoch 43\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.2012, device='cuda:0', grad_fn=<KlDivBackward>)\n",
      "Train Epoch: 43 [0/67 (0%)]\tLoss: 0.201223\n",
      "tensor(0.2148, device='cuda:0', grad_fn=<KlDivBackward>)\n",
      "tensor(0.1805, device='cuda:0', grad_fn=<KlDivBackward>)\n",
      "tensor(0.2095, device='cuda:0', grad_fn=<KlDivBackward>)\n",
      "tensor(0.2191, device='cuda:0', grad_fn=<KlDivBackward>)\n",
      "tensor(0.2307, device='cuda:0', grad_fn=<KlDivBackward>)\n",
      "Train Epoch: 43 [40/67 (56%)]\tLoss: 0.230745\n",
      "tensor(0.2602, device='cuda:0', grad_fn=<KlDivBackward>)\n",
      "tensor(0.1916, device='cuda:0', grad_fn=<KlDivBackward>)\n",
      "tensor(0.2091, device='cuda:0', grad_fn=<KlDivBackward>)\n",
      "tensor([[-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094]], device='cuda:0',\n",
      "       grad_fn=<LogSoftmaxBackward>)\n",
      "tensor([[0.0388, 0.0388, 0.7784, 0.1053, 0.0388],\n",
      "        [0.3483, 0.3483, 0.0471, 0.1281, 0.1281],\n",
      "        [0.0161, 0.0438, 0.8801, 0.0438, 0.0161],\n",
      "        [0.0317, 0.0861, 0.0117, 0.2341, 0.6364],\n",
      "        [0.0157, 0.0426, 0.0426, 0.0426, 0.8564],\n",
      "        [0.0117, 0.0861, 0.6364, 0.2341, 0.0317],\n",
      "        [0.0202, 0.1491, 0.4053, 0.0202, 0.4053],\n",
      "        [0.1441, 0.3917, 0.0530, 0.0195, 0.3917]], device='cuda:0')\n",
      "\n",
      "\n",
      "tensor([[-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094]], device='cuda:0',\n",
      "       grad_fn=<LogSoftmaxBackward>)\n",
      "tensor([[0.1643, 0.0604, 0.4466, 0.1643, 0.1643],\n",
      "        [0.6869, 0.0930, 0.0930, 0.0342, 0.0930],\n",
      "        [0.1915, 0.0705, 0.5206, 0.0259, 0.1915],\n",
      "        [0.0408, 0.0150, 0.8185, 0.0150, 0.1108],\n",
      "        [0.0161, 0.0438, 0.0438, 0.0161, 0.8801],\n",
      "        [0.0047, 0.0345, 0.6931, 0.0127, 0.2550],\n",
      "        [0.0146, 0.0397, 0.7979, 0.1080, 0.0397],\n",
      "        [0.1044, 0.0141, 0.0052, 0.1044, 0.7718]], device='cuda:0')\n",
      "\n",
      "\n",
      "\n",
      "Test set: Accuracy: 2/16 (12%)\n",
      "\n",
      "training epoch 44\n",
      "tensor(0.2038, device='cuda:0', grad_fn=<KlDivBackward>)\n",
      "Train Epoch: 44 [0/67 (0%)]\tLoss: 0.203755\n",
      "tensor(0.1742, device='cuda:0', grad_fn=<KlDivBackward>)\n",
      "tensor(0.1902, device='cuda:0', grad_fn=<KlDivBackward>)\n",
      "tensor(0.2499, device='cuda:0', grad_fn=<KlDivBackward>)\n",
      "tensor(0.2174, device='cuda:0', grad_fn=<KlDivBackward>)\n",
      "tensor(0.2007, device='cuda:0', grad_fn=<KlDivBackward>)\n",
      "Train Epoch: 44 [40/67 (56%)]\tLoss: 0.200673\n",
      "tensor(0.2423, device='cuda:0', grad_fn=<KlDivBackward>)\n",
      "tensor(0.2141, device='cuda:0', grad_fn=<KlDivBackward>)\n",
      "tensor(0.2487, device='cuda:0', grad_fn=<KlDivBackward>)\n",
      "tensor([[-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094]], device='cuda:0',\n",
      "       grad_fn=<LogSoftmaxBackward>)\n",
      "tensor([[0.0408, 0.0150, 0.8185, 0.0150, 0.1108],\n",
      "        [0.6869, 0.0930, 0.0930, 0.0342, 0.0930],\n",
      "        [0.0388, 0.0388, 0.7784, 0.1053, 0.0388],\n",
      "        [0.1915, 0.0705, 0.5206, 0.0259, 0.1915],\n",
      "        [0.0117, 0.0861, 0.6364, 0.2341, 0.0317],\n",
      "        [0.0047, 0.0345, 0.6931, 0.0127, 0.2550],\n",
      "        [0.0317, 0.0861, 0.0117, 0.2341, 0.6364],\n",
      "        [0.0146, 0.0397, 0.7979, 0.1080, 0.0397]], device='cuda:0')\n",
      "\n",
      "\n",
      "tensor([[-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094]], device='cuda:0',\n",
      "       grad_fn=<LogSoftmaxBackward>)\n",
      "tensor([[0.0157, 0.0426, 0.0426, 0.0426, 0.8564],\n",
      "        [0.1044, 0.0141, 0.0052, 0.1044, 0.7718],\n",
      "        [0.0202, 0.1491, 0.4053, 0.0202, 0.4053],\n",
      "        [0.1643, 0.0604, 0.4466, 0.1643, 0.1643],\n",
      "        [0.0161, 0.0438, 0.8801, 0.0438, 0.0161],\n",
      "        [0.3483, 0.3483, 0.0471, 0.1281, 0.1281],\n",
      "        [0.1441, 0.3917, 0.0530, 0.0195, 0.3917],\n",
      "        [0.0161, 0.0438, 0.0438, 0.0161, 0.8801]], device='cuda:0')\n",
      "\n",
      "\n",
      "\n",
      "Test set: Accuracy: 2/16 (12%)\n",
      "\n",
      "training epoch 45\n",
      "tensor(0.1596, device='cuda:0', grad_fn=<KlDivBackward>)\n",
      "Train Epoch: 45 [0/67 (0%)]\tLoss: 0.159580\n",
      "tensor(0.2467, device='cuda:0', grad_fn=<KlDivBackward>)\n",
      "tensor(0.2343, device='cuda:0', grad_fn=<KlDivBackward>)\n",
      "tensor(0.2014, device='cuda:0', grad_fn=<KlDivBackward>)\n",
      "tensor(0.2042, device='cuda:0', grad_fn=<KlDivBackward>)\n",
      "tensor(0.2423, device='cuda:0', grad_fn=<KlDivBackward>)\n",
      "Train Epoch: 45 [40/67 (56%)]\tLoss: 0.242285\n",
      "tensor(0.2300, device='cuda:0', grad_fn=<KlDivBackward>)\n",
      "tensor(0.1882, device='cuda:0', grad_fn=<KlDivBackward>)\n",
      "tensor(0.2113, device='cuda:0', grad_fn=<KlDivBackward>)\n",
      "tensor([[-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094]], device='cuda:0',\n",
      "       grad_fn=<LogSoftmaxBackward>)\n",
      "tensor([[0.0157, 0.0426, 0.0426, 0.0426, 0.8564],\n",
      "        [0.0388, 0.0388, 0.7784, 0.1053, 0.0388],\n",
      "        [0.1044, 0.0141, 0.0052, 0.1044, 0.7718],\n",
      "        [0.0117, 0.0861, 0.6364, 0.2341, 0.0317],\n",
      "        [0.0408, 0.0150, 0.8185, 0.0150, 0.1108],\n",
      "        [0.1643, 0.0604, 0.4466, 0.1643, 0.1643],\n",
      "        [0.0146, 0.0397, 0.7979, 0.1080, 0.0397],\n",
      "        [0.0161, 0.0438, 0.8801, 0.0438, 0.0161]], device='cuda:0')\n",
      "\n",
      "\n",
      "tensor([[-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094]], device='cuda:0',\n",
      "       grad_fn=<LogSoftmaxBackward>)\n",
      "tensor([[0.6869, 0.0930, 0.0930, 0.0342, 0.0930],\n",
      "        [0.0161, 0.0438, 0.0438, 0.0161, 0.8801],\n",
      "        [0.1441, 0.3917, 0.0530, 0.0195, 0.3917],\n",
      "        [0.1915, 0.0705, 0.5206, 0.0259, 0.1915],\n",
      "        [0.0202, 0.1491, 0.4053, 0.0202, 0.4053],\n",
      "        [0.3483, 0.3483, 0.0471, 0.1281, 0.1281],\n",
      "        [0.0047, 0.0345, 0.6931, 0.0127, 0.2550],\n",
      "        [0.0317, 0.0861, 0.0117, 0.2341, 0.6364]], device='cuda:0')\n",
      "\n",
      "\n",
      "\n",
      "Test set: Accuracy: 2/16 (12%)\n",
      "\n",
      "training epoch 46\n",
      "tensor(0.2113, device='cuda:0', grad_fn=<KlDivBackward>)\n",
      "Train Epoch: 46 [0/67 (0%)]\tLoss: 0.211341\n",
      "tensor(0.1450, device='cuda:0', grad_fn=<KlDivBackward>)\n",
      "tensor(0.2250, device='cuda:0', grad_fn=<KlDivBackward>)\n",
      "tensor(0.2024, device='cuda:0', grad_fn=<KlDivBackward>)\n",
      "tensor(0.2132, device='cuda:0', grad_fn=<KlDivBackward>)\n",
      "tensor(0.2458, device='cuda:0', grad_fn=<KlDivBackward>)\n",
      "Train Epoch: 46 [40/67 (56%)]\tLoss: 0.245843\n",
      "tensor(0.2283, device='cuda:0', grad_fn=<KlDivBackward>)\n",
      "tensor(0.2308, device='cuda:0', grad_fn=<KlDivBackward>)\n",
      "tensor(0.2244, device='cuda:0', grad_fn=<KlDivBackward>)\n",
      "tensor([[-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094]], device='cuda:0',\n",
      "       grad_fn=<LogSoftmaxBackward>)\n",
      "tensor([[0.0202, 0.1491, 0.4053, 0.0202, 0.4053],\n",
      "        [0.0117, 0.0861, 0.6364, 0.2341, 0.0317],\n",
      "        [0.1915, 0.0705, 0.5206, 0.0259, 0.1915],\n",
      "        [0.0161, 0.0438, 0.8801, 0.0438, 0.0161],\n",
      "        [0.0146, 0.0397, 0.7979, 0.1080, 0.0397],\n",
      "        [0.1044, 0.0141, 0.0052, 0.1044, 0.7718],\n",
      "        [0.6869, 0.0930, 0.0930, 0.0342, 0.0930],\n",
      "        [0.0408, 0.0150, 0.8185, 0.0150, 0.1108]], device='cuda:0')\n",
      "\n",
      "\n",
      "tensor([[-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094]], device='cuda:0',\n",
      "       grad_fn=<LogSoftmaxBackward>)\n",
      "tensor([[0.1643, 0.0604, 0.4466, 0.1643, 0.1643],\n",
      "        [0.0161, 0.0438, 0.0438, 0.0161, 0.8801],\n",
      "        [0.3483, 0.3483, 0.0471, 0.1281, 0.1281],\n",
      "        [0.0388, 0.0388, 0.7784, 0.1053, 0.0388],\n",
      "        [0.0047, 0.0345, 0.6931, 0.0127, 0.2550],\n",
      "        [0.0317, 0.0861, 0.0117, 0.2341, 0.6364],\n",
      "        [0.0157, 0.0426, 0.0426, 0.0426, 0.8564],\n",
      "        [0.1441, 0.3917, 0.0530, 0.0195, 0.3917]], device='cuda:0')\n",
      "\n",
      "\n",
      "\n",
      "Test set: Accuracy: 2/16 (12%)\n",
      "\n",
      "training epoch 47\n",
      "tensor(0.2065, device='cuda:0', grad_fn=<KlDivBackward>)\n",
      "Train Epoch: 47 [0/67 (0%)]\tLoss: 0.206482\n",
      "tensor(0.2568, device='cuda:0', grad_fn=<KlDivBackward>)\n",
      "tensor(0.2601, device='cuda:0', grad_fn=<KlDivBackward>)\n",
      "tensor(0.1942, device='cuda:0', grad_fn=<KlDivBackward>)\n",
      "tensor(0.2166, device='cuda:0', grad_fn=<KlDivBackward>)\n",
      "tensor(0.1926, device='cuda:0', grad_fn=<KlDivBackward>)\n",
      "Train Epoch: 47 [40/67 (56%)]\tLoss: 0.192614\n",
      "tensor(0.1987, device='cuda:0', grad_fn=<KlDivBackward>)\n",
      "tensor(0.1765, device='cuda:0', grad_fn=<KlDivBackward>)\n",
      "tensor(0.2238, device='cuda:0', grad_fn=<KlDivBackward>)\n",
      "tensor([[-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094]], device='cuda:0',\n",
      "       grad_fn=<LogSoftmaxBackward>)\n",
      "tensor([[0.0146, 0.0397, 0.7979, 0.1080, 0.0397],\n",
      "        [0.0157, 0.0426, 0.0426, 0.0426, 0.8564],\n",
      "        [0.1643, 0.0604, 0.4466, 0.1643, 0.1643],\n",
      "        [0.0202, 0.1491, 0.4053, 0.0202, 0.4053],\n",
      "        [0.1044, 0.0141, 0.0052, 0.1044, 0.7718],\n",
      "        [0.6869, 0.0930, 0.0930, 0.0342, 0.0930],\n",
      "        [0.0388, 0.0388, 0.7784, 0.1053, 0.0388],\n",
      "        [0.0317, 0.0861, 0.0117, 0.2341, 0.6364]], device='cuda:0')\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094]], device='cuda:0',\n",
      "       grad_fn=<LogSoftmaxBackward>)\n",
      "tensor([[0.0161, 0.0438, 0.8801, 0.0438, 0.0161],\n",
      "        [0.3483, 0.3483, 0.0471, 0.1281, 0.1281],\n",
      "        [0.0047, 0.0345, 0.6931, 0.0127, 0.2550],\n",
      "        [0.1915, 0.0705, 0.5206, 0.0259, 0.1915],\n",
      "        [0.0117, 0.0861, 0.6364, 0.2341, 0.0317],\n",
      "        [0.1441, 0.3917, 0.0530, 0.0195, 0.3917],\n",
      "        [0.0408, 0.0150, 0.8185, 0.0150, 0.1108],\n",
      "        [0.0161, 0.0438, 0.0438, 0.0161, 0.8801]], device='cuda:0')\n",
      "\n",
      "\n",
      "\n",
      "Test set: Accuracy: 2/16 (12%)\n",
      "\n",
      "training epoch 48\n",
      "tensor(0.2007, device='cuda:0', grad_fn=<KlDivBackward>)\n",
      "Train Epoch: 48 [0/67 (0%)]\tLoss: 0.200718\n",
      "tensor(0.2079, device='cuda:0', grad_fn=<KlDivBackward>)\n",
      "tensor(0.2366, device='cuda:0', grad_fn=<KlDivBackward>)\n",
      "tensor(0.2298, device='cuda:0', grad_fn=<KlDivBackward>)\n",
      "tensor(0.1927, device='cuda:0', grad_fn=<KlDivBackward>)\n",
      "tensor(0.2336, device='cuda:0', grad_fn=<KlDivBackward>)\n",
      "Train Epoch: 48 [40/67 (56%)]\tLoss: 0.233619\n",
      "tensor(0.2272, device='cuda:0', grad_fn=<KlDivBackward>)\n",
      "tensor(0.1682, device='cuda:0', grad_fn=<KlDivBackward>)\n",
      "tensor(0.2379, device='cuda:0', grad_fn=<KlDivBackward>)\n",
      "tensor([[-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094]], device='cuda:0',\n",
      "       grad_fn=<LogSoftmaxBackward>)\n",
      "tensor([[0.0317, 0.0861, 0.0117, 0.2341, 0.6364],\n",
      "        [0.1915, 0.0705, 0.5206, 0.0259, 0.1915],\n",
      "        [0.1044, 0.0141, 0.0052, 0.1044, 0.7718],\n",
      "        [0.6869, 0.0930, 0.0930, 0.0342, 0.0930],\n",
      "        [0.1643, 0.0604, 0.4466, 0.1643, 0.1643],\n",
      "        [0.0408, 0.0150, 0.8185, 0.0150, 0.1108],\n",
      "        [0.3483, 0.3483, 0.0471, 0.1281, 0.1281],\n",
      "        [0.0202, 0.1491, 0.4053, 0.0202, 0.4053]], device='cuda:0')\n",
      "\n",
      "\n",
      "tensor([[-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094]], device='cuda:0',\n",
      "       grad_fn=<LogSoftmaxBackward>)\n",
      "tensor([[0.0146, 0.0397, 0.7979, 0.1080, 0.0397],\n",
      "        [0.0117, 0.0861, 0.6364, 0.2341, 0.0317],\n",
      "        [0.0161, 0.0438, 0.0438, 0.0161, 0.8801],\n",
      "        [0.0388, 0.0388, 0.7784, 0.1053, 0.0388],\n",
      "        [0.0157, 0.0426, 0.0426, 0.0426, 0.8564],\n",
      "        [0.1441, 0.3917, 0.0530, 0.0195, 0.3917],\n",
      "        [0.0047, 0.0345, 0.6931, 0.0127, 0.2550],\n",
      "        [0.0161, 0.0438, 0.8801, 0.0438, 0.0161]], device='cuda:0')\n",
      "\n",
      "\n",
      "\n",
      "Test set: Accuracy: 2/16 (12%)\n",
      "\n",
      "training epoch 49\n",
      "tensor(0.2153, device='cuda:0', grad_fn=<KlDivBackward>)\n",
      "Train Epoch: 49 [0/67 (0%)]\tLoss: 0.215309\n",
      "tensor(0.2361, device='cuda:0', grad_fn=<KlDivBackward>)\n",
      "tensor(0.2603, device='cuda:0', grad_fn=<KlDivBackward>)\n",
      "tensor(0.1509, device='cuda:0', grad_fn=<KlDivBackward>)\n",
      "tensor(0.2010, device='cuda:0', grad_fn=<KlDivBackward>)\n",
      "tensor(0.2313, device='cuda:0', grad_fn=<KlDivBackward>)\n",
      "Train Epoch: 49 [40/67 (56%)]\tLoss: 0.231251\n",
      "tensor(0.1782, device='cuda:0', grad_fn=<KlDivBackward>)\n",
      "tensor(0.2294, device='cuda:0', grad_fn=<KlDivBackward>)\n",
      "tensor(0.2224, device='cuda:0', grad_fn=<KlDivBackward>)\n",
      "tensor([[-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094]], device='cuda:0',\n",
      "       grad_fn=<LogSoftmaxBackward>)\n",
      "tensor([[0.0388, 0.0388, 0.7784, 0.1053, 0.0388],\n",
      "        [0.0157, 0.0426, 0.0426, 0.0426, 0.8564],\n",
      "        [0.1441, 0.3917, 0.0530, 0.0195, 0.3917],\n",
      "        [0.0117, 0.0861, 0.6364, 0.2341, 0.0317],\n",
      "        [0.0146, 0.0397, 0.7979, 0.1080, 0.0397],\n",
      "        [0.0047, 0.0345, 0.6931, 0.0127, 0.2550],\n",
      "        [0.1643, 0.0604, 0.4466, 0.1643, 0.1643],\n",
      "        [0.0161, 0.0438, 0.8801, 0.0438, 0.0161]], device='cuda:0')\n",
      "\n",
      "\n",
      "tensor([[-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094]], device='cuda:0',\n",
      "       grad_fn=<LogSoftmaxBackward>)\n",
      "tensor([[0.0408, 0.0150, 0.8185, 0.0150, 0.1108],\n",
      "        [0.0161, 0.0438, 0.0438, 0.0161, 0.8801],\n",
      "        [0.0202, 0.1491, 0.4053, 0.0202, 0.4053],\n",
      "        [0.6869, 0.0930, 0.0930, 0.0342, 0.0930],\n",
      "        [0.1915, 0.0705, 0.5206, 0.0259, 0.1915],\n",
      "        [0.3483, 0.3483, 0.0471, 0.1281, 0.1281],\n",
      "        [0.0317, 0.0861, 0.0117, 0.2341, 0.6364],\n",
      "        [0.1044, 0.0141, 0.0052, 0.1044, 0.7718]], device='cuda:0')\n",
      "\n",
      "\n",
      "\n",
      "Test set: Accuracy: 2/16 (12%)\n",
      "\n",
      "training epoch 50\n",
      "tensor(0.2568, device='cuda:0', grad_fn=<KlDivBackward>)\n",
      "Train Epoch: 50 [0/67 (0%)]\tLoss: 0.256821\n",
      "tensor(0.1675, device='cuda:0', grad_fn=<KlDivBackward>)\n",
      "tensor(0.2253, device='cuda:0', grad_fn=<KlDivBackward>)\n",
      "tensor(0.2311, device='cuda:0', grad_fn=<KlDivBackward>)\n",
      "tensor(0.1850, device='cuda:0', grad_fn=<KlDivBackward>)\n",
      "tensor(0.2397, device='cuda:0', grad_fn=<KlDivBackward>)\n",
      "Train Epoch: 50 [40/67 (56%)]\tLoss: 0.239744\n",
      "tensor(0.2065, device='cuda:0', grad_fn=<KlDivBackward>)\n",
      "tensor(0.1993, device='cuda:0', grad_fn=<KlDivBackward>)\n",
      "tensor(0.1994, device='cuda:0', grad_fn=<KlDivBackward>)\n",
      "tensor([[-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094]], device='cuda:0',\n",
      "       grad_fn=<LogSoftmaxBackward>)\n",
      "tensor([[0.0161, 0.0438, 0.0438, 0.0161, 0.8801],\n",
      "        [0.0047, 0.0345, 0.6931, 0.0127, 0.2550],\n",
      "        [0.1643, 0.0604, 0.4466, 0.1643, 0.1643],\n",
      "        [0.0317, 0.0861, 0.0117, 0.2341, 0.6364],\n",
      "        [0.0408, 0.0150, 0.8185, 0.0150, 0.1108],\n",
      "        [0.1915, 0.0705, 0.5206, 0.0259, 0.1915],\n",
      "        [0.0117, 0.0861, 0.6364, 0.2341, 0.0317],\n",
      "        [0.0388, 0.0388, 0.7784, 0.1053, 0.0388]], device='cuda:0')\n",
      "\n",
      "\n",
      "tensor([[-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094]], device='cuda:0',\n",
      "       grad_fn=<LogSoftmaxBackward>)\n",
      "tensor([[0.1044, 0.0141, 0.0052, 0.1044, 0.7718],\n",
      "        [0.6869, 0.0930, 0.0930, 0.0342, 0.0930],\n",
      "        [0.0202, 0.1491, 0.4053, 0.0202, 0.4053],\n",
      "        [0.0157, 0.0426, 0.0426, 0.0426, 0.8564],\n",
      "        [0.0146, 0.0397, 0.7979, 0.1080, 0.0397],\n",
      "        [0.0161, 0.0438, 0.8801, 0.0438, 0.0161],\n",
      "        [0.3483, 0.3483, 0.0471, 0.1281, 0.1281],\n",
      "        [0.1441, 0.3917, 0.0530, 0.0195, 0.3917]], device='cuda:0')\n",
      "\n",
      "\n",
      "\n",
      "Test set: Accuracy: 2/16 (12%)\n",
      "\n",
      "training epoch 51\n",
      "tensor(0.2347, device='cuda:0', grad_fn=<KlDivBackward>)\n",
      "Train Epoch: 51 [0/67 (0%)]\tLoss: 0.234747\n",
      "tensor(0.2297, device='cuda:0', grad_fn=<KlDivBackward>)\n",
      "tensor(0.2106, device='cuda:0', grad_fn=<KlDivBackward>)\n",
      "tensor(0.1650, device='cuda:0', grad_fn=<KlDivBackward>)\n",
      "tensor(0.1619, device='cuda:0', grad_fn=<KlDivBackward>)\n",
      "tensor(0.2543, device='cuda:0', grad_fn=<KlDivBackward>)\n",
      "Train Epoch: 51 [40/67 (56%)]\tLoss: 0.254280\n",
      "tensor(0.2059, device='cuda:0', grad_fn=<KlDivBackward>)\n",
      "tensor(0.2748, device='cuda:0', grad_fn=<KlDivBackward>)\n",
      "tensor(0.1308, device='cuda:0', grad_fn=<KlDivBackward>)\n",
      "tensor([[-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094]], device='cuda:0',\n",
      "       grad_fn=<LogSoftmaxBackward>)\n",
      "tensor([[0.0047, 0.0345, 0.6931, 0.0127, 0.2550],\n",
      "        [0.3483, 0.3483, 0.0471, 0.1281, 0.1281],\n",
      "        [0.0408, 0.0150, 0.8185, 0.0150, 0.1108],\n",
      "        [0.0117, 0.0861, 0.6364, 0.2341, 0.0317],\n",
      "        [0.1643, 0.0604, 0.4466, 0.1643, 0.1643],\n",
      "        [0.0157, 0.0426, 0.0426, 0.0426, 0.8564],\n",
      "        [0.1044, 0.0141, 0.0052, 0.1044, 0.7718],\n",
      "        [0.0388, 0.0388, 0.7784, 0.1053, 0.0388]], device='cuda:0')\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094]], device='cuda:0',\n",
      "       grad_fn=<LogSoftmaxBackward>)\n",
      "tensor([[0.0317, 0.0861, 0.0117, 0.2341, 0.6364],\n",
      "        [0.6869, 0.0930, 0.0930, 0.0342, 0.0930],\n",
      "        [0.1441, 0.3917, 0.0530, 0.0195, 0.3917],\n",
      "        [0.0161, 0.0438, 0.0438, 0.0161, 0.8801],\n",
      "        [0.0146, 0.0397, 0.7979, 0.1080, 0.0397],\n",
      "        [0.0161, 0.0438, 0.8801, 0.0438, 0.0161],\n",
      "        [0.0202, 0.1491, 0.4053, 0.0202, 0.4053],\n",
      "        [0.1915, 0.0705, 0.5206, 0.0259, 0.1915]], device='cuda:0')\n",
      "\n",
      "\n",
      "\n",
      "Test set: Accuracy: 2/16 (12%)\n",
      "\n",
      "training epoch 52\n",
      "tensor(0.1901, device='cuda:0', grad_fn=<KlDivBackward>)\n",
      "Train Epoch: 52 [0/67 (0%)]\tLoss: 0.190095\n",
      "tensor(0.2160, device='cuda:0', grad_fn=<KlDivBackward>)\n",
      "tensor(0.2402, device='cuda:0', grad_fn=<KlDivBackward>)\n",
      "tensor(0.1940, device='cuda:0', grad_fn=<KlDivBackward>)\n",
      "tensor(0.2319, device='cuda:0', grad_fn=<KlDivBackward>)\n",
      "tensor(0.2051, device='cuda:0', grad_fn=<KlDivBackward>)\n",
      "Train Epoch: 52 [40/67 (56%)]\tLoss: 0.205145\n",
      "tensor(0.2191, device='cuda:0', grad_fn=<KlDivBackward>)\n",
      "tensor(0.2017, device='cuda:0', grad_fn=<KlDivBackward>)\n",
      "tensor(0.2342, device='cuda:0', grad_fn=<KlDivBackward>)\n",
      "tensor([[-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094]], device='cuda:0',\n",
      "       grad_fn=<LogSoftmaxBackward>)\n",
      "tensor([[0.3483, 0.3483, 0.0471, 0.1281, 0.1281],\n",
      "        [0.0157, 0.0426, 0.0426, 0.0426, 0.8564],\n",
      "        [0.0317, 0.0861, 0.0117, 0.2341, 0.6364],\n",
      "        [0.0047, 0.0345, 0.6931, 0.0127, 0.2550],\n",
      "        [0.1915, 0.0705, 0.5206, 0.0259, 0.1915],\n",
      "        [0.0408, 0.0150, 0.8185, 0.0150, 0.1108],\n",
      "        [0.0161, 0.0438, 0.8801, 0.0438, 0.0161],\n",
      "        [0.1643, 0.0604, 0.4466, 0.1643, 0.1643]], device='cuda:0')\n",
      "\n",
      "\n",
      "tensor([[-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094]], device='cuda:0',\n",
      "       grad_fn=<LogSoftmaxBackward>)\n",
      "tensor([[0.0117, 0.0861, 0.6364, 0.2341, 0.0317],\n",
      "        [0.0388, 0.0388, 0.7784, 0.1053, 0.0388],\n",
      "        [0.1441, 0.3917, 0.0530, 0.0195, 0.3917],\n",
      "        [0.0161, 0.0438, 0.0438, 0.0161, 0.8801],\n",
      "        [0.0202, 0.1491, 0.4053, 0.0202, 0.4053],\n",
      "        [0.6869, 0.0930, 0.0930, 0.0342, 0.0930],\n",
      "        [0.1044, 0.0141, 0.0052, 0.1044, 0.7718],\n",
      "        [0.0146, 0.0397, 0.7979, 0.1080, 0.0397]], device='cuda:0')\n",
      "\n",
      "\n",
      "\n",
      "Test set: Accuracy: 2/16 (12%)\n",
      "\n",
      "training epoch 53\n",
      "tensor(0.2595, device='cuda:0', grad_fn=<KlDivBackward>)\n",
      "Train Epoch: 53 [0/67 (0%)]\tLoss: 0.259510\n",
      "tensor(0.2345, device='cuda:0', grad_fn=<KlDivBackward>)\n",
      "tensor(0.1718, device='cuda:0', grad_fn=<KlDivBackward>)\n",
      "tensor(0.2493, device='cuda:0', grad_fn=<KlDivBackward>)\n",
      "tensor(0.2116, device='cuda:0', grad_fn=<KlDivBackward>)\n",
      "tensor(0.2136, device='cuda:0', grad_fn=<KlDivBackward>)\n",
      "Train Epoch: 53 [40/67 (56%)]\tLoss: 0.213606\n",
      "tensor(0.1733, device='cuda:0', grad_fn=<KlDivBackward>)\n",
      "tensor(0.1962, device='cuda:0', grad_fn=<KlDivBackward>)\n",
      "tensor(0.2030, device='cuda:0', grad_fn=<KlDivBackward>)\n",
      "tensor([[-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094]], device='cuda:0',\n",
      "       grad_fn=<LogSoftmaxBackward>)\n",
      "tensor([[0.0388, 0.0388, 0.7784, 0.1053, 0.0388],\n",
      "        [0.3483, 0.3483, 0.0471, 0.1281, 0.1281],\n",
      "        [0.0047, 0.0345, 0.6931, 0.0127, 0.2550],\n",
      "        [0.1044, 0.0141, 0.0052, 0.1044, 0.7718],\n",
      "        [0.1441, 0.3917, 0.0530, 0.0195, 0.3917],\n",
      "        [0.1643, 0.0604, 0.4466, 0.1643, 0.1643],\n",
      "        [0.6869, 0.0930, 0.0930, 0.0342, 0.0930],\n",
      "        [0.0161, 0.0438, 0.8801, 0.0438, 0.0161]], device='cuda:0')\n",
      "\n",
      "\n",
      "tensor([[-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094]], device='cuda:0',\n",
      "       grad_fn=<LogSoftmaxBackward>)\n",
      "tensor([[0.0157, 0.0426, 0.0426, 0.0426, 0.8564],\n",
      "        [0.0161, 0.0438, 0.0438, 0.0161, 0.8801],\n",
      "        [0.0202, 0.1491, 0.4053, 0.0202, 0.4053],\n",
      "        [0.0146, 0.0397, 0.7979, 0.1080, 0.0397],\n",
      "        [0.0317, 0.0861, 0.0117, 0.2341, 0.6364],\n",
      "        [0.0117, 0.0861, 0.6364, 0.2341, 0.0317],\n",
      "        [0.1915, 0.0705, 0.5206, 0.0259, 0.1915],\n",
      "        [0.0408, 0.0150, 0.8185, 0.0150, 0.1108]], device='cuda:0')\n",
      "\n",
      "\n",
      "\n",
      "Test set: Accuracy: 2/16 (12%)\n",
      "\n",
      "training epoch 54\n",
      "tensor(0.1849, device='cuda:0', grad_fn=<KlDivBackward>)\n",
      "Train Epoch: 54 [0/67 (0%)]\tLoss: 0.184893\n",
      "tensor(0.1806, device='cuda:0', grad_fn=<KlDivBackward>)\n",
      "tensor(0.2278, device='cuda:0', grad_fn=<KlDivBackward>)\n",
      "tensor(0.2748, device='cuda:0', grad_fn=<KlDivBackward>)\n",
      "tensor(0.1683, device='cuda:0', grad_fn=<KlDivBackward>)\n",
      "tensor(0.2492, device='cuda:0', grad_fn=<KlDivBackward>)\n",
      "Train Epoch: 54 [40/67 (56%)]\tLoss: 0.249182\n",
      "tensor(0.2033, device='cuda:0', grad_fn=<KlDivBackward>)\n",
      "tensor(0.2106, device='cuda:0', grad_fn=<KlDivBackward>)\n",
      "tensor(0.2304, device='cuda:0', grad_fn=<KlDivBackward>)\n",
      "tensor([[-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094]], device='cuda:0',\n",
      "       grad_fn=<LogSoftmaxBackward>)\n",
      "tensor([[0.0202, 0.1491, 0.4053, 0.0202, 0.4053],\n",
      "        [0.1643, 0.0604, 0.4466, 0.1643, 0.1643],\n",
      "        [0.0117, 0.0861, 0.6364, 0.2341, 0.0317],\n",
      "        [0.0388, 0.0388, 0.7784, 0.1053, 0.0388],\n",
      "        [0.0161, 0.0438, 0.8801, 0.0438, 0.0161],\n",
      "        [0.0408, 0.0150, 0.8185, 0.0150, 0.1108],\n",
      "        [0.0161, 0.0438, 0.0438, 0.0161, 0.8801],\n",
      "        [0.0317, 0.0861, 0.0117, 0.2341, 0.6364]], device='cuda:0')\n",
      "\n",
      "\n",
      "tensor([[-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094]], device='cuda:0',\n",
      "       grad_fn=<LogSoftmaxBackward>)\n",
      "tensor([[0.1441, 0.3917, 0.0530, 0.0195, 0.3917],\n",
      "        [0.1044, 0.0141, 0.0052, 0.1044, 0.7718],\n",
      "        [0.0146, 0.0397, 0.7979, 0.1080, 0.0397],\n",
      "        [0.3483, 0.3483, 0.0471, 0.1281, 0.1281],\n",
      "        [0.0047, 0.0345, 0.6931, 0.0127, 0.2550],\n",
      "        [0.0157, 0.0426, 0.0426, 0.0426, 0.8564],\n",
      "        [0.6869, 0.0930, 0.0930, 0.0342, 0.0930],\n",
      "        [0.1915, 0.0705, 0.5206, 0.0259, 0.1915]], device='cuda:0')\n",
      "\n",
      "\n",
      "\n",
      "Test set: Accuracy: 2/16 (12%)\n",
      "\n",
      "training epoch 55\n",
      "tensor(0.2527, device='cuda:0', grad_fn=<KlDivBackward>)\n",
      "Train Epoch: 55 [0/67 (0%)]\tLoss: 0.252733\n",
      "tensor(0.2294, device='cuda:0', grad_fn=<KlDivBackward>)\n",
      "tensor(0.2096, device='cuda:0', grad_fn=<KlDivBackward>)\n",
      "tensor(0.1870, device='cuda:0', grad_fn=<KlDivBackward>)\n",
      "tensor(0.1982, device='cuda:0', grad_fn=<KlDivBackward>)\n",
      "tensor(0.2121, device='cuda:0', grad_fn=<KlDivBackward>)\n",
      "Train Epoch: 55 [40/67 (56%)]\tLoss: 0.212095\n",
      "tensor(0.2078, device='cuda:0', grad_fn=<KlDivBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.2008, device='cuda:0', grad_fn=<KlDivBackward>)\n",
      "tensor(0.2357, device='cuda:0', grad_fn=<KlDivBackward>)\n",
      "tensor([[-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094]], device='cuda:0',\n",
      "       grad_fn=<LogSoftmaxBackward>)\n",
      "tensor([[0.1915, 0.0705, 0.5206, 0.0259, 0.1915],\n",
      "        [0.1643, 0.0604, 0.4466, 0.1643, 0.1643],\n",
      "        [0.0317, 0.0861, 0.0117, 0.2341, 0.6364],\n",
      "        [0.0146, 0.0397, 0.7979, 0.1080, 0.0397],\n",
      "        [0.1441, 0.3917, 0.0530, 0.0195, 0.3917],\n",
      "        [0.0161, 0.0438, 0.8801, 0.0438, 0.0161],\n",
      "        [0.0047, 0.0345, 0.6931, 0.0127, 0.2550],\n",
      "        [0.0161, 0.0438, 0.0438, 0.0161, 0.8801]], device='cuda:0')\n",
      "\n",
      "\n",
      "tensor([[-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094]], device='cuda:0',\n",
      "       grad_fn=<LogSoftmaxBackward>)\n",
      "tensor([[0.0408, 0.0150, 0.8185, 0.0150, 0.1108],\n",
      "        [0.1044, 0.0141, 0.0052, 0.1044, 0.7718],\n",
      "        [0.0117, 0.0861, 0.6364, 0.2341, 0.0317],\n",
      "        [0.0388, 0.0388, 0.7784, 0.1053, 0.0388],\n",
      "        [0.3483, 0.3483, 0.0471, 0.1281, 0.1281],\n",
      "        [0.0157, 0.0426, 0.0426, 0.0426, 0.8564],\n",
      "        [0.0202, 0.1491, 0.4053, 0.0202, 0.4053],\n",
      "        [0.6869, 0.0930, 0.0930, 0.0342, 0.0930]], device='cuda:0')\n",
      "\n",
      "\n",
      "\n",
      "Test set: Accuracy: 2/16 (12%)\n",
      "\n",
      "training epoch 56\n",
      "tensor(0.2379, device='cuda:0', grad_fn=<KlDivBackward>)\n",
      "Train Epoch: 56 [0/67 (0%)]\tLoss: 0.237922\n",
      "tensor(0.2237, device='cuda:0', grad_fn=<KlDivBackward>)\n",
      "tensor(0.2305, device='cuda:0', grad_fn=<KlDivBackward>)\n",
      "tensor(0.2230, device='cuda:0', grad_fn=<KlDivBackward>)\n",
      "tensor(0.2135, device='cuda:0', grad_fn=<KlDivBackward>)\n",
      "tensor(0.2057, device='cuda:0', grad_fn=<KlDivBackward>)\n",
      "Train Epoch: 56 [40/67 (56%)]\tLoss: 0.205735\n",
      "tensor(0.1927, device='cuda:0', grad_fn=<KlDivBackward>)\n",
      "tensor(0.1916, device='cuda:0', grad_fn=<KlDivBackward>)\n",
      "tensor(0.1793, device='cuda:0', grad_fn=<KlDivBackward>)\n",
      "tensor([[-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094]], device='cuda:0',\n",
      "       grad_fn=<LogSoftmaxBackward>)\n",
      "tensor([[0.0157, 0.0426, 0.0426, 0.0426, 0.8564],\n",
      "        [0.1044, 0.0141, 0.0052, 0.1044, 0.7718],\n",
      "        [0.1441, 0.3917, 0.0530, 0.0195, 0.3917],\n",
      "        [0.6869, 0.0930, 0.0930, 0.0342, 0.0930],\n",
      "        [0.0317, 0.0861, 0.0117, 0.2341, 0.6364],\n",
      "        [0.3483, 0.3483, 0.0471, 0.1281, 0.1281],\n",
      "        [0.0161, 0.0438, 0.0438, 0.0161, 0.8801],\n",
      "        [0.1915, 0.0705, 0.5206, 0.0259, 0.1915]], device='cuda:0')\n",
      "\n",
      "\n",
      "tensor([[-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094]], device='cuda:0',\n",
      "       grad_fn=<LogSoftmaxBackward>)\n",
      "tensor([[0.0146, 0.0397, 0.7979, 0.1080, 0.0397],\n",
      "        [0.1643, 0.0604, 0.4466, 0.1643, 0.1643],\n",
      "        [0.0117, 0.0861, 0.6364, 0.2341, 0.0317],\n",
      "        [0.0388, 0.0388, 0.7784, 0.1053, 0.0388],\n",
      "        [0.0047, 0.0345, 0.6931, 0.0127, 0.2550],\n",
      "        [0.0202, 0.1491, 0.4053, 0.0202, 0.4053],\n",
      "        [0.0161, 0.0438, 0.8801, 0.0438, 0.0161],\n",
      "        [0.0408, 0.0150, 0.8185, 0.0150, 0.1108]], device='cuda:0')\n",
      "\n",
      "\n",
      "\n",
      "Test set: Accuracy: 2/16 (12%)\n",
      "\n",
      "training epoch 57\n",
      "tensor(0.1601, device='cuda:0', grad_fn=<KlDivBackward>)\n",
      "Train Epoch: 57 [0/67 (0%)]\tLoss: 0.160061\n",
      "tensor(0.2209, device='cuda:0', grad_fn=<KlDivBackward>)\n",
      "tensor(0.2536, device='cuda:0', grad_fn=<KlDivBackward>)\n",
      "tensor(0.1917, device='cuda:0', grad_fn=<KlDivBackward>)\n",
      "tensor(0.2300, device='cuda:0', grad_fn=<KlDivBackward>)\n",
      "tensor(0.1919, device='cuda:0', grad_fn=<KlDivBackward>)\n",
      "Train Epoch: 57 [40/67 (56%)]\tLoss: 0.191885\n",
      "tensor(0.2428, device='cuda:0', grad_fn=<KlDivBackward>)\n",
      "tensor(0.2406, device='cuda:0', grad_fn=<KlDivBackward>)\n",
      "tensor(0.1450, device='cuda:0', grad_fn=<KlDivBackward>)\n",
      "tensor([[-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094]], device='cuda:0',\n",
      "       grad_fn=<LogSoftmaxBackward>)\n",
      "tensor([[0.0202, 0.1491, 0.4053, 0.0202, 0.4053],\n",
      "        [0.1441, 0.3917, 0.0530, 0.0195, 0.3917],\n",
      "        [0.0047, 0.0345, 0.6931, 0.0127, 0.2550],\n",
      "        [0.1044, 0.0141, 0.0052, 0.1044, 0.7718],\n",
      "        [0.0388, 0.0388, 0.7784, 0.1053, 0.0388],\n",
      "        [0.0408, 0.0150, 0.8185, 0.0150, 0.1108],\n",
      "        [0.1915, 0.0705, 0.5206, 0.0259, 0.1915],\n",
      "        [0.1643, 0.0604, 0.4466, 0.1643, 0.1643]], device='cuda:0')\n",
      "\n",
      "\n",
      "tensor([[-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094]], device='cuda:0',\n",
      "       grad_fn=<LogSoftmaxBackward>)\n",
      "tensor([[0.6869, 0.0930, 0.0930, 0.0342, 0.0930],\n",
      "        [0.3483, 0.3483, 0.0471, 0.1281, 0.1281],\n",
      "        [0.0161, 0.0438, 0.8801, 0.0438, 0.0161],\n",
      "        [0.0161, 0.0438, 0.0438, 0.0161, 0.8801],\n",
      "        [0.0317, 0.0861, 0.0117, 0.2341, 0.6364],\n",
      "        [0.0157, 0.0426, 0.0426, 0.0426, 0.8564],\n",
      "        [0.0146, 0.0397, 0.7979, 0.1080, 0.0397],\n",
      "        [0.0117, 0.0861, 0.6364, 0.2341, 0.0317]], device='cuda:0')\n",
      "\n",
      "\n",
      "\n",
      "Test set: Accuracy: 2/16 (12%)\n",
      "\n",
      "training epoch 58\n",
      "tensor(0.2324, device='cuda:0', grad_fn=<KlDivBackward>)\n",
      "Train Epoch: 58 [0/67 (0%)]\tLoss: 0.232367\n",
      "tensor(0.1914, device='cuda:0', grad_fn=<KlDivBackward>)\n",
      "tensor(0.2194, device='cuda:0', grad_fn=<KlDivBackward>)\n",
      "tensor(0.1902, device='cuda:0', grad_fn=<KlDivBackward>)\n",
      "tensor(0.2402, device='cuda:0', grad_fn=<KlDivBackward>)\n",
      "tensor(0.2258, device='cuda:0', grad_fn=<KlDivBackward>)\n",
      "Train Epoch: 58 [40/67 (56%)]\tLoss: 0.225807\n",
      "tensor(0.2157, device='cuda:0', grad_fn=<KlDivBackward>)\n",
      "tensor(0.1949, device='cuda:0', grad_fn=<KlDivBackward>)\n",
      "tensor(0.2028, device='cuda:0', grad_fn=<KlDivBackward>)\n",
      "tensor([[-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094]], device='cuda:0',\n",
      "       grad_fn=<LogSoftmaxBackward>)\n",
      "tensor([[0.0146, 0.0397, 0.7979, 0.1080, 0.0397],\n",
      "        [0.0202, 0.1491, 0.4053, 0.0202, 0.4053],\n",
      "        [0.6869, 0.0930, 0.0930, 0.0342, 0.0930],\n",
      "        [0.0161, 0.0438, 0.0438, 0.0161, 0.8801],\n",
      "        [0.1441, 0.3917, 0.0530, 0.0195, 0.3917],\n",
      "        [0.0161, 0.0438, 0.8801, 0.0438, 0.0161],\n",
      "        [0.0157, 0.0426, 0.0426, 0.0426, 0.8564],\n",
      "        [0.0388, 0.0388, 0.7784, 0.1053, 0.0388]], device='cuda:0')\n",
      "\n",
      "\n",
      "tensor([[-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094]], device='cuda:0',\n",
      "       grad_fn=<LogSoftmaxBackward>)\n",
      "tensor([[0.0408, 0.0150, 0.8185, 0.0150, 0.1108],\n",
      "        [0.1044, 0.0141, 0.0052, 0.1044, 0.7718],\n",
      "        [0.1643, 0.0604, 0.4466, 0.1643, 0.1643],\n",
      "        [0.0047, 0.0345, 0.6931, 0.0127, 0.2550],\n",
      "        [0.1915, 0.0705, 0.5206, 0.0259, 0.1915],\n",
      "        [0.0117, 0.0861, 0.6364, 0.2341, 0.0317],\n",
      "        [0.3483, 0.3483, 0.0471, 0.1281, 0.1281],\n",
      "        [0.0317, 0.0861, 0.0117, 0.2341, 0.6364]], device='cuda:0')\n",
      "\n",
      "\n",
      "\n",
      "Test set: Accuracy: 2/16 (12%)\n",
      "\n",
      "training epoch 59\n",
      "tensor(0.2080, device='cuda:0', grad_fn=<KlDivBackward>)\n",
      "Train Epoch: 59 [0/67 (0%)]\tLoss: 0.208002\n",
      "tensor(0.2074, device='cuda:0', grad_fn=<KlDivBackward>)\n",
      "tensor(0.1796, device='cuda:0', grad_fn=<KlDivBackward>)\n",
      "tensor(0.1983, device='cuda:0', grad_fn=<KlDivBackward>)\n",
      "tensor(0.2257, device='cuda:0', grad_fn=<KlDivBackward>)\n",
      "tensor(0.2633, device='cuda:0', grad_fn=<KlDivBackward>)\n",
      "Train Epoch: 59 [40/67 (56%)]\tLoss: 0.263288\n",
      "tensor(0.2170, device='cuda:0', grad_fn=<KlDivBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.2227, device='cuda:0', grad_fn=<KlDivBackward>)\n",
      "tensor(0.1706, device='cuda:0', grad_fn=<KlDivBackward>)\n",
      "tensor([[-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094]], device='cuda:0',\n",
      "       grad_fn=<LogSoftmaxBackward>)\n",
      "tensor([[0.0161, 0.0438, 0.0438, 0.0161, 0.8801],\n",
      "        [0.0388, 0.0388, 0.7784, 0.1053, 0.0388],\n",
      "        [0.0157, 0.0426, 0.0426, 0.0426, 0.8564],\n",
      "        [0.0317, 0.0861, 0.0117, 0.2341, 0.6364],\n",
      "        [0.1441, 0.3917, 0.0530, 0.0195, 0.3917],\n",
      "        [0.3483, 0.3483, 0.0471, 0.1281, 0.1281],\n",
      "        [0.1915, 0.0705, 0.5206, 0.0259, 0.1915],\n",
      "        [0.0146, 0.0397, 0.7979, 0.1080, 0.0397]], device='cuda:0')\n",
      "\n",
      "\n",
      "tensor([[-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094]], device='cuda:0',\n",
      "       grad_fn=<LogSoftmaxBackward>)\n",
      "tensor([[0.0047, 0.0345, 0.6931, 0.0127, 0.2550],\n",
      "        [0.0408, 0.0150, 0.8185, 0.0150, 0.1108],\n",
      "        [0.0202, 0.1491, 0.4053, 0.0202, 0.4053],\n",
      "        [0.1643, 0.0604, 0.4466, 0.1643, 0.1643],\n",
      "        [0.0161, 0.0438, 0.8801, 0.0438, 0.0161],\n",
      "        [0.0117, 0.0861, 0.6364, 0.2341, 0.0317],\n",
      "        [0.6869, 0.0930, 0.0930, 0.0342, 0.0930],\n",
      "        [0.1044, 0.0141, 0.0052, 0.1044, 0.7718]], device='cuda:0')\n",
      "\n",
      "\n",
      "\n",
      "Test set: Accuracy: 2/16 (12%)\n",
      "\n",
      "training epoch 60\n",
      "tensor(0.2237, device='cuda:0', grad_fn=<KlDivBackward>)\n",
      "Train Epoch: 60 [0/67 (0%)]\tLoss: 0.223666\n",
      "tensor(0.2372, device='cuda:0', grad_fn=<KlDivBackward>)\n",
      "tensor(0.2206, device='cuda:0', grad_fn=<KlDivBackward>)\n",
      "tensor(0.2553, device='cuda:0', grad_fn=<KlDivBackward>)\n",
      "tensor(0.2121, device='cuda:0', grad_fn=<KlDivBackward>)\n",
      "tensor(0.1720, device='cuda:0', grad_fn=<KlDivBackward>)\n",
      "Train Epoch: 60 [40/67 (56%)]\tLoss: 0.171971\n",
      "tensor(0.2116, device='cuda:0', grad_fn=<KlDivBackward>)\n",
      "tensor(0.1848, device='cuda:0', grad_fn=<KlDivBackward>)\n",
      "tensor(0.1831, device='cuda:0', grad_fn=<KlDivBackward>)\n",
      "tensor([[-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094]], device='cuda:0',\n",
      "       grad_fn=<LogSoftmaxBackward>)\n",
      "tensor([[0.0317, 0.0861, 0.0117, 0.2341, 0.6364],\n",
      "        [0.0388, 0.0388, 0.7784, 0.1053, 0.0388],\n",
      "        [0.0161, 0.0438, 0.0438, 0.0161, 0.8801],\n",
      "        [0.0408, 0.0150, 0.8185, 0.0150, 0.1108],\n",
      "        [0.0157, 0.0426, 0.0426, 0.0426, 0.8564],\n",
      "        [0.0047, 0.0345, 0.6931, 0.0127, 0.2550],\n",
      "        [0.1044, 0.0141, 0.0052, 0.1044, 0.7718],\n",
      "        [0.1441, 0.3917, 0.0530, 0.0195, 0.3917]], device='cuda:0')\n",
      "\n",
      "\n",
      "tensor([[-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094]], device='cuda:0',\n",
      "       grad_fn=<LogSoftmaxBackward>)\n",
      "tensor([[0.6869, 0.0930, 0.0930, 0.0342, 0.0930],\n",
      "        [0.0146, 0.0397, 0.7979, 0.1080, 0.0397],\n",
      "        [0.0117, 0.0861, 0.6364, 0.2341, 0.0317],\n",
      "        [0.1915, 0.0705, 0.5206, 0.0259, 0.1915],\n",
      "        [0.0202, 0.1491, 0.4053, 0.0202, 0.4053],\n",
      "        [0.0161, 0.0438, 0.8801, 0.0438, 0.0161],\n",
      "        [0.1643, 0.0604, 0.4466, 0.1643, 0.1643],\n",
      "        [0.3483, 0.3483, 0.0471, 0.1281, 0.1281]], device='cuda:0')\n",
      "\n",
      "\n",
      "\n",
      "Test set: Accuracy: 2/16 (12%)\n",
      "\n",
      "training epoch 61\n",
      "tensor(0.2053, device='cuda:0', grad_fn=<KlDivBackward>)\n",
      "Train Epoch: 61 [0/67 (0%)]\tLoss: 0.205250\n",
      "tensor(0.2436, device='cuda:0', grad_fn=<KlDivBackward>)\n",
      "tensor(0.2405, device='cuda:0', grad_fn=<KlDivBackward>)\n",
      "tensor(0.2450, device='cuda:0', grad_fn=<KlDivBackward>)\n",
      "tensor(0.1588, device='cuda:0', grad_fn=<KlDivBackward>)\n",
      "tensor(0.2058, device='cuda:0', grad_fn=<KlDivBackward>)\n",
      "Train Epoch: 61 [40/67 (56%)]\tLoss: 0.205816\n",
      "tensor(0.2243, device='cuda:0', grad_fn=<KlDivBackward>)\n",
      "tensor(0.1912, device='cuda:0', grad_fn=<KlDivBackward>)\n",
      "tensor(0.1904, device='cuda:0', grad_fn=<KlDivBackward>)\n",
      "tensor([[-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094]], device='cuda:0',\n",
      "       grad_fn=<LogSoftmaxBackward>)\n",
      "tensor([[0.1915, 0.0705, 0.5206, 0.0259, 0.1915],\n",
      "        [0.0202, 0.1491, 0.4053, 0.0202, 0.4053],\n",
      "        [0.0317, 0.0861, 0.0117, 0.2341, 0.6364],\n",
      "        [0.6869, 0.0930, 0.0930, 0.0342, 0.0930],\n",
      "        [0.0161, 0.0438, 0.8801, 0.0438, 0.0161],\n",
      "        [0.0047, 0.0345, 0.6931, 0.0127, 0.2550],\n",
      "        [0.1441, 0.3917, 0.0530, 0.0195, 0.3917],\n",
      "        [0.0388, 0.0388, 0.7784, 0.1053, 0.0388]], device='cuda:0')\n",
      "\n",
      "\n",
      "tensor([[-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094]], device='cuda:0',\n",
      "       grad_fn=<LogSoftmaxBackward>)\n",
      "tensor([[0.3483, 0.3483, 0.0471, 0.1281, 0.1281],\n",
      "        [0.0146, 0.0397, 0.7979, 0.1080, 0.0397],\n",
      "        [0.1044, 0.0141, 0.0052, 0.1044, 0.7718],\n",
      "        [0.0117, 0.0861, 0.6364, 0.2341, 0.0317],\n",
      "        [0.1643, 0.0604, 0.4466, 0.1643, 0.1643],\n",
      "        [0.0161, 0.0438, 0.0438, 0.0161, 0.8801],\n",
      "        [0.0157, 0.0426, 0.0426, 0.0426, 0.8564],\n",
      "        [0.0408, 0.0150, 0.8185, 0.0150, 0.1108]], device='cuda:0')\n",
      "\n",
      "\n",
      "\n",
      "Test set: Accuracy: 2/16 (12%)\n",
      "\n",
      "training epoch 62\n",
      "tensor(0.2750, device='cuda:0', grad_fn=<KlDivBackward>)\n",
      "Train Epoch: 62 [0/67 (0%)]\tLoss: 0.275024\n",
      "tensor(0.2450, device='cuda:0', grad_fn=<KlDivBackward>)\n",
      "tensor(0.1790, device='cuda:0', grad_fn=<KlDivBackward>)\n",
      "tensor(0.1673, device='cuda:0', grad_fn=<KlDivBackward>)\n",
      "tensor(0.2216, device='cuda:0', grad_fn=<KlDivBackward>)\n",
      "tensor(0.1808, device='cuda:0', grad_fn=<KlDivBackward>)\n",
      "Train Epoch: 62 [40/67 (56%)]\tLoss: 0.180774\n",
      "tensor(0.2648, device='cuda:0', grad_fn=<KlDivBackward>)\n",
      "tensor(0.1820, device='cuda:0', grad_fn=<KlDivBackward>)\n",
      "tensor(0.1875, device='cuda:0', grad_fn=<KlDivBackward>)\n",
      "tensor([[-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094]], device='cuda:0',\n",
      "       grad_fn=<LogSoftmaxBackward>)\n",
      "tensor([[0.0161, 0.0438, 0.0438, 0.0161, 0.8801],\n",
      "        [0.3483, 0.3483, 0.0471, 0.1281, 0.1281],\n",
      "        [0.1044, 0.0141, 0.0052, 0.1044, 0.7718],\n",
      "        [0.1643, 0.0604, 0.4466, 0.1643, 0.1643],\n",
      "        [0.0047, 0.0345, 0.6931, 0.0127, 0.2550],\n",
      "        [0.0317, 0.0861, 0.0117, 0.2341, 0.6364],\n",
      "        [0.0161, 0.0438, 0.8801, 0.0438, 0.0161],\n",
      "        [0.1915, 0.0705, 0.5206, 0.0259, 0.1915]], device='cuda:0')\n",
      "\n",
      "\n",
      "tensor([[-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094]], device='cuda:0',\n",
      "       grad_fn=<LogSoftmaxBackward>)\n",
      "tensor([[0.0408, 0.0150, 0.8185, 0.0150, 0.1108],\n",
      "        [0.0202, 0.1491, 0.4053, 0.0202, 0.4053],\n",
      "        [0.1441, 0.3917, 0.0530, 0.0195, 0.3917],\n",
      "        [0.0117, 0.0861, 0.6364, 0.2341, 0.0317],\n",
      "        [0.0157, 0.0426, 0.0426, 0.0426, 0.8564],\n",
      "        [0.0388, 0.0388, 0.7784, 0.1053, 0.0388],\n",
      "        [0.6869, 0.0930, 0.0930, 0.0342, 0.0930],\n",
      "        [0.0146, 0.0397, 0.7979, 0.1080, 0.0397]], device='cuda:0')\n",
      "\n",
      "\n",
      "\n",
      "Test set: Accuracy: 2/16 (12%)\n",
      "\n",
      "training epoch 63\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.2615, device='cuda:0', grad_fn=<KlDivBackward>)\n",
      "Train Epoch: 63 [0/67 (0%)]\tLoss: 0.261535\n",
      "tensor(0.1607, device='cuda:0', grad_fn=<KlDivBackward>)\n",
      "tensor(0.1811, device='cuda:0', grad_fn=<KlDivBackward>)\n",
      "tensor(0.2157, device='cuda:0', grad_fn=<KlDivBackward>)\n",
      "tensor(0.2151, device='cuda:0', grad_fn=<KlDivBackward>)\n",
      "tensor(0.1748, device='cuda:0', grad_fn=<KlDivBackward>)\n",
      "Train Epoch: 63 [40/67 (56%)]\tLoss: 0.174794\n",
      "tensor(0.2509, device='cuda:0', grad_fn=<KlDivBackward>)\n",
      "tensor(0.2647, device='cuda:0', grad_fn=<KlDivBackward>)\n",
      "tensor(0.1636, device='cuda:0', grad_fn=<KlDivBackward>)\n",
      "tensor([[-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094]], device='cuda:0',\n",
      "       grad_fn=<LogSoftmaxBackward>)\n",
      "tensor([[0.0117, 0.0861, 0.6364, 0.2341, 0.0317],\n",
      "        [0.0146, 0.0397, 0.7979, 0.1080, 0.0397],\n",
      "        [0.0157, 0.0426, 0.0426, 0.0426, 0.8564],\n",
      "        [0.1044, 0.0141, 0.0052, 0.1044, 0.7718],\n",
      "        [0.0388, 0.0388, 0.7784, 0.1053, 0.0388],\n",
      "        [0.0408, 0.0150, 0.8185, 0.0150, 0.1108],\n",
      "        [0.1643, 0.0604, 0.4466, 0.1643, 0.1643],\n",
      "        [0.1915, 0.0705, 0.5206, 0.0259, 0.1915]], device='cuda:0')\n",
      "\n",
      "\n",
      "tensor([[-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094]], device='cuda:0',\n",
      "       grad_fn=<LogSoftmaxBackward>)\n",
      "tensor([[0.0202, 0.1491, 0.4053, 0.0202, 0.4053],\n",
      "        [0.6869, 0.0930, 0.0930, 0.0342, 0.0930],\n",
      "        [0.0161, 0.0438, 0.8801, 0.0438, 0.0161],\n",
      "        [0.0047, 0.0345, 0.6931, 0.0127, 0.2550],\n",
      "        [0.1441, 0.3917, 0.0530, 0.0195, 0.3917],\n",
      "        [0.3483, 0.3483, 0.0471, 0.1281, 0.1281],\n",
      "        [0.0317, 0.0861, 0.0117, 0.2341, 0.6364],\n",
      "        [0.0161, 0.0438, 0.0438, 0.0161, 0.8801]], device='cuda:0')\n",
      "\n",
      "\n",
      "\n",
      "Test set: Accuracy: 2/16 (12%)\n",
      "\n",
      "training epoch 64\n",
      "tensor(0.1472, device='cuda:0', grad_fn=<KlDivBackward>)\n",
      "Train Epoch: 64 [0/67 (0%)]\tLoss: 0.147218\n",
      "tensor(0.1947, device='cuda:0', grad_fn=<KlDivBackward>)\n",
      "tensor(0.2250, device='cuda:0', grad_fn=<KlDivBackward>)\n",
      "tensor(0.2599, device='cuda:0', grad_fn=<KlDivBackward>)\n",
      "tensor(0.2174, device='cuda:0', grad_fn=<KlDivBackward>)\n",
      "tensor(0.2085, device='cuda:0', grad_fn=<KlDivBackward>)\n",
      "Train Epoch: 64 [40/67 (56%)]\tLoss: 0.208537\n",
      "tensor(0.1991, device='cuda:0', grad_fn=<KlDivBackward>)\n",
      "tensor(0.2346, device='cuda:0', grad_fn=<KlDivBackward>)\n",
      "tensor(0.2650, device='cuda:0', grad_fn=<KlDivBackward>)\n",
      "tensor([[-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094]], device='cuda:0',\n",
      "       grad_fn=<LogSoftmaxBackward>)\n",
      "tensor([[0.0146, 0.0397, 0.7979, 0.1080, 0.0397],\n",
      "        [0.0161, 0.0438, 0.0438, 0.0161, 0.8801],\n",
      "        [0.1044, 0.0141, 0.0052, 0.1044, 0.7718],\n",
      "        [0.0388, 0.0388, 0.7784, 0.1053, 0.0388],\n",
      "        [0.6869, 0.0930, 0.0930, 0.0342, 0.0930],\n",
      "        [0.0117, 0.0861, 0.6364, 0.2341, 0.0317],\n",
      "        [0.0317, 0.0861, 0.0117, 0.2341, 0.6364],\n",
      "        [0.1441, 0.3917, 0.0530, 0.0195, 0.3917]], device='cuda:0')\n",
      "\n",
      "\n",
      "tensor([[-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094]], device='cuda:0',\n",
      "       grad_fn=<LogSoftmaxBackward>)\n",
      "tensor([[0.1643, 0.0604, 0.4466, 0.1643, 0.1643],\n",
      "        [0.0047, 0.0345, 0.6931, 0.0127, 0.2550],\n",
      "        [0.0408, 0.0150, 0.8185, 0.0150, 0.1108],\n",
      "        [0.0161, 0.0438, 0.8801, 0.0438, 0.0161],\n",
      "        [0.0202, 0.1491, 0.4053, 0.0202, 0.4053],\n",
      "        [0.1915, 0.0705, 0.5206, 0.0259, 0.1915],\n",
      "        [0.3483, 0.3483, 0.0471, 0.1281, 0.1281],\n",
      "        [0.0157, 0.0426, 0.0426, 0.0426, 0.8564]], device='cuda:0')\n",
      "\n",
      "\n",
      "\n",
      "Test set: Accuracy: 2/16 (12%)\n",
      "\n",
      "training epoch 65\n",
      "tensor(0.1877, device='cuda:0', grad_fn=<KlDivBackward>)\n",
      "Train Epoch: 65 [0/67 (0%)]\tLoss: 0.187722\n",
      "tensor(0.2059, device='cuda:0', grad_fn=<KlDivBackward>)\n",
      "tensor(0.2328, device='cuda:0', grad_fn=<KlDivBackward>)\n",
      "tensor(0.2289, device='cuda:0', grad_fn=<KlDivBackward>)\n",
      "tensor(0.1794, device='cuda:0', grad_fn=<KlDivBackward>)\n",
      "tensor(0.1888, device='cuda:0', grad_fn=<KlDivBackward>)\n",
      "Train Epoch: 65 [40/67 (56%)]\tLoss: 0.188819\n",
      "tensor(0.2400, device='cuda:0', grad_fn=<KlDivBackward>)\n",
      "tensor(0.2272, device='cuda:0', grad_fn=<KlDivBackward>)\n",
      "tensor(0.2540, device='cuda:0', grad_fn=<KlDivBackward>)\n",
      "tensor([[-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094]], device='cuda:0',\n",
      "       grad_fn=<LogSoftmaxBackward>)\n",
      "tensor([[0.1044, 0.0141, 0.0052, 0.1044, 0.7718],\n",
      "        [0.0317, 0.0861, 0.0117, 0.2341, 0.6364],\n",
      "        [0.1915, 0.0705, 0.5206, 0.0259, 0.1915],\n",
      "        [0.0146, 0.0397, 0.7979, 0.1080, 0.0397],\n",
      "        [0.1441, 0.3917, 0.0530, 0.0195, 0.3917],\n",
      "        [0.0408, 0.0150, 0.8185, 0.0150, 0.1108],\n",
      "        [0.3483, 0.3483, 0.0471, 0.1281, 0.1281],\n",
      "        [0.0161, 0.0438, 0.0438, 0.0161, 0.8801]], device='cuda:0')\n",
      "\n",
      "\n",
      "tensor([[-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094]], device='cuda:0',\n",
      "       grad_fn=<LogSoftmaxBackward>)\n",
      "tensor([[0.0388, 0.0388, 0.7784, 0.1053, 0.0388],\n",
      "        [0.0047, 0.0345, 0.6931, 0.0127, 0.2550],\n",
      "        [0.1643, 0.0604, 0.4466, 0.1643, 0.1643],\n",
      "        [0.0202, 0.1491, 0.4053, 0.0202, 0.4053],\n",
      "        [0.0161, 0.0438, 0.8801, 0.0438, 0.0161],\n",
      "        [0.6869, 0.0930, 0.0930, 0.0342, 0.0930],\n",
      "        [0.0117, 0.0861, 0.6364, 0.2341, 0.0317],\n",
      "        [0.0157, 0.0426, 0.0426, 0.0426, 0.8564]], device='cuda:0')\n",
      "\n",
      "\n",
      "\n",
      "Test set: Accuracy: 2/16 (12%)\n",
      "\n",
      "training epoch 66\n",
      "tensor(0.2233, device='cuda:0', grad_fn=<KlDivBackward>)\n",
      "Train Epoch: 66 [0/67 (0%)]\tLoss: 0.223324\n",
      "tensor(0.2298, device='cuda:0', grad_fn=<KlDivBackward>)\n",
      "tensor(0.2172, device='cuda:0', grad_fn=<KlDivBackward>)\n",
      "tensor(0.2059, device='cuda:0', grad_fn=<KlDivBackward>)\n",
      "tensor(0.2138, device='cuda:0', grad_fn=<KlDivBackward>)\n",
      "tensor(0.1874, device='cuda:0', grad_fn=<KlDivBackward>)\n",
      "Train Epoch: 66 [40/67 (56%)]\tLoss: 0.187361\n",
      "tensor(0.2062, device='cuda:0', grad_fn=<KlDivBackward>)\n",
      "tensor(0.2054, device='cuda:0', grad_fn=<KlDivBackward>)\n",
      "tensor(0.2584, device='cuda:0', grad_fn=<KlDivBackward>)\n",
      "tensor([[-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094]], device='cuda:0',\n",
      "       grad_fn=<LogSoftmaxBackward>)\n",
      "tensor([[0.1643, 0.0604, 0.4466, 0.1643, 0.1643],\n",
      "        [0.0117, 0.0861, 0.6364, 0.2341, 0.0317],\n",
      "        [0.0388, 0.0388, 0.7784, 0.1053, 0.0388],\n",
      "        [0.3483, 0.3483, 0.0471, 0.1281, 0.1281],\n",
      "        [0.0047, 0.0345, 0.6931, 0.0127, 0.2550],\n",
      "        [0.0317, 0.0861, 0.0117, 0.2341, 0.6364],\n",
      "        [0.0161, 0.0438, 0.0438, 0.0161, 0.8801],\n",
      "        [0.0161, 0.0438, 0.8801, 0.0438, 0.0161]], device='cuda:0')\n",
      "\n",
      "\n",
      "tensor([[-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094]], device='cuda:0',\n",
      "       grad_fn=<LogSoftmaxBackward>)\n",
      "tensor([[0.1915, 0.0705, 0.5206, 0.0259, 0.1915],\n",
      "        [0.0146, 0.0397, 0.7979, 0.1080, 0.0397],\n",
      "        [0.1441, 0.3917, 0.0530, 0.0195, 0.3917],\n",
      "        [0.0202, 0.1491, 0.4053, 0.0202, 0.4053],\n",
      "        [0.0408, 0.0150, 0.8185, 0.0150, 0.1108],\n",
      "        [0.0157, 0.0426, 0.0426, 0.0426, 0.8564],\n",
      "        [0.1044, 0.0141, 0.0052, 0.1044, 0.7718],\n",
      "        [0.6869, 0.0930, 0.0930, 0.0342, 0.0930]], device='cuda:0')\n",
      "\n",
      "\n",
      "\n",
      "Test set: Accuracy: 2/16 (12%)\n",
      "\n",
      "training epoch 67\n",
      "tensor(0.2470, device='cuda:0', grad_fn=<KlDivBackward>)\n",
      "Train Epoch: 67 [0/67 (0%)]\tLoss: 0.246969\n",
      "tensor(0.2328, device='cuda:0', grad_fn=<KlDivBackward>)\n",
      "tensor(0.2202, device='cuda:0', grad_fn=<KlDivBackward>)\n",
      "tensor(0.1978, device='cuda:0', grad_fn=<KlDivBackward>)\n",
      "tensor(0.2355, device='cuda:0', grad_fn=<KlDivBackward>)\n",
      "tensor(0.1670, device='cuda:0', grad_fn=<KlDivBackward>)\n",
      "Train Epoch: 67 [40/67 (56%)]\tLoss: 0.166962\n",
      "tensor(0.2239, device='cuda:0', grad_fn=<KlDivBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.2019, device='cuda:0', grad_fn=<KlDivBackward>)\n",
      "tensor(0.1595, device='cuda:0', grad_fn=<KlDivBackward>)\n",
      "tensor([[-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094]], device='cuda:0',\n",
      "       grad_fn=<LogSoftmaxBackward>)\n",
      "tensor([[0.0146, 0.0397, 0.7979, 0.1080, 0.0397],\n",
      "        [0.0161, 0.0438, 0.0438, 0.0161, 0.8801],\n",
      "        [0.6869, 0.0930, 0.0930, 0.0342, 0.0930],\n",
      "        [0.3483, 0.3483, 0.0471, 0.1281, 0.1281],\n",
      "        [0.0408, 0.0150, 0.8185, 0.0150, 0.1108],\n",
      "        [0.1441, 0.3917, 0.0530, 0.0195, 0.3917],\n",
      "        [0.0161, 0.0438, 0.8801, 0.0438, 0.0161],\n",
      "        [0.1915, 0.0705, 0.5206, 0.0259, 0.1915]], device='cuda:0')\n",
      "\n",
      "\n",
      "tensor([[-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094]], device='cuda:0',\n",
      "       grad_fn=<LogSoftmaxBackward>)\n",
      "tensor([[0.0047, 0.0345, 0.6931, 0.0127, 0.2550],\n",
      "        [0.0388, 0.0388, 0.7784, 0.1053, 0.0388],\n",
      "        [0.0157, 0.0426, 0.0426, 0.0426, 0.8564],\n",
      "        [0.1044, 0.0141, 0.0052, 0.1044, 0.7718],\n",
      "        [0.0202, 0.1491, 0.4053, 0.0202, 0.4053],\n",
      "        [0.0317, 0.0861, 0.0117, 0.2341, 0.6364],\n",
      "        [0.1643, 0.0604, 0.4466, 0.1643, 0.1643],\n",
      "        [0.0117, 0.0861, 0.6364, 0.2341, 0.0317]], device='cuda:0')\n",
      "\n",
      "\n",
      "\n",
      "Test set: Accuracy: 2/16 (12%)\n",
      "\n",
      "training epoch 68\n",
      "tensor(0.1894, device='cuda:0', grad_fn=<KlDivBackward>)\n",
      "Train Epoch: 68 [0/67 (0%)]\tLoss: 0.189434\n",
      "tensor(0.1954, device='cuda:0', grad_fn=<KlDivBackward>)\n",
      "tensor(0.2390, device='cuda:0', grad_fn=<KlDivBackward>)\n",
      "tensor(0.2422, device='cuda:0', grad_fn=<KlDivBackward>)\n",
      "tensor(0.1465, device='cuda:0', grad_fn=<KlDivBackward>)\n",
      "tensor(0.2658, device='cuda:0', grad_fn=<KlDivBackward>)\n",
      "Train Epoch: 68 [40/67 (56%)]\tLoss: 0.265831\n",
      "tensor(0.1947, device='cuda:0', grad_fn=<KlDivBackward>)\n",
      "tensor(0.2208, device='cuda:0', grad_fn=<KlDivBackward>)\n",
      "tensor(0.2451, device='cuda:0', grad_fn=<KlDivBackward>)\n",
      "tensor([[-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094]], device='cuda:0',\n",
      "       grad_fn=<LogSoftmaxBackward>)\n",
      "tensor([[0.0161, 0.0438, 0.0438, 0.0161, 0.8801],\n",
      "        [0.1643, 0.0604, 0.4466, 0.1643, 0.1643],\n",
      "        [0.0202, 0.1491, 0.4053, 0.0202, 0.4053],\n",
      "        [0.0388, 0.0388, 0.7784, 0.1053, 0.0388],\n",
      "        [0.6869, 0.0930, 0.0930, 0.0342, 0.0930],\n",
      "        [0.0161, 0.0438, 0.8801, 0.0438, 0.0161],\n",
      "        [0.0117, 0.0861, 0.6364, 0.2341, 0.0317],\n",
      "        [0.1915, 0.0705, 0.5206, 0.0259, 0.1915]], device='cuda:0')\n",
      "\n",
      "\n",
      "tensor([[-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094]], device='cuda:0',\n",
      "       grad_fn=<LogSoftmaxBackward>)\n",
      "tensor([[0.0317, 0.0861, 0.0117, 0.2341, 0.6364],\n",
      "        [0.0146, 0.0397, 0.7979, 0.1080, 0.0397],\n",
      "        [0.1441, 0.3917, 0.0530, 0.0195, 0.3917],\n",
      "        [0.0408, 0.0150, 0.8185, 0.0150, 0.1108],\n",
      "        [0.1044, 0.0141, 0.0052, 0.1044, 0.7718],\n",
      "        [0.3483, 0.3483, 0.0471, 0.1281, 0.1281],\n",
      "        [0.0157, 0.0426, 0.0426, 0.0426, 0.8564],\n",
      "        [0.0047, 0.0345, 0.6931, 0.0127, 0.2550]], device='cuda:0')\n",
      "\n",
      "\n",
      "\n",
      "Test set: Accuracy: 2/16 (12%)\n",
      "\n",
      "training epoch 69\n",
      "tensor(0.2359, device='cuda:0', grad_fn=<KlDivBackward>)\n",
      "Train Epoch: 69 [0/67 (0%)]\tLoss: 0.235902\n",
      "tensor(0.2322, device='cuda:0', grad_fn=<KlDivBackward>)\n",
      "tensor(0.1652, device='cuda:0', grad_fn=<KlDivBackward>)\n",
      "tensor(0.2484, device='cuda:0', grad_fn=<KlDivBackward>)\n",
      "tensor(0.2482, device='cuda:0', grad_fn=<KlDivBackward>)\n",
      "tensor(0.1993, device='cuda:0', grad_fn=<KlDivBackward>)\n",
      "Train Epoch: 69 [40/67 (56%)]\tLoss: 0.199294\n",
      "tensor(0.1797, device='cuda:0', grad_fn=<KlDivBackward>)\n",
      "tensor(0.2192, device='cuda:0', grad_fn=<KlDivBackward>)\n",
      "tensor(0.1543, device='cuda:0', grad_fn=<KlDivBackward>)\n",
      "tensor([[-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094]], device='cuda:0',\n",
      "       grad_fn=<LogSoftmaxBackward>)\n",
      "tensor([[0.1915, 0.0705, 0.5206, 0.0259, 0.1915],\n",
      "        [0.0117, 0.0861, 0.6364, 0.2341, 0.0317],\n",
      "        [0.0388, 0.0388, 0.7784, 0.1053, 0.0388],\n",
      "        [0.0161, 0.0438, 0.0438, 0.0161, 0.8801],\n",
      "        [0.1643, 0.0604, 0.4466, 0.1643, 0.1643],\n",
      "        [0.0317, 0.0861, 0.0117, 0.2341, 0.6364],\n",
      "        [0.6869, 0.0930, 0.0930, 0.0342, 0.0930],\n",
      "        [0.0146, 0.0397, 0.7979, 0.1080, 0.0397]], device='cuda:0')\n",
      "\n",
      "\n",
      "tensor([[-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094]], device='cuda:0',\n",
      "       grad_fn=<LogSoftmaxBackward>)\n",
      "tensor([[0.0047, 0.0345, 0.6931, 0.0127, 0.2550],\n",
      "        [0.0408, 0.0150, 0.8185, 0.0150, 0.1108],\n",
      "        [0.1441, 0.3917, 0.0530, 0.0195, 0.3917],\n",
      "        [0.0157, 0.0426, 0.0426, 0.0426, 0.8564],\n",
      "        [0.0161, 0.0438, 0.8801, 0.0438, 0.0161],\n",
      "        [0.1044, 0.0141, 0.0052, 0.1044, 0.7718],\n",
      "        [0.3483, 0.3483, 0.0471, 0.1281, 0.1281],\n",
      "        [0.0202, 0.1491, 0.4053, 0.0202, 0.4053]], device='cuda:0')\n",
      "\n",
      "\n",
      "\n",
      "Test set: Accuracy: 2/16 (12%)\n",
      "\n",
      "training epoch 70\n",
      "tensor(0.2430, device='cuda:0', grad_fn=<KlDivBackward>)\n",
      "Train Epoch: 70 [0/67 (0%)]\tLoss: 0.243000\n",
      "tensor(0.1759, device='cuda:0', grad_fn=<KlDivBackward>)\n",
      "tensor(0.1620, device='cuda:0', grad_fn=<KlDivBackward>)\n",
      "tensor(0.2227, device='cuda:0', grad_fn=<KlDivBackward>)\n",
      "tensor(0.2457, device='cuda:0', grad_fn=<KlDivBackward>)\n",
      "tensor(0.2047, device='cuda:0', grad_fn=<KlDivBackward>)\n",
      "Train Epoch: 70 [40/67 (56%)]\tLoss: 0.204727\n",
      "tensor(0.1870, device='cuda:0', grad_fn=<KlDivBackward>)\n",
      "tensor(0.2384, device='cuda:0', grad_fn=<KlDivBackward>)\n",
      "tensor(0.2844, device='cuda:0', grad_fn=<KlDivBackward>)\n",
      "tensor([[-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094]], device='cuda:0',\n",
      "       grad_fn=<LogSoftmaxBackward>)\n",
      "tensor([[0.3483, 0.3483, 0.0471, 0.1281, 0.1281],\n",
      "        [0.6869, 0.0930, 0.0930, 0.0342, 0.0930],\n",
      "        [0.0408, 0.0150, 0.8185, 0.0150, 0.1108],\n",
      "        [0.0157, 0.0426, 0.0426, 0.0426, 0.8564],\n",
      "        [0.1915, 0.0705, 0.5206, 0.0259, 0.1915],\n",
      "        [0.1643, 0.0604, 0.4466, 0.1643, 0.1643],\n",
      "        [0.1044, 0.0141, 0.0052, 0.1044, 0.7718],\n",
      "        [0.0117, 0.0861, 0.6364, 0.2341, 0.0317]], device='cuda:0')\n",
      "\n",
      "\n",
      "tensor([[-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094]], device='cuda:0',\n",
      "       grad_fn=<LogSoftmaxBackward>)\n",
      "tensor([[0.0161, 0.0438, 0.0438, 0.0161, 0.8801],\n",
      "        [0.0146, 0.0397, 0.7979, 0.1080, 0.0397],\n",
      "        [0.0161, 0.0438, 0.8801, 0.0438, 0.0161],\n",
      "        [0.0388, 0.0388, 0.7784, 0.1053, 0.0388],\n",
      "        [0.0047, 0.0345, 0.6931, 0.0127, 0.2550],\n",
      "        [0.1441, 0.3917, 0.0530, 0.0195, 0.3917],\n",
      "        [0.0317, 0.0861, 0.0117, 0.2341, 0.6364],\n",
      "        [0.0202, 0.1491, 0.4053, 0.0202, 0.4053]], device='cuda:0')\n",
      "\n",
      "\n",
      "\n",
      "Test set: Accuracy: 2/16 (12%)\n",
      "\n",
      "training epoch 71\n",
      "tensor(0.2215, device='cuda:0', grad_fn=<KlDivBackward>)\n",
      "Train Epoch: 71 [0/67 (0%)]\tLoss: 0.221465\n",
      "tensor(0.2207, device='cuda:0', grad_fn=<KlDivBackward>)\n",
      "tensor(0.2031, device='cuda:0', grad_fn=<KlDivBackward>)\n",
      "tensor(0.2071, device='cuda:0', grad_fn=<KlDivBackward>)\n",
      "tensor(0.2124, device='cuda:0', grad_fn=<KlDivBackward>)\n",
      "tensor(0.2257, device='cuda:0', grad_fn=<KlDivBackward>)\n",
      "Train Epoch: 71 [40/67 (56%)]\tLoss: 0.225708\n",
      "tensor(0.1789, device='cuda:0', grad_fn=<KlDivBackward>)\n",
      "tensor(0.2268, device='cuda:0', grad_fn=<KlDivBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.2389, device='cuda:0', grad_fn=<KlDivBackward>)\n",
      "tensor([[-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094]], device='cuda:0',\n",
      "       grad_fn=<LogSoftmaxBackward>)\n",
      "tensor([[0.1915, 0.0705, 0.5206, 0.0259, 0.1915],\n",
      "        [0.0146, 0.0397, 0.7979, 0.1080, 0.0397],\n",
      "        [0.1643, 0.0604, 0.4466, 0.1643, 0.1643],\n",
      "        [0.0047, 0.0345, 0.6931, 0.0127, 0.2550],\n",
      "        [0.0388, 0.0388, 0.7784, 0.1053, 0.0388],\n",
      "        [0.6869, 0.0930, 0.0930, 0.0342, 0.0930],\n",
      "        [0.0202, 0.1491, 0.4053, 0.0202, 0.4053],\n",
      "        [0.0317, 0.0861, 0.0117, 0.2341, 0.6364]], device='cuda:0')\n",
      "\n",
      "\n",
      "tensor([[-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094]], device='cuda:0',\n",
      "       grad_fn=<LogSoftmaxBackward>)\n",
      "tensor([[0.0117, 0.0861, 0.6364, 0.2341, 0.0317],\n",
      "        [0.0408, 0.0150, 0.8185, 0.0150, 0.1108],\n",
      "        [0.0161, 0.0438, 0.0438, 0.0161, 0.8801],\n",
      "        [0.3483, 0.3483, 0.0471, 0.1281, 0.1281],\n",
      "        [0.1044, 0.0141, 0.0052, 0.1044, 0.7718],\n",
      "        [0.1441, 0.3917, 0.0530, 0.0195, 0.3917],\n",
      "        [0.0161, 0.0438, 0.8801, 0.0438, 0.0161],\n",
      "        [0.0157, 0.0426, 0.0426, 0.0426, 0.8564]], device='cuda:0')\n",
      "\n",
      "\n",
      "\n",
      "Test set: Accuracy: 2/16 (12%)\n",
      "\n",
      "training epoch 72\n",
      "tensor(0.2176, device='cuda:0', grad_fn=<KlDivBackward>)\n",
      "Train Epoch: 72 [0/67 (0%)]\tLoss: 0.217573\n",
      "tensor(0.1921, device='cuda:0', grad_fn=<KlDivBackward>)\n",
      "tensor(0.1712, device='cuda:0', grad_fn=<KlDivBackward>)\n",
      "tensor(0.2230, device='cuda:0', grad_fn=<KlDivBackward>)\n",
      "tensor(0.2057, device='cuda:0', grad_fn=<KlDivBackward>)\n",
      "tensor(0.2378, device='cuda:0', grad_fn=<KlDivBackward>)\n",
      "Train Epoch: 72 [40/67 (56%)]\tLoss: 0.237825\n",
      "tensor(0.2294, device='cuda:0', grad_fn=<KlDivBackward>)\n",
      "tensor(0.1939, device='cuda:0', grad_fn=<KlDivBackward>)\n",
      "tensor(0.3072, device='cuda:0', grad_fn=<KlDivBackward>)\n",
      "tensor([[-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094]], device='cuda:0',\n",
      "       grad_fn=<LogSoftmaxBackward>)\n",
      "tensor([[0.0161, 0.0438, 0.8801, 0.0438, 0.0161],\n",
      "        [0.1643, 0.0604, 0.4466, 0.1643, 0.1643],\n",
      "        [0.1441, 0.3917, 0.0530, 0.0195, 0.3917],\n",
      "        [0.0317, 0.0861, 0.0117, 0.2341, 0.6364],\n",
      "        [0.0161, 0.0438, 0.0438, 0.0161, 0.8801],\n",
      "        [0.0388, 0.0388, 0.7784, 0.1053, 0.0388],\n",
      "        [0.6869, 0.0930, 0.0930, 0.0342, 0.0930],\n",
      "        [0.0202, 0.1491, 0.4053, 0.0202, 0.4053]], device='cuda:0')\n",
      "\n",
      "\n",
      "tensor([[-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094]], device='cuda:0',\n",
      "       grad_fn=<LogSoftmaxBackward>)\n",
      "tensor([[0.3483, 0.3483, 0.0471, 0.1281, 0.1281],\n",
      "        [0.1915, 0.0705, 0.5206, 0.0259, 0.1915],\n",
      "        [0.0157, 0.0426, 0.0426, 0.0426, 0.8564],\n",
      "        [0.0408, 0.0150, 0.8185, 0.0150, 0.1108],\n",
      "        [0.0117, 0.0861, 0.6364, 0.2341, 0.0317],\n",
      "        [0.0146, 0.0397, 0.7979, 0.1080, 0.0397],\n",
      "        [0.0047, 0.0345, 0.6931, 0.0127, 0.2550],\n",
      "        [0.1044, 0.0141, 0.0052, 0.1044, 0.7718]], device='cuda:0')\n",
      "\n",
      "\n",
      "\n",
      "Test set: Accuracy: 2/16 (12%)\n",
      "\n",
      "training epoch 73\n",
      "tensor(0.2245, device='cuda:0', grad_fn=<KlDivBackward>)\n",
      "Train Epoch: 73 [0/67 (0%)]\tLoss: 0.224460\n",
      "tensor(0.1933, device='cuda:0', grad_fn=<KlDivBackward>)\n",
      "tensor(0.2399, device='cuda:0', grad_fn=<KlDivBackward>)\n",
      "tensor(0.2060, device='cuda:0', grad_fn=<KlDivBackward>)\n",
      "tensor(0.1802, device='cuda:0', grad_fn=<KlDivBackward>)\n",
      "tensor(0.2149, device='cuda:0', grad_fn=<KlDivBackward>)\n",
      "Train Epoch: 73 [40/67 (56%)]\tLoss: 0.214919\n",
      "tensor(0.2408, device='cuda:0', grad_fn=<KlDivBackward>)\n",
      "tensor(0.2181, device='cuda:0', grad_fn=<KlDivBackward>)\n",
      "tensor(0.1822, device='cuda:0', grad_fn=<KlDivBackward>)\n",
      "tensor([[-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094]], device='cuda:0',\n",
      "       grad_fn=<LogSoftmaxBackward>)\n",
      "tensor([[0.1643, 0.0604, 0.4466, 0.1643, 0.1643],\n",
      "        [0.0157, 0.0426, 0.0426, 0.0426, 0.8564],\n",
      "        [0.0388, 0.0388, 0.7784, 0.1053, 0.0388],\n",
      "        [0.0408, 0.0150, 0.8185, 0.0150, 0.1108],\n",
      "        [0.0117, 0.0861, 0.6364, 0.2341, 0.0317],\n",
      "        [0.3483, 0.3483, 0.0471, 0.1281, 0.1281],\n",
      "        [0.0047, 0.0345, 0.6931, 0.0127, 0.2550],\n",
      "        [0.0146, 0.0397, 0.7979, 0.1080, 0.0397]], device='cuda:0')\n",
      "\n",
      "\n",
      "tensor([[-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094]], device='cuda:0',\n",
      "       grad_fn=<LogSoftmaxBackward>)\n",
      "tensor([[0.0161, 0.0438, 0.8801, 0.0438, 0.0161],\n",
      "        [0.0202, 0.1491, 0.4053, 0.0202, 0.4053],\n",
      "        [0.1044, 0.0141, 0.0052, 0.1044, 0.7718],\n",
      "        [0.1441, 0.3917, 0.0530, 0.0195, 0.3917],\n",
      "        [0.6869, 0.0930, 0.0930, 0.0342, 0.0930],\n",
      "        [0.1915, 0.0705, 0.5206, 0.0259, 0.1915],\n",
      "        [0.0317, 0.0861, 0.0117, 0.2341, 0.6364],\n",
      "        [0.0161, 0.0438, 0.0438, 0.0161, 0.8801]], device='cuda:0')\n",
      "\n",
      "\n",
      "\n",
      "Test set: Accuracy: 2/16 (12%)\n",
      "\n",
      "training epoch 74\n",
      "tensor(0.1506, device='cuda:0', grad_fn=<KlDivBackward>)\n",
      "Train Epoch: 74 [0/67 (0%)]\tLoss: 0.150645\n",
      "tensor(0.2348, device='cuda:0', grad_fn=<KlDivBackward>)\n",
      "tensor(0.2505, device='cuda:0', grad_fn=<KlDivBackward>)\n",
      "tensor(0.2191, device='cuda:0', grad_fn=<KlDivBackward>)\n",
      "tensor(0.2259, device='cuda:0', grad_fn=<KlDivBackward>)\n",
      "tensor(0.2209, device='cuda:0', grad_fn=<KlDivBackward>)\n",
      "Train Epoch: 74 [40/67 (56%)]\tLoss: 0.220915\n",
      "tensor(0.1773, device='cuda:0', grad_fn=<KlDivBackward>)\n",
      "tensor(0.2406, device='cuda:0', grad_fn=<KlDivBackward>)\n",
      "tensor(0.1765, device='cuda:0', grad_fn=<KlDivBackward>)\n",
      "tensor([[-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094]], device='cuda:0',\n",
      "       grad_fn=<LogSoftmaxBackward>)\n",
      "tensor([[0.0117, 0.0861, 0.6364, 0.2341, 0.0317],\n",
      "        [0.0146, 0.0397, 0.7979, 0.1080, 0.0397],\n",
      "        [0.0388, 0.0388, 0.7784, 0.1053, 0.0388],\n",
      "        [0.0317, 0.0861, 0.0117, 0.2341, 0.6364],\n",
      "        [0.0161, 0.0438, 0.8801, 0.0438, 0.0161],\n",
      "        [0.0047, 0.0345, 0.6931, 0.0127, 0.2550],\n",
      "        [0.1441, 0.3917, 0.0530, 0.0195, 0.3917],\n",
      "        [0.1044, 0.0141, 0.0052, 0.1044, 0.7718]], device='cuda:0')\n",
      "\n",
      "\n",
      "tensor([[-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094]], device='cuda:0',\n",
      "       grad_fn=<LogSoftmaxBackward>)\n",
      "tensor([[0.0202, 0.1491, 0.4053, 0.0202, 0.4053],\n",
      "        [0.3483, 0.3483, 0.0471, 0.1281, 0.1281],\n",
      "        [0.1915, 0.0705, 0.5206, 0.0259, 0.1915],\n",
      "        [0.0161, 0.0438, 0.0438, 0.0161, 0.8801],\n",
      "        [0.1643, 0.0604, 0.4466, 0.1643, 0.1643],\n",
      "        [0.0157, 0.0426, 0.0426, 0.0426, 0.8564],\n",
      "        [0.0408, 0.0150, 0.8185, 0.0150, 0.1108],\n",
      "        [0.6869, 0.0930, 0.0930, 0.0342, 0.0930]], device='cuda:0')\n",
      "\n",
      "\n",
      "\n",
      "Test set: Accuracy: 2/16 (12%)\n",
      "\n",
      "training epoch 75\n",
      "tensor(0.2085, device='cuda:0', grad_fn=<KlDivBackward>)\n",
      "Train Epoch: 75 [0/67 (0%)]\tLoss: 0.208504\n",
      "tensor(0.1902, device='cuda:0', grad_fn=<KlDivBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.2414, device='cuda:0', grad_fn=<KlDivBackward>)\n",
      "tensor(0.2044, device='cuda:0', grad_fn=<KlDivBackward>)\n",
      "tensor(0.2466, device='cuda:0', grad_fn=<KlDivBackward>)\n",
      "tensor(0.2030, device='cuda:0', grad_fn=<KlDivBackward>)\n",
      "Train Epoch: 75 [40/67 (56%)]\tLoss: 0.203039\n",
      "tensor(0.2178, device='cuda:0', grad_fn=<KlDivBackward>)\n",
      "tensor(0.2252, device='cuda:0', grad_fn=<KlDivBackward>)\n",
      "tensor(0.1301, device='cuda:0', grad_fn=<KlDivBackward>)\n",
      "tensor([[-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094]], device='cuda:0',\n",
      "       grad_fn=<LogSoftmaxBackward>)\n",
      "tensor([[0.3483, 0.3483, 0.0471, 0.1281, 0.1281],\n",
      "        [0.0202, 0.1491, 0.4053, 0.0202, 0.4053],\n",
      "        [0.0047, 0.0345, 0.6931, 0.0127, 0.2550],\n",
      "        [0.0117, 0.0861, 0.6364, 0.2341, 0.0317],\n",
      "        [0.0388, 0.0388, 0.7784, 0.1053, 0.0388],\n",
      "        [0.0161, 0.0438, 0.0438, 0.0161, 0.8801],\n",
      "        [0.0161, 0.0438, 0.8801, 0.0438, 0.0161],\n",
      "        [0.1915, 0.0705, 0.5206, 0.0259, 0.1915]], device='cuda:0')\n",
      "\n",
      "\n",
      "tensor([[-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094]], device='cuda:0',\n",
      "       grad_fn=<LogSoftmaxBackward>)\n",
      "tensor([[0.6869, 0.0930, 0.0930, 0.0342, 0.0930],\n",
      "        [0.1643, 0.0604, 0.4466, 0.1643, 0.1643],\n",
      "        [0.1044, 0.0141, 0.0052, 0.1044, 0.7718],\n",
      "        [0.0408, 0.0150, 0.8185, 0.0150, 0.1108],\n",
      "        [0.0146, 0.0397, 0.7979, 0.1080, 0.0397],\n",
      "        [0.0317, 0.0861, 0.0117, 0.2341, 0.6364],\n",
      "        [0.0157, 0.0426, 0.0426, 0.0426, 0.8564],\n",
      "        [0.1441, 0.3917, 0.0530, 0.0195, 0.3917]], device='cuda:0')\n",
      "\n",
      "\n",
      "\n",
      "Test set: Accuracy: 2/16 (12%)\n",
      "\n",
      "training epoch 76\n",
      "tensor(0.1780, device='cuda:0', grad_fn=<KlDivBackward>)\n",
      "Train Epoch: 76 [0/67 (0%)]\tLoss: 0.177979\n",
      "tensor(0.2082, device='cuda:0', grad_fn=<KlDivBackward>)\n",
      "tensor(0.2900, device='cuda:0', grad_fn=<KlDivBackward>)\n",
      "tensor(0.2038, device='cuda:0', grad_fn=<KlDivBackward>)\n",
      "tensor(0.2334, device='cuda:0', grad_fn=<KlDivBackward>)\n",
      "tensor(0.2202, device='cuda:0', grad_fn=<KlDivBackward>)\n",
      "Train Epoch: 76 [40/67 (56%)]\tLoss: 0.220225\n",
      "tensor(0.1980, device='cuda:0', grad_fn=<KlDivBackward>)\n",
      "tensor(0.1769, device='cuda:0', grad_fn=<KlDivBackward>)\n",
      "tensor(0.2068, device='cuda:0', grad_fn=<KlDivBackward>)\n",
      "tensor([[-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094]], device='cuda:0',\n",
      "       grad_fn=<LogSoftmaxBackward>)\n",
      "tensor([[0.0408, 0.0150, 0.8185, 0.0150, 0.1108],\n",
      "        [0.0157, 0.0426, 0.0426, 0.0426, 0.8564],\n",
      "        [0.0047, 0.0345, 0.6931, 0.0127, 0.2550],\n",
      "        [0.1441, 0.3917, 0.0530, 0.0195, 0.3917],\n",
      "        [0.0161, 0.0438, 0.0438, 0.0161, 0.8801],\n",
      "        [0.0161, 0.0438, 0.8801, 0.0438, 0.0161],\n",
      "        [0.3483, 0.3483, 0.0471, 0.1281, 0.1281],\n",
      "        [0.0317, 0.0861, 0.0117, 0.2341, 0.6364]], device='cuda:0')\n",
      "\n",
      "\n",
      "tensor([[-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094]], device='cuda:0',\n",
      "       grad_fn=<LogSoftmaxBackward>)\n",
      "tensor([[0.6869, 0.0930, 0.0930, 0.0342, 0.0930],\n",
      "        [0.0202, 0.1491, 0.4053, 0.0202, 0.4053],\n",
      "        [0.0388, 0.0388, 0.7784, 0.1053, 0.0388],\n",
      "        [0.1643, 0.0604, 0.4466, 0.1643, 0.1643],\n",
      "        [0.1915, 0.0705, 0.5206, 0.0259, 0.1915],\n",
      "        [0.1044, 0.0141, 0.0052, 0.1044, 0.7718],\n",
      "        [0.0146, 0.0397, 0.7979, 0.1080, 0.0397],\n",
      "        [0.0117, 0.0861, 0.6364, 0.2341, 0.0317]], device='cuda:0')\n",
      "\n",
      "\n",
      "\n",
      "Test set: Accuracy: 2/16 (12%)\n",
      "\n",
      "training epoch 77\n",
      "tensor(0.2552, device='cuda:0', grad_fn=<KlDivBackward>)\n",
      "Train Epoch: 77 [0/67 (0%)]\tLoss: 0.255168\n",
      "tensor(0.1971, device='cuda:0', grad_fn=<KlDivBackward>)\n",
      "tensor(0.2269, device='cuda:0', grad_fn=<KlDivBackward>)\n",
      "tensor(0.1605, device='cuda:0', grad_fn=<KlDivBackward>)\n",
      "tensor(0.2505, device='cuda:0', grad_fn=<KlDivBackward>)\n",
      "tensor(0.1813, device='cuda:0', grad_fn=<KlDivBackward>)\n",
      "Train Epoch: 77 [40/67 (56%)]\tLoss: 0.181268\n",
      "tensor(0.1916, device='cuda:0', grad_fn=<KlDivBackward>)\n",
      "tensor(0.2238, device='cuda:0', grad_fn=<KlDivBackward>)\n",
      "tensor(0.2641, device='cuda:0', grad_fn=<KlDivBackward>)\n",
      "tensor([[-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094]], device='cuda:0',\n",
      "       grad_fn=<LogSoftmaxBackward>)\n",
      "tensor([[0.0317, 0.0861, 0.0117, 0.2341, 0.6364],\n",
      "        [0.0202, 0.1491, 0.4053, 0.0202, 0.4053],\n",
      "        [0.1643, 0.0604, 0.4466, 0.1643, 0.1643],\n",
      "        [0.0117, 0.0861, 0.6364, 0.2341, 0.0317],\n",
      "        [0.1441, 0.3917, 0.0530, 0.0195, 0.3917],\n",
      "        [0.0157, 0.0426, 0.0426, 0.0426, 0.8564],\n",
      "        [0.1044, 0.0141, 0.0052, 0.1044, 0.7718],\n",
      "        [0.0047, 0.0345, 0.6931, 0.0127, 0.2550]], device='cuda:0')\n",
      "\n",
      "\n",
      "tensor([[-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094]], device='cuda:0',\n",
      "       grad_fn=<LogSoftmaxBackward>)\n",
      "tensor([[0.0161, 0.0438, 0.8801, 0.0438, 0.0161],\n",
      "        [0.1915, 0.0705, 0.5206, 0.0259, 0.1915],\n",
      "        [0.0408, 0.0150, 0.8185, 0.0150, 0.1108],\n",
      "        [0.0146, 0.0397, 0.7979, 0.1080, 0.0397],\n",
      "        [0.0161, 0.0438, 0.0438, 0.0161, 0.8801],\n",
      "        [0.6869, 0.0930, 0.0930, 0.0342, 0.0930],\n",
      "        [0.0388, 0.0388, 0.7784, 0.1053, 0.0388],\n",
      "        [0.3483, 0.3483, 0.0471, 0.1281, 0.1281]], device='cuda:0')\n",
      "\n",
      "\n",
      "\n",
      "Test set: Accuracy: 2/16 (12%)\n",
      "\n",
      "training epoch 78\n",
      "tensor(0.2438, device='cuda:0', grad_fn=<KlDivBackward>)\n",
      "Train Epoch: 78 [0/67 (0%)]\tLoss: 0.243849\n",
      "tensor(0.1796, device='cuda:0', grad_fn=<KlDivBackward>)\n",
      "tensor(0.1965, device='cuda:0', grad_fn=<KlDivBackward>)\n",
      "tensor(0.1749, device='cuda:0', grad_fn=<KlDivBackward>)\n",
      "tensor(0.2480, device='cuda:0', grad_fn=<KlDivBackward>)\n",
      "tensor(0.1853, device='cuda:0', grad_fn=<KlDivBackward>)\n",
      "Train Epoch: 78 [40/67 (56%)]\tLoss: 0.185265\n",
      "tensor(0.2507, device='cuda:0', grad_fn=<KlDivBackward>)\n",
      "tensor(0.2108, device='cuda:0', grad_fn=<KlDivBackward>)\n",
      "tensor(0.2567, device='cuda:0', grad_fn=<KlDivBackward>)\n",
      "tensor([[-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094]], device='cuda:0',\n",
      "       grad_fn=<LogSoftmaxBackward>)\n",
      "tensor([[0.1044, 0.0141, 0.0052, 0.1044, 0.7718],\n",
      "        [0.0146, 0.0397, 0.7979, 0.1080, 0.0397],\n",
      "        [0.0161, 0.0438, 0.0438, 0.0161, 0.8801],\n",
      "        [0.0388, 0.0388, 0.7784, 0.1053, 0.0388],\n",
      "        [0.0161, 0.0438, 0.8801, 0.0438, 0.0161],\n",
      "        [0.0117, 0.0861, 0.6364, 0.2341, 0.0317],\n",
      "        [0.0047, 0.0345, 0.6931, 0.0127, 0.2550],\n",
      "        [0.0408, 0.0150, 0.8185, 0.0150, 0.1108]], device='cuda:0')\n",
      "\n",
      "\n",
      "tensor([[-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094]], device='cuda:0',\n",
      "       grad_fn=<LogSoftmaxBackward>)\n",
      "tensor([[0.0202, 0.1491, 0.4053, 0.0202, 0.4053],\n",
      "        [0.0157, 0.0426, 0.0426, 0.0426, 0.8564],\n",
      "        [0.1441, 0.3917, 0.0530, 0.0195, 0.3917],\n",
      "        [0.1643, 0.0604, 0.4466, 0.1643, 0.1643],\n",
      "        [0.0317, 0.0861, 0.0117, 0.2341, 0.6364],\n",
      "        [0.1915, 0.0705, 0.5206, 0.0259, 0.1915],\n",
      "        [0.6869, 0.0930, 0.0930, 0.0342, 0.0930],\n",
      "        [0.3483, 0.3483, 0.0471, 0.1281, 0.1281]], device='cuda:0')\n",
      "\n",
      "\n",
      "\n",
      "Test set: Accuracy: 2/16 (12%)\n",
      "\n",
      "training epoch 79\n",
      "tensor(0.2316, device='cuda:0', grad_fn=<KlDivBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 79 [0/67 (0%)]\tLoss: 0.231582\n",
      "tensor(0.1719, device='cuda:0', grad_fn=<KlDivBackward>)\n",
      "tensor(0.1983, device='cuda:0', grad_fn=<KlDivBackward>)\n",
      "tensor(0.2525, device='cuda:0', grad_fn=<KlDivBackward>)\n",
      "tensor(0.1880, device='cuda:0', grad_fn=<KlDivBackward>)\n",
      "tensor(0.2838, device='cuda:0', grad_fn=<KlDivBackward>)\n",
      "Train Epoch: 79 [40/67 (56%)]\tLoss: 0.283792\n",
      "tensor(0.1477, device='cuda:0', grad_fn=<KlDivBackward>)\n",
      "tensor(0.2155, device='cuda:0', grad_fn=<KlDivBackward>)\n",
      "tensor(0.2578, device='cuda:0', grad_fn=<KlDivBackward>)\n",
      "tensor([[-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094]], device='cuda:0',\n",
      "       grad_fn=<LogSoftmaxBackward>)\n",
      "tensor([[0.3483, 0.3483, 0.0471, 0.1281, 0.1281],\n",
      "        [0.0146, 0.0397, 0.7979, 0.1080, 0.0397],\n",
      "        [0.1441, 0.3917, 0.0530, 0.0195, 0.3917],\n",
      "        [0.1044, 0.0141, 0.0052, 0.1044, 0.7718],\n",
      "        [0.1915, 0.0705, 0.5206, 0.0259, 0.1915],\n",
      "        [0.0117, 0.0861, 0.6364, 0.2341, 0.0317],\n",
      "        [0.0388, 0.0388, 0.7784, 0.1053, 0.0388],\n",
      "        [0.1643, 0.0604, 0.4466, 0.1643, 0.1643]], device='cuda:0')\n",
      "\n",
      "\n",
      "tensor([[-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094]], device='cuda:0',\n",
      "       grad_fn=<LogSoftmaxBackward>)\n",
      "tensor([[0.0157, 0.0426, 0.0426, 0.0426, 0.8564],\n",
      "        [0.0408, 0.0150, 0.8185, 0.0150, 0.1108],\n",
      "        [0.0047, 0.0345, 0.6931, 0.0127, 0.2550],\n",
      "        [0.0161, 0.0438, 0.8801, 0.0438, 0.0161],\n",
      "        [0.0202, 0.1491, 0.4053, 0.0202, 0.4053],\n",
      "        [0.0161, 0.0438, 0.0438, 0.0161, 0.8801],\n",
      "        [0.6869, 0.0930, 0.0930, 0.0342, 0.0930],\n",
      "        [0.0317, 0.0861, 0.0117, 0.2341, 0.6364]], device='cuda:0')\n",
      "\n",
      "\n",
      "\n",
      "Test set: Accuracy: 2/16 (12%)\n",
      "\n",
      "training epoch 80\n",
      "tensor(0.2557, device='cuda:0', grad_fn=<KlDivBackward>)\n",
      "Train Epoch: 80 [0/67 (0%)]\tLoss: 0.255663\n",
      "tensor(0.1989, device='cuda:0', grad_fn=<KlDivBackward>)\n",
      "tensor(0.2114, device='cuda:0', grad_fn=<KlDivBackward>)\n",
      "tensor(0.2072, device='cuda:0', grad_fn=<KlDivBackward>)\n",
      "tensor(0.1538, device='cuda:0', grad_fn=<KlDivBackward>)\n",
      "tensor(0.1965, device='cuda:0', grad_fn=<KlDivBackward>)\n",
      "Train Epoch: 80 [40/67 (56%)]\tLoss: 0.196522\n",
      "tensor(0.2540, device='cuda:0', grad_fn=<KlDivBackward>)\n",
      "tensor(0.2190, device='cuda:0', grad_fn=<KlDivBackward>)\n",
      "tensor(0.2388, device='cuda:0', grad_fn=<KlDivBackward>)\n",
      "tensor([[-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094]], device='cuda:0',\n",
      "       grad_fn=<LogSoftmaxBackward>)\n",
      "tensor([[0.1044, 0.0141, 0.0052, 0.1044, 0.7718],\n",
      "        [0.1643, 0.0604, 0.4466, 0.1643, 0.1643],\n",
      "        [0.0388, 0.0388, 0.7784, 0.1053, 0.0388],\n",
      "        [0.0161, 0.0438, 0.0438, 0.0161, 0.8801],\n",
      "        [0.0146, 0.0397, 0.7979, 0.1080, 0.0397],\n",
      "        [0.1441, 0.3917, 0.0530, 0.0195, 0.3917],\n",
      "        [0.0408, 0.0150, 0.8185, 0.0150, 0.1108],\n",
      "        [0.0047, 0.0345, 0.6931, 0.0127, 0.2550]], device='cuda:0')\n",
      "\n",
      "\n",
      "tensor([[-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094]], device='cuda:0',\n",
      "       grad_fn=<LogSoftmaxBackward>)\n",
      "tensor([[0.3483, 0.3483, 0.0471, 0.1281, 0.1281],\n",
      "        [0.0202, 0.1491, 0.4053, 0.0202, 0.4053],\n",
      "        [0.0317, 0.0861, 0.0117, 0.2341, 0.6364],\n",
      "        [0.6869, 0.0930, 0.0930, 0.0342, 0.0930],\n",
      "        [0.0161, 0.0438, 0.8801, 0.0438, 0.0161],\n",
      "        [0.0117, 0.0861, 0.6364, 0.2341, 0.0317],\n",
      "        [0.0157, 0.0426, 0.0426, 0.0426, 0.8564],\n",
      "        [0.1915, 0.0705, 0.5206, 0.0259, 0.1915]], device='cuda:0')\n",
      "\n",
      "\n",
      "\n",
      "Test set: Accuracy: 2/16 (12%)\n",
      "\n",
      "training epoch 81\n",
      "tensor(0.2553, device='cuda:0', grad_fn=<KlDivBackward>)\n",
      "Train Epoch: 81 [0/67 (0%)]\tLoss: 0.255336\n",
      "tensor(0.2018, device='cuda:0', grad_fn=<KlDivBackward>)\n",
      "tensor(0.2388, device='cuda:0', grad_fn=<KlDivBackward>)\n",
      "tensor(0.2049, device='cuda:0', grad_fn=<KlDivBackward>)\n",
      "tensor(0.1735, device='cuda:0', grad_fn=<KlDivBackward>)\n",
      "tensor(0.2289, device='cuda:0', grad_fn=<KlDivBackward>)\n",
      "Train Epoch: 81 [40/67 (56%)]\tLoss: 0.228899\n",
      "tensor(0.2092, device='cuda:0', grad_fn=<KlDivBackward>)\n",
      "tensor(0.1615, device='cuda:0', grad_fn=<KlDivBackward>)\n",
      "tensor(0.2986, device='cuda:0', grad_fn=<KlDivBackward>)\n",
      "tensor([[-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094]], device='cuda:0',\n",
      "       grad_fn=<LogSoftmaxBackward>)\n",
      "tensor([[0.0161, 0.0438, 0.0438, 0.0161, 0.8801],\n",
      "        [0.0202, 0.1491, 0.4053, 0.0202, 0.4053],\n",
      "        [0.0388, 0.0388, 0.7784, 0.1053, 0.0388],\n",
      "        [0.1441, 0.3917, 0.0530, 0.0195, 0.3917],\n",
      "        [0.0161, 0.0438, 0.8801, 0.0438, 0.0161],\n",
      "        [0.0408, 0.0150, 0.8185, 0.0150, 0.1108],\n",
      "        [0.1044, 0.0141, 0.0052, 0.1044, 0.7718],\n",
      "        [0.6869, 0.0930, 0.0930, 0.0342, 0.0930]], device='cuda:0')\n",
      "\n",
      "\n",
      "tensor([[-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094]], device='cuda:0',\n",
      "       grad_fn=<LogSoftmaxBackward>)\n",
      "tensor([[0.0047, 0.0345, 0.6931, 0.0127, 0.2550],\n",
      "        [0.0317, 0.0861, 0.0117, 0.2341, 0.6364],\n",
      "        [0.1915, 0.0705, 0.5206, 0.0259, 0.1915],\n",
      "        [0.0157, 0.0426, 0.0426, 0.0426, 0.8564],\n",
      "        [0.3483, 0.3483, 0.0471, 0.1281, 0.1281],\n",
      "        [0.0146, 0.0397, 0.7979, 0.1080, 0.0397],\n",
      "        [0.0117, 0.0861, 0.6364, 0.2341, 0.0317],\n",
      "        [0.1643, 0.0604, 0.4466, 0.1643, 0.1643]], device='cuda:0')\n",
      "\n",
      "\n",
      "\n",
      "Test set: Accuracy: 2/16 (12%)\n",
      "\n",
      "training epoch 82\n",
      "tensor(0.2460, device='cuda:0', grad_fn=<KlDivBackward>)\n",
      "Train Epoch: 82 [0/67 (0%)]\tLoss: 0.246015\n",
      "tensor(0.2121, device='cuda:0', grad_fn=<KlDivBackward>)\n",
      "tensor(0.2220, device='cuda:0', grad_fn=<KlDivBackward>)\n",
      "tensor(0.2370, device='cuda:0', grad_fn=<KlDivBackward>)\n",
      "tensor(0.1697, device='cuda:0', grad_fn=<KlDivBackward>)\n",
      "tensor(0.2021, device='cuda:0', grad_fn=<KlDivBackward>)\n",
      "Train Epoch: 82 [40/67 (56%)]\tLoss: 0.202085\n",
      "tensor(0.2295, device='cuda:0', grad_fn=<KlDivBackward>)\n",
      "tensor(0.1950, device='cuda:0', grad_fn=<KlDivBackward>)\n",
      "tensor(0.1934, device='cuda:0', grad_fn=<KlDivBackward>)\n",
      "tensor([[-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094]], device='cuda:0',\n",
      "       grad_fn=<LogSoftmaxBackward>)\n",
      "tensor([[0.0047, 0.0345, 0.6931, 0.0127, 0.2550],\n",
      "        [0.0117, 0.0861, 0.6364, 0.2341, 0.0317],\n",
      "        [0.3483, 0.3483, 0.0471, 0.1281, 0.1281],\n",
      "        [0.0161, 0.0438, 0.8801, 0.0438, 0.0161],\n",
      "        [0.1044, 0.0141, 0.0052, 0.1044, 0.7718],\n",
      "        [0.0146, 0.0397, 0.7979, 0.1080, 0.0397],\n",
      "        [0.0388, 0.0388, 0.7784, 0.1053, 0.0388],\n",
      "        [0.1643, 0.0604, 0.4466, 0.1643, 0.1643]], device='cuda:0')\n",
      "\n",
      "\n",
      "tensor([[-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094]], device='cuda:0',\n",
      "       grad_fn=<LogSoftmaxBackward>)\n",
      "tensor([[0.0202, 0.1491, 0.4053, 0.0202, 0.4053],\n",
      "        [0.1441, 0.3917, 0.0530, 0.0195, 0.3917],\n",
      "        [0.0408, 0.0150, 0.8185, 0.0150, 0.1108],\n",
      "        [0.0317, 0.0861, 0.0117, 0.2341, 0.6364],\n",
      "        [0.6869, 0.0930, 0.0930, 0.0342, 0.0930],\n",
      "        [0.0161, 0.0438, 0.0438, 0.0161, 0.8801],\n",
      "        [0.0157, 0.0426, 0.0426, 0.0426, 0.8564],\n",
      "        [0.1915, 0.0705, 0.5206, 0.0259, 0.1915]], device='cuda:0')\n",
      "\n",
      "\n",
      "\n",
      "Test set: Accuracy: 2/16 (12%)\n",
      "\n",
      "training epoch 83\n",
      "tensor(0.2141, device='cuda:0', grad_fn=<KlDivBackward>)\n",
      "Train Epoch: 83 [0/67 (0%)]\tLoss: 0.214135\n",
      "tensor(0.1951, device='cuda:0', grad_fn=<KlDivBackward>)\n",
      "tensor(0.2173, device='cuda:0', grad_fn=<KlDivBackward>)\n",
      "tensor(0.1757, device='cuda:0', grad_fn=<KlDivBackward>)\n",
      "tensor(0.2285, device='cuda:0', grad_fn=<KlDivBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.2411, device='cuda:0', grad_fn=<KlDivBackward>)\n",
      "Train Epoch: 83 [40/67 (56%)]\tLoss: 0.241111\n",
      "tensor(0.1865, device='cuda:0', grad_fn=<KlDivBackward>)\n",
      "tensor(0.2259, device='cuda:0', grad_fn=<KlDivBackward>)\n",
      "tensor(0.2714, device='cuda:0', grad_fn=<KlDivBackward>)\n",
      "tensor([[-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094]], device='cuda:0',\n",
      "       grad_fn=<LogSoftmaxBackward>)\n",
      "tensor([[0.1643, 0.0604, 0.4466, 0.1643, 0.1643],\n",
      "        [0.0161, 0.0438, 0.8801, 0.0438, 0.0161],\n",
      "        [0.1044, 0.0141, 0.0052, 0.1044, 0.7718],\n",
      "        [0.0388, 0.0388, 0.7784, 0.1053, 0.0388],\n",
      "        [0.1915, 0.0705, 0.5206, 0.0259, 0.1915],\n",
      "        [0.0408, 0.0150, 0.8185, 0.0150, 0.1108],\n",
      "        [0.1441, 0.3917, 0.0530, 0.0195, 0.3917],\n",
      "        [0.0117, 0.0861, 0.6364, 0.2341, 0.0317]], device='cuda:0')\n",
      "\n",
      "\n",
      "tensor([[-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094]], device='cuda:0',\n",
      "       grad_fn=<LogSoftmaxBackward>)\n",
      "tensor([[0.6869, 0.0930, 0.0930, 0.0342, 0.0930],\n",
      "        [0.0157, 0.0426, 0.0426, 0.0426, 0.8564],\n",
      "        [0.0146, 0.0397, 0.7979, 0.1080, 0.0397],\n",
      "        [0.0317, 0.0861, 0.0117, 0.2341, 0.6364],\n",
      "        [0.0047, 0.0345, 0.6931, 0.0127, 0.2550],\n",
      "        [0.0202, 0.1491, 0.4053, 0.0202, 0.4053],\n",
      "        [0.3483, 0.3483, 0.0471, 0.1281, 0.1281],\n",
      "        [0.0161, 0.0438, 0.0438, 0.0161, 0.8801]], device='cuda:0')\n",
      "\n",
      "\n",
      "\n",
      "Test set: Accuracy: 2/16 (12%)\n",
      "\n",
      "training epoch 84\n",
      "tensor(0.1852, device='cuda:0', grad_fn=<KlDivBackward>)\n",
      "Train Epoch: 84 [0/67 (0%)]\tLoss: 0.185174\n",
      "tensor(0.2242, device='cuda:0', grad_fn=<KlDivBackward>)\n",
      "tensor(0.2583, device='cuda:0', grad_fn=<KlDivBackward>)\n",
      "tensor(0.2063, device='cuda:0', grad_fn=<KlDivBackward>)\n",
      "tensor(0.2386, device='cuda:0', grad_fn=<KlDivBackward>)\n",
      "tensor(0.2418, device='cuda:0', grad_fn=<KlDivBackward>)\n",
      "Train Epoch: 84 [40/67 (56%)]\tLoss: 0.241780\n",
      "tensor(0.1914, device='cuda:0', grad_fn=<KlDivBackward>)\n",
      "tensor(0.1819, device='cuda:0', grad_fn=<KlDivBackward>)\n",
      "tensor(0.1556, device='cuda:0', grad_fn=<KlDivBackward>)\n",
      "tensor([[-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094]], device='cuda:0',\n",
      "       grad_fn=<LogSoftmaxBackward>)\n",
      "tensor([[0.0146, 0.0397, 0.7979, 0.1080, 0.0397],\n",
      "        [0.0117, 0.0861, 0.6364, 0.2341, 0.0317],\n",
      "        [0.0161, 0.0438, 0.0438, 0.0161, 0.8801],\n",
      "        [0.1044, 0.0141, 0.0052, 0.1044, 0.7718],\n",
      "        [0.0157, 0.0426, 0.0426, 0.0426, 0.8564],\n",
      "        [0.6869, 0.0930, 0.0930, 0.0342, 0.0930],\n",
      "        [0.0388, 0.0388, 0.7784, 0.1053, 0.0388],\n",
      "        [0.1441, 0.3917, 0.0530, 0.0195, 0.3917]], device='cuda:0')\n",
      "\n",
      "\n",
      "tensor([[-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094]], device='cuda:0',\n",
      "       grad_fn=<LogSoftmaxBackward>)\n",
      "tensor([[0.0317, 0.0861, 0.0117, 0.2341, 0.6364],\n",
      "        [0.3483, 0.3483, 0.0471, 0.1281, 0.1281],\n",
      "        [0.0047, 0.0345, 0.6931, 0.0127, 0.2550],\n",
      "        [0.1643, 0.0604, 0.4466, 0.1643, 0.1643],\n",
      "        [0.1915, 0.0705, 0.5206, 0.0259, 0.1915],\n",
      "        [0.0161, 0.0438, 0.8801, 0.0438, 0.0161],\n",
      "        [0.0202, 0.1491, 0.4053, 0.0202, 0.4053],\n",
      "        [0.0408, 0.0150, 0.8185, 0.0150, 0.1108]], device='cuda:0')\n",
      "\n",
      "\n",
      "\n",
      "Test set: Accuracy: 2/16 (12%)\n",
      "\n",
      "training epoch 85\n",
      "tensor(0.2314, device='cuda:0', grad_fn=<KlDivBackward>)\n",
      "Train Epoch: 85 [0/67 (0%)]\tLoss: 0.231374\n",
      "tensor(0.2047, device='cuda:0', grad_fn=<KlDivBackward>)\n",
      "tensor(0.2255, device='cuda:0', grad_fn=<KlDivBackward>)\n",
      "tensor(0.2231, device='cuda:0', grad_fn=<KlDivBackward>)\n",
      "tensor(0.2335, device='cuda:0', grad_fn=<KlDivBackward>)\n",
      "tensor(0.1829, device='cuda:0', grad_fn=<KlDivBackward>)\n",
      "Train Epoch: 85 [40/67 (56%)]\tLoss: 0.182936\n",
      "tensor(0.1994, device='cuda:0', grad_fn=<KlDivBackward>)\n",
      "tensor(0.2080, device='cuda:0', grad_fn=<KlDivBackward>)\n",
      "tensor(0.2066, device='cuda:0', grad_fn=<KlDivBackward>)\n",
      "tensor([[-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094]], device='cuda:0',\n",
      "       grad_fn=<LogSoftmaxBackward>)\n",
      "tensor([[0.0202, 0.1491, 0.4053, 0.0202, 0.4053],\n",
      "        [0.0146, 0.0397, 0.7979, 0.1080, 0.0397],\n",
      "        [0.1643, 0.0604, 0.4466, 0.1643, 0.1643],\n",
      "        [0.0388, 0.0388, 0.7784, 0.1053, 0.0388],\n",
      "        [0.0161, 0.0438, 0.0438, 0.0161, 0.8801],\n",
      "        [0.0161, 0.0438, 0.8801, 0.0438, 0.0161],\n",
      "        [0.1441, 0.3917, 0.0530, 0.0195, 0.3917],\n",
      "        [0.6869, 0.0930, 0.0930, 0.0342, 0.0930]], device='cuda:0')\n",
      "\n",
      "\n",
      "tensor([[-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094]], device='cuda:0',\n",
      "       grad_fn=<LogSoftmaxBackward>)\n",
      "tensor([[0.1044, 0.0141, 0.0052, 0.1044, 0.7718],\n",
      "        [0.3483, 0.3483, 0.0471, 0.1281, 0.1281],\n",
      "        [0.0047, 0.0345, 0.6931, 0.0127, 0.2550],\n",
      "        [0.0157, 0.0426, 0.0426, 0.0426, 0.8564],\n",
      "        [0.0317, 0.0861, 0.0117, 0.2341, 0.6364],\n",
      "        [0.1915, 0.0705, 0.5206, 0.0259, 0.1915],\n",
      "        [0.0117, 0.0861, 0.6364, 0.2341, 0.0317],\n",
      "        [0.0408, 0.0150, 0.8185, 0.0150, 0.1108]], device='cuda:0')\n",
      "\n",
      "\n",
      "\n",
      "Test set: Accuracy: 2/16 (12%)\n",
      "\n",
      "training epoch 86\n",
      "tensor(0.1882, device='cuda:0', grad_fn=<KlDivBackward>)\n",
      "Train Epoch: 86 [0/67 (0%)]\tLoss: 0.188201\n",
      "tensor(0.2196, device='cuda:0', grad_fn=<KlDivBackward>)\n",
      "tensor(0.1841, device='cuda:0', grad_fn=<KlDivBackward>)\n",
      "tensor(0.2173, device='cuda:0', grad_fn=<KlDivBackward>)\n",
      "tensor(0.2303, device='cuda:0', grad_fn=<KlDivBackward>)\n",
      "tensor(0.2384, device='cuda:0', grad_fn=<KlDivBackward>)\n",
      "Train Epoch: 86 [40/67 (56%)]\tLoss: 0.238379\n",
      "tensor(0.1930, device='cuda:0', grad_fn=<KlDivBackward>)\n",
      "tensor(0.2409, device='cuda:0', grad_fn=<KlDivBackward>)\n",
      "tensor(0.1976, device='cuda:0', grad_fn=<KlDivBackward>)\n",
      "tensor([[-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094]], device='cuda:0',\n",
      "       grad_fn=<LogSoftmaxBackward>)\n",
      "tensor([[0.0117, 0.0861, 0.6364, 0.2341, 0.0317],\n",
      "        [0.0388, 0.0388, 0.7784, 0.1053, 0.0388],\n",
      "        [0.0157, 0.0426, 0.0426, 0.0426, 0.8564],\n",
      "        [0.1441, 0.3917, 0.0530, 0.0195, 0.3917],\n",
      "        [0.1915, 0.0705, 0.5206, 0.0259, 0.1915],\n",
      "        [0.0202, 0.1491, 0.4053, 0.0202, 0.4053],\n",
      "        [0.1044, 0.0141, 0.0052, 0.1044, 0.7718],\n",
      "        [0.3483, 0.3483, 0.0471, 0.1281, 0.1281]], device='cuda:0')\n",
      "\n",
      "\n",
      "tensor([[-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094]], device='cuda:0',\n",
      "       grad_fn=<LogSoftmaxBackward>)\n",
      "tensor([[0.0146, 0.0397, 0.7979, 0.1080, 0.0397],\n",
      "        [0.6869, 0.0930, 0.0930, 0.0342, 0.0930],\n",
      "        [0.0161, 0.0438, 0.8801, 0.0438, 0.0161],\n",
      "        [0.0047, 0.0345, 0.6931, 0.0127, 0.2550],\n",
      "        [0.0408, 0.0150, 0.8185, 0.0150, 0.1108],\n",
      "        [0.0161, 0.0438, 0.0438, 0.0161, 0.8801],\n",
      "        [0.1643, 0.0604, 0.4466, 0.1643, 0.1643],\n",
      "        [0.0317, 0.0861, 0.0117, 0.2341, 0.6364]], device='cuda:0')\n",
      "\n",
      "\n",
      "\n",
      "Test set: Accuracy: 2/16 (12%)\n",
      "\n",
      "training epoch 87\n",
      "tensor(0.1842, device='cuda:0', grad_fn=<KlDivBackward>)\n",
      "Train Epoch: 87 [0/67 (0%)]\tLoss: 0.184206\n",
      "tensor(0.2164, device='cuda:0', grad_fn=<KlDivBackward>)\n",
      "tensor(0.2027, device='cuda:0', grad_fn=<KlDivBackward>)\n",
      "tensor(0.2307, device='cuda:0', grad_fn=<KlDivBackward>)\n",
      "tensor(0.2129, device='cuda:0', grad_fn=<KlDivBackward>)\n",
      "tensor(0.2348, device='cuda:0', grad_fn=<KlDivBackward>)\n",
      "Train Epoch: 87 [40/67 (56%)]\tLoss: 0.234850\n",
      "tensor(0.2594, device='cuda:0', grad_fn=<KlDivBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.1736, device='cuda:0', grad_fn=<KlDivBackward>)\n",
      "tensor(0.1897, device='cuda:0', grad_fn=<KlDivBackward>)\n",
      "tensor([[-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094]], device='cuda:0',\n",
      "       grad_fn=<LogSoftmaxBackward>)\n",
      "tensor([[0.1044, 0.0141, 0.0052, 0.1044, 0.7718],\n",
      "        [0.0117, 0.0861, 0.6364, 0.2341, 0.0317],\n",
      "        [0.1643, 0.0604, 0.4466, 0.1643, 0.1643],\n",
      "        [0.0317, 0.0861, 0.0117, 0.2341, 0.6364],\n",
      "        [0.0161, 0.0438, 0.0438, 0.0161, 0.8801],\n",
      "        [0.0161, 0.0438, 0.8801, 0.0438, 0.0161],\n",
      "        [0.1441, 0.3917, 0.0530, 0.0195, 0.3917],\n",
      "        [0.0047, 0.0345, 0.6931, 0.0127, 0.2550]], device='cuda:0')\n",
      "\n",
      "\n",
      "tensor([[-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094]], device='cuda:0',\n",
      "       grad_fn=<LogSoftmaxBackward>)\n",
      "tensor([[0.3483, 0.3483, 0.0471, 0.1281, 0.1281],\n",
      "        [0.0408, 0.0150, 0.8185, 0.0150, 0.1108],\n",
      "        [0.0202, 0.1491, 0.4053, 0.0202, 0.4053],\n",
      "        [0.1915, 0.0705, 0.5206, 0.0259, 0.1915],\n",
      "        [0.6869, 0.0930, 0.0930, 0.0342, 0.0930],\n",
      "        [0.0388, 0.0388, 0.7784, 0.1053, 0.0388],\n",
      "        [0.0146, 0.0397, 0.7979, 0.1080, 0.0397],\n",
      "        [0.0157, 0.0426, 0.0426, 0.0426, 0.8564]], device='cuda:0')\n",
      "\n",
      "\n",
      "\n",
      "Test set: Accuracy: 2/16 (12%)\n",
      "\n",
      "training epoch 88\n",
      "tensor(0.1590, device='cuda:0', grad_fn=<KlDivBackward>)\n",
      "Train Epoch: 88 [0/67 (0%)]\tLoss: 0.158995\n",
      "tensor(0.2596, device='cuda:0', grad_fn=<KlDivBackward>)\n",
      "tensor(0.1852, device='cuda:0', grad_fn=<KlDivBackward>)\n",
      "tensor(0.2266, device='cuda:0', grad_fn=<KlDivBackward>)\n",
      "tensor(0.2143, device='cuda:0', grad_fn=<KlDivBackward>)\n",
      "tensor(0.2319, device='cuda:0', grad_fn=<KlDivBackward>)\n",
      "Train Epoch: 88 [40/67 (56%)]\tLoss: 0.231891\n",
      "tensor(0.2379, device='cuda:0', grad_fn=<KlDivBackward>)\n",
      "tensor(0.2068, device='cuda:0', grad_fn=<KlDivBackward>)\n",
      "tensor(0.1727, device='cuda:0', grad_fn=<KlDivBackward>)\n",
      "tensor([[-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094]], device='cuda:0',\n",
      "       grad_fn=<LogSoftmaxBackward>)\n",
      "tensor([[0.0146, 0.0397, 0.7979, 0.1080, 0.0397],\n",
      "        [0.0161, 0.0438, 0.8801, 0.0438, 0.0161],\n",
      "        [0.0161, 0.0438, 0.0438, 0.0161, 0.8801],\n",
      "        [0.3483, 0.3483, 0.0471, 0.1281, 0.1281],\n",
      "        [0.0408, 0.0150, 0.8185, 0.0150, 0.1108],\n",
      "        [0.1643, 0.0604, 0.4466, 0.1643, 0.1643],\n",
      "        [0.0317, 0.0861, 0.0117, 0.2341, 0.6364],\n",
      "        [0.1441, 0.3917, 0.0530, 0.0195, 0.3917]], device='cuda:0')\n",
      "\n",
      "\n",
      "tensor([[-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094]], device='cuda:0',\n",
      "       grad_fn=<LogSoftmaxBackward>)\n",
      "tensor([[0.1044, 0.0141, 0.0052, 0.1044, 0.7718],\n",
      "        [0.6869, 0.0930, 0.0930, 0.0342, 0.0930],\n",
      "        [0.0388, 0.0388, 0.7784, 0.1053, 0.0388],\n",
      "        [0.0047, 0.0345, 0.6931, 0.0127, 0.2550],\n",
      "        [0.0157, 0.0426, 0.0426, 0.0426, 0.8564],\n",
      "        [0.0202, 0.1491, 0.4053, 0.0202, 0.4053],\n",
      "        [0.1915, 0.0705, 0.5206, 0.0259, 0.1915],\n",
      "        [0.0117, 0.0861, 0.6364, 0.2341, 0.0317]], device='cuda:0')\n",
      "\n",
      "\n",
      "\n",
      "Test set: Accuracy: 2/16 (12%)\n",
      "\n",
      "training epoch 89\n",
      "tensor(0.2284, device='cuda:0', grad_fn=<KlDivBackward>)\n",
      "Train Epoch: 89 [0/67 (0%)]\tLoss: 0.228427\n",
      "tensor(0.2597, device='cuda:0', grad_fn=<KlDivBackward>)\n",
      "tensor(0.1915, device='cuda:0', grad_fn=<KlDivBackward>)\n",
      "tensor(0.1891, device='cuda:0', grad_fn=<KlDivBackward>)\n",
      "tensor(0.1791, device='cuda:0', grad_fn=<KlDivBackward>)\n",
      "tensor(0.2515, device='cuda:0', grad_fn=<KlDivBackward>)\n",
      "Train Epoch: 89 [40/67 (56%)]\tLoss: 0.251495\n",
      "tensor(0.2252, device='cuda:0', grad_fn=<KlDivBackward>)\n",
      "tensor(0.2004, device='cuda:0', grad_fn=<KlDivBackward>)\n",
      "tensor(0.1625, device='cuda:0', grad_fn=<KlDivBackward>)\n",
      "tensor([[-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094]], device='cuda:0',\n",
      "       grad_fn=<LogSoftmaxBackward>)\n",
      "tensor([[0.0161, 0.0438, 0.8801, 0.0438, 0.0161],\n",
      "        [0.0388, 0.0388, 0.7784, 0.1053, 0.0388],\n",
      "        [0.0408, 0.0150, 0.8185, 0.0150, 0.1108],\n",
      "        [0.0317, 0.0861, 0.0117, 0.2341, 0.6364],\n",
      "        [0.0117, 0.0861, 0.6364, 0.2341, 0.0317],\n",
      "        [0.0161, 0.0438, 0.0438, 0.0161, 0.8801],\n",
      "        [0.0146, 0.0397, 0.7979, 0.1080, 0.0397],\n",
      "        [0.3483, 0.3483, 0.0471, 0.1281, 0.1281]], device='cuda:0')\n",
      "\n",
      "\n",
      "tensor([[-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094]], device='cuda:0',\n",
      "       grad_fn=<LogSoftmaxBackward>)\n",
      "tensor([[0.0047, 0.0345, 0.6931, 0.0127, 0.2550],\n",
      "        [0.6869, 0.0930, 0.0930, 0.0342, 0.0930],\n",
      "        [0.0202, 0.1491, 0.4053, 0.0202, 0.4053],\n",
      "        [0.1441, 0.3917, 0.0530, 0.0195, 0.3917],\n",
      "        [0.1915, 0.0705, 0.5206, 0.0259, 0.1915],\n",
      "        [0.1044, 0.0141, 0.0052, 0.1044, 0.7718],\n",
      "        [0.1643, 0.0604, 0.4466, 0.1643, 0.1643],\n",
      "        [0.0157, 0.0426, 0.0426, 0.0426, 0.8564]], device='cuda:0')\n",
      "\n",
      "\n",
      "\n",
      "Test set: Accuracy: 2/16 (12%)\n",
      "\n",
      "training epoch 90\n",
      "tensor(0.2456, device='cuda:0', grad_fn=<KlDivBackward>)\n",
      "Train Epoch: 90 [0/67 (0%)]\tLoss: 0.245584\n",
      "tensor(0.1870, device='cuda:0', grad_fn=<KlDivBackward>)\n",
      "tensor(0.1975, device='cuda:0', grad_fn=<KlDivBackward>)\n",
      "tensor(0.2230, device='cuda:0', grad_fn=<KlDivBackward>)\n",
      "tensor(0.2150, device='cuda:0', grad_fn=<KlDivBackward>)\n",
      "tensor(0.2272, device='cuda:0', grad_fn=<KlDivBackward>)\n",
      "Train Epoch: 90 [40/67 (56%)]\tLoss: 0.227223\n",
      "tensor(0.1743, device='cuda:0', grad_fn=<KlDivBackward>)\n",
      "tensor(0.2491, device='cuda:0', grad_fn=<KlDivBackward>)\n",
      "tensor(0.1794, device='cuda:0', grad_fn=<KlDivBackward>)\n",
      "tensor([[-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094]], device='cuda:0',\n",
      "       grad_fn=<LogSoftmaxBackward>)\n",
      "tensor([[0.0146, 0.0397, 0.7979, 0.1080, 0.0397],\n",
      "        [0.0317, 0.0861, 0.0117, 0.2341, 0.6364],\n",
      "        [0.1643, 0.0604, 0.4466, 0.1643, 0.1643],\n",
      "        [0.0161, 0.0438, 0.0438, 0.0161, 0.8801],\n",
      "        [0.0047, 0.0345, 0.6931, 0.0127, 0.2550],\n",
      "        [0.0117, 0.0861, 0.6364, 0.2341, 0.0317],\n",
      "        [0.0157, 0.0426, 0.0426, 0.0426, 0.8564],\n",
      "        [0.0161, 0.0438, 0.8801, 0.0438, 0.0161]], device='cuda:0')\n",
      "\n",
      "\n",
      "tensor([[-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094]], device='cuda:0',\n",
      "       grad_fn=<LogSoftmaxBackward>)\n",
      "tensor([[0.1915, 0.0705, 0.5206, 0.0259, 0.1915],\n",
      "        [0.1044, 0.0141, 0.0052, 0.1044, 0.7718],\n",
      "        [0.0388, 0.0388, 0.7784, 0.1053, 0.0388],\n",
      "        [0.6869, 0.0930, 0.0930, 0.0342, 0.0930],\n",
      "        [0.3483, 0.3483, 0.0471, 0.1281, 0.1281],\n",
      "        [0.1441, 0.3917, 0.0530, 0.0195, 0.3917],\n",
      "        [0.0202, 0.1491, 0.4053, 0.0202, 0.4053],\n",
      "        [0.0408, 0.0150, 0.8185, 0.0150, 0.1108]], device='cuda:0')\n",
      "\n",
      "\n",
      "\n",
      "Test set: Accuracy: 2/16 (12%)\n",
      "\n",
      "training epoch 91\n",
      "tensor(0.1991, device='cuda:0', grad_fn=<KlDivBackward>)\n",
      "Train Epoch: 91 [0/67 (0%)]\tLoss: 0.199138\n",
      "tensor(0.1840, device='cuda:0', grad_fn=<KlDivBackward>)\n",
      "tensor(0.2233, device='cuda:0', grad_fn=<KlDivBackward>)\n",
      "tensor(0.2087, device='cuda:0', grad_fn=<KlDivBackward>)\n",
      "tensor(0.2033, device='cuda:0', grad_fn=<KlDivBackward>)\n",
      "tensor(0.2764, device='cuda:0', grad_fn=<KlDivBackward>)\n",
      "Train Epoch: 91 [40/67 (56%)]\tLoss: 0.276410\n",
      "tensor(0.2168, device='cuda:0', grad_fn=<KlDivBackward>)\n",
      "tensor(0.1939, device='cuda:0', grad_fn=<KlDivBackward>)\n",
      "tensor(0.2143, device='cuda:0', grad_fn=<KlDivBackward>)\n",
      "tensor([[-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094]], device='cuda:0',\n",
      "       grad_fn=<LogSoftmaxBackward>)\n",
      "tensor([[0.0146, 0.0397, 0.7979, 0.1080, 0.0397],\n",
      "        [0.0408, 0.0150, 0.8185, 0.0150, 0.1108],\n",
      "        [0.0317, 0.0861, 0.0117, 0.2341, 0.6364],\n",
      "        [0.0117, 0.0861, 0.6364, 0.2341, 0.0317],\n",
      "        [0.3483, 0.3483, 0.0471, 0.1281, 0.1281],\n",
      "        [0.1643, 0.0604, 0.4466, 0.1643, 0.1643],\n",
      "        [0.0047, 0.0345, 0.6931, 0.0127, 0.2550],\n",
      "        [0.1441, 0.3917, 0.0530, 0.0195, 0.3917]], device='cuda:0')\n",
      "\n",
      "\n",
      "tensor([[-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094]], device='cuda:0',\n",
      "       grad_fn=<LogSoftmaxBackward>)\n",
      "tensor([[0.0202, 0.1491, 0.4053, 0.0202, 0.4053],\n",
      "        [0.1915, 0.0705, 0.5206, 0.0259, 0.1915],\n",
      "        [0.0157, 0.0426, 0.0426, 0.0426, 0.8564],\n",
      "        [0.0161, 0.0438, 0.8801, 0.0438, 0.0161],\n",
      "        [0.0161, 0.0438, 0.0438, 0.0161, 0.8801],\n",
      "        [0.0388, 0.0388, 0.7784, 0.1053, 0.0388],\n",
      "        [0.1044, 0.0141, 0.0052, 0.1044, 0.7718],\n",
      "        [0.6869, 0.0930, 0.0930, 0.0342, 0.0930]], device='cuda:0')\n",
      "\n",
      "\n",
      "\n",
      "Test set: Accuracy: 2/16 (12%)\n",
      "\n",
      "training epoch 92\n",
      "tensor(0.1596, device='cuda:0', grad_fn=<KlDivBackward>)\n",
      "Train Epoch: 92 [0/67 (0%)]\tLoss: 0.159560\n",
      "tensor(0.2417, device='cuda:0', grad_fn=<KlDivBackward>)\n",
      "tensor(0.2126, device='cuda:0', grad_fn=<KlDivBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.1835, device='cuda:0', grad_fn=<KlDivBackward>)\n",
      "tensor(0.2337, device='cuda:0', grad_fn=<KlDivBackward>)\n",
      "tensor(0.2529, device='cuda:0', grad_fn=<KlDivBackward>)\n",
      "Train Epoch: 92 [40/67 (56%)]\tLoss: 0.252929\n",
      "tensor(0.1992, device='cuda:0', grad_fn=<KlDivBackward>)\n",
      "tensor(0.2229, device='cuda:0', grad_fn=<KlDivBackward>)\n",
      "tensor(0.2132, device='cuda:0', grad_fn=<KlDivBackward>)\n",
      "tensor([[-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094]], device='cuda:0',\n",
      "       grad_fn=<LogSoftmaxBackward>)\n",
      "tensor([[0.0117, 0.0861, 0.6364, 0.2341, 0.0317],\n",
      "        [0.0317, 0.0861, 0.0117, 0.2341, 0.6364],\n",
      "        [0.3483, 0.3483, 0.0471, 0.1281, 0.1281],\n",
      "        [0.1915, 0.0705, 0.5206, 0.0259, 0.1915],\n",
      "        [0.1441, 0.3917, 0.0530, 0.0195, 0.3917],\n",
      "        [0.0146, 0.0397, 0.7979, 0.1080, 0.0397],\n",
      "        [0.0157, 0.0426, 0.0426, 0.0426, 0.8564],\n",
      "        [0.1044, 0.0141, 0.0052, 0.1044, 0.7718]], device='cuda:0')\n",
      "\n",
      "\n",
      "tensor([[-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094]], device='cuda:0',\n",
      "       grad_fn=<LogSoftmaxBackward>)\n",
      "tensor([[0.0161, 0.0438, 0.8801, 0.0438, 0.0161],\n",
      "        [0.0161, 0.0438, 0.0438, 0.0161, 0.8801],\n",
      "        [0.0047, 0.0345, 0.6931, 0.0127, 0.2550],\n",
      "        [0.0202, 0.1491, 0.4053, 0.0202, 0.4053],\n",
      "        [0.0408, 0.0150, 0.8185, 0.0150, 0.1108],\n",
      "        [0.1643, 0.0604, 0.4466, 0.1643, 0.1643],\n",
      "        [0.0388, 0.0388, 0.7784, 0.1053, 0.0388],\n",
      "        [0.6869, 0.0930, 0.0930, 0.0342, 0.0930]], device='cuda:0')\n",
      "\n",
      "\n",
      "\n",
      "Test set: Accuracy: 2/16 (12%)\n",
      "\n",
      "training epoch 93\n",
      "tensor(0.1534, device='cuda:0', grad_fn=<KlDivBackward>)\n",
      "Train Epoch: 93 [0/67 (0%)]\tLoss: 0.153364\n",
      "tensor(0.2271, device='cuda:0', grad_fn=<KlDivBackward>)\n",
      "tensor(0.2450, device='cuda:0', grad_fn=<KlDivBackward>)\n",
      "tensor(0.2165, device='cuda:0', grad_fn=<KlDivBackward>)\n",
      "tensor(0.2036, device='cuda:0', grad_fn=<KlDivBackward>)\n",
      "tensor(0.2033, device='cuda:0', grad_fn=<KlDivBackward>)\n",
      "Train Epoch: 93 [40/67 (56%)]\tLoss: 0.203278\n",
      "tensor(0.2163, device='cuda:0', grad_fn=<KlDivBackward>)\n",
      "tensor(0.2547, device='cuda:0', grad_fn=<KlDivBackward>)\n",
      "tensor(0.1763, device='cuda:0', grad_fn=<KlDivBackward>)\n",
      "tensor([[-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094]], device='cuda:0',\n",
      "       grad_fn=<LogSoftmaxBackward>)\n",
      "tensor([[0.6869, 0.0930, 0.0930, 0.0342, 0.0930],\n",
      "        [0.3483, 0.3483, 0.0471, 0.1281, 0.1281],\n",
      "        [0.1643, 0.0604, 0.4466, 0.1643, 0.1643],\n",
      "        [0.0146, 0.0397, 0.7979, 0.1080, 0.0397],\n",
      "        [0.1915, 0.0705, 0.5206, 0.0259, 0.1915],\n",
      "        [0.0117, 0.0861, 0.6364, 0.2341, 0.0317],\n",
      "        [0.0408, 0.0150, 0.8185, 0.0150, 0.1108],\n",
      "        [0.0047, 0.0345, 0.6931, 0.0127, 0.2550]], device='cuda:0')\n",
      "\n",
      "\n",
      "tensor([[-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094]], device='cuda:0',\n",
      "       grad_fn=<LogSoftmaxBackward>)\n",
      "tensor([[0.1044, 0.0141, 0.0052, 0.1044, 0.7718],\n",
      "        [0.0388, 0.0388, 0.7784, 0.1053, 0.0388],\n",
      "        [0.1441, 0.3917, 0.0530, 0.0195, 0.3917],\n",
      "        [0.0161, 0.0438, 0.0438, 0.0161, 0.8801],\n",
      "        [0.0161, 0.0438, 0.8801, 0.0438, 0.0161],\n",
      "        [0.0317, 0.0861, 0.0117, 0.2341, 0.6364],\n",
      "        [0.0202, 0.1491, 0.4053, 0.0202, 0.4053],\n",
      "        [0.0157, 0.0426, 0.0426, 0.0426, 0.8564]], device='cuda:0')\n",
      "\n",
      "\n",
      "\n",
      "Test set: Accuracy: 2/16 (12%)\n",
      "\n",
      "training epoch 94\n",
      "tensor(0.2247, device='cuda:0', grad_fn=<KlDivBackward>)\n",
      "Train Epoch: 94 [0/67 (0%)]\tLoss: 0.224665\n",
      "tensor(0.2519, device='cuda:0', grad_fn=<KlDivBackward>)\n",
      "tensor(0.2373, device='cuda:0', grad_fn=<KlDivBackward>)\n",
      "tensor(0.2448, device='cuda:0', grad_fn=<KlDivBackward>)\n",
      "tensor(0.1926, device='cuda:0', grad_fn=<KlDivBackward>)\n",
      "tensor(0.1772, device='cuda:0', grad_fn=<KlDivBackward>)\n",
      "Train Epoch: 94 [40/67 (56%)]\tLoss: 0.177213\n",
      "tensor(0.2167, device='cuda:0', grad_fn=<KlDivBackward>)\n",
      "tensor(0.2052, device='cuda:0', grad_fn=<KlDivBackward>)\n",
      "tensor(0.0950, device='cuda:0', grad_fn=<KlDivBackward>)\n",
      "tensor([[-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094]], device='cuda:0',\n",
      "       grad_fn=<LogSoftmaxBackward>)\n",
      "tensor([[0.1643, 0.0604, 0.4466, 0.1643, 0.1643],\n",
      "        [0.0388, 0.0388, 0.7784, 0.1053, 0.0388],\n",
      "        [0.6869, 0.0930, 0.0930, 0.0342, 0.0930],\n",
      "        [0.0047, 0.0345, 0.6931, 0.0127, 0.2550],\n",
      "        [0.1915, 0.0705, 0.5206, 0.0259, 0.1915],\n",
      "        [0.0202, 0.1491, 0.4053, 0.0202, 0.4053],\n",
      "        [0.0317, 0.0861, 0.0117, 0.2341, 0.6364],\n",
      "        [0.0157, 0.0426, 0.0426, 0.0426, 0.8564]], device='cuda:0')\n",
      "\n",
      "\n",
      "tensor([[-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094]], device='cuda:0',\n",
      "       grad_fn=<LogSoftmaxBackward>)\n",
      "tensor([[0.1044, 0.0141, 0.0052, 0.1044, 0.7718],\n",
      "        [0.0161, 0.0438, 0.8801, 0.0438, 0.0161],\n",
      "        [0.0408, 0.0150, 0.8185, 0.0150, 0.1108],\n",
      "        [0.3483, 0.3483, 0.0471, 0.1281, 0.1281],\n",
      "        [0.1441, 0.3917, 0.0530, 0.0195, 0.3917],\n",
      "        [0.0146, 0.0397, 0.7979, 0.1080, 0.0397],\n",
      "        [0.0161, 0.0438, 0.0438, 0.0161, 0.8801],\n",
      "        [0.0117, 0.0861, 0.6364, 0.2341, 0.0317]], device='cuda:0')\n",
      "\n",
      "\n",
      "\n",
      "Test set: Accuracy: 2/16 (12%)\n",
      "\n",
      "training epoch 95\n",
      "tensor(0.2144, device='cuda:0', grad_fn=<KlDivBackward>)\n",
      "Train Epoch: 95 [0/67 (0%)]\tLoss: 0.214381\n",
      "tensor(0.2721, device='cuda:0', grad_fn=<KlDivBackward>)\n",
      "tensor(0.1981, device='cuda:0', grad_fn=<KlDivBackward>)\n",
      "tensor(0.1966, device='cuda:0', grad_fn=<KlDivBackward>)\n",
      "tensor(0.2193, device='cuda:0', grad_fn=<KlDivBackward>)\n",
      "tensor(0.2163, device='cuda:0', grad_fn=<KlDivBackward>)\n",
      "Train Epoch: 95 [40/67 (56%)]\tLoss: 0.216251\n",
      "tensor(0.2219, device='cuda:0', grad_fn=<KlDivBackward>)\n",
      "tensor(0.2017, device='cuda:0', grad_fn=<KlDivBackward>)\n",
      "tensor(0.1216, device='cuda:0', grad_fn=<KlDivBackward>)\n",
      "tensor([[-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094]], device='cuda:0',\n",
      "       grad_fn=<LogSoftmaxBackward>)\n",
      "tensor([[0.0146, 0.0397, 0.7979, 0.1080, 0.0397],\n",
      "        [0.0202, 0.1491, 0.4053, 0.0202, 0.4053],\n",
      "        [0.0317, 0.0861, 0.0117, 0.2341, 0.6364],\n",
      "        [0.3483, 0.3483, 0.0471, 0.1281, 0.1281],\n",
      "        [0.0161, 0.0438, 0.8801, 0.0438, 0.0161],\n",
      "        [0.0117, 0.0861, 0.6364, 0.2341, 0.0317],\n",
      "        [0.1044, 0.0141, 0.0052, 0.1044, 0.7718],\n",
      "        [0.1643, 0.0604, 0.4466, 0.1643, 0.1643]], device='cuda:0')\n",
      "\n",
      "\n",
      "tensor([[-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094]], device='cuda:0',\n",
      "       grad_fn=<LogSoftmaxBackward>)\n",
      "tensor([[0.0161, 0.0438, 0.0438, 0.0161, 0.8801],\n",
      "        [0.0408, 0.0150, 0.8185, 0.0150, 0.1108],\n",
      "        [0.1915, 0.0705, 0.5206, 0.0259, 0.1915],\n",
      "        [0.6869, 0.0930, 0.0930, 0.0342, 0.0930],\n",
      "        [0.1441, 0.3917, 0.0530, 0.0195, 0.3917],\n",
      "        [0.0047, 0.0345, 0.6931, 0.0127, 0.2550],\n",
      "        [0.0157, 0.0426, 0.0426, 0.0426, 0.8564],\n",
      "        [0.0388, 0.0388, 0.7784, 0.1053, 0.0388]], device='cuda:0')\n",
      "\n",
      "\n",
      "\n",
      "Test set: Accuracy: 2/16 (12%)\n",
      "\n",
      "training epoch 96\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.1864, device='cuda:0', grad_fn=<KlDivBackward>)\n",
      "Train Epoch: 96 [0/67 (0%)]\tLoss: 0.186449\n",
      "tensor(0.2172, device='cuda:0', grad_fn=<KlDivBackward>)\n",
      "tensor(0.1822, device='cuda:0', grad_fn=<KlDivBackward>)\n",
      "tensor(0.2443, device='cuda:0', grad_fn=<KlDivBackward>)\n",
      "tensor(0.2171, device='cuda:0', grad_fn=<KlDivBackward>)\n",
      "tensor(0.2226, device='cuda:0', grad_fn=<KlDivBackward>)\n",
      "Train Epoch: 96 [40/67 (56%)]\tLoss: 0.222606\n",
      "tensor(0.2170, device='cuda:0', grad_fn=<KlDivBackward>)\n",
      "tensor(0.2150, device='cuda:0', grad_fn=<KlDivBackward>)\n",
      "tensor(0.2246, device='cuda:0', grad_fn=<KlDivBackward>)\n",
      "tensor([[-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094]], device='cuda:0',\n",
      "       grad_fn=<LogSoftmaxBackward>)\n",
      "tensor([[0.6869, 0.0930, 0.0930, 0.0342, 0.0930],\n",
      "        [0.0388, 0.0388, 0.7784, 0.1053, 0.0388],\n",
      "        [0.0146, 0.0397, 0.7979, 0.1080, 0.0397],\n",
      "        [0.0161, 0.0438, 0.0438, 0.0161, 0.8801],\n",
      "        [0.0317, 0.0861, 0.0117, 0.2341, 0.6364],\n",
      "        [0.1044, 0.0141, 0.0052, 0.1044, 0.7718],\n",
      "        [0.1643, 0.0604, 0.4466, 0.1643, 0.1643],\n",
      "        [0.0202, 0.1491, 0.4053, 0.0202, 0.4053]], device='cuda:0')\n",
      "\n",
      "\n",
      "tensor([[-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094]], device='cuda:0',\n",
      "       grad_fn=<LogSoftmaxBackward>)\n",
      "tensor([[0.3483, 0.3483, 0.0471, 0.1281, 0.1281],\n",
      "        [0.0408, 0.0150, 0.8185, 0.0150, 0.1108],\n",
      "        [0.1915, 0.0705, 0.5206, 0.0259, 0.1915],\n",
      "        [0.0157, 0.0426, 0.0426, 0.0426, 0.8564],\n",
      "        [0.0161, 0.0438, 0.8801, 0.0438, 0.0161],\n",
      "        [0.1441, 0.3917, 0.0530, 0.0195, 0.3917],\n",
      "        [0.0047, 0.0345, 0.6931, 0.0127, 0.2550],\n",
      "        [0.0117, 0.0861, 0.6364, 0.2341, 0.0317]], device='cuda:0')\n",
      "\n",
      "\n",
      "\n",
      "Test set: Accuracy: 2/16 (12%)\n",
      "\n",
      "training epoch 97\n",
      "tensor(0.2006, device='cuda:0', grad_fn=<KlDivBackward>)\n",
      "Train Epoch: 97 [0/67 (0%)]\tLoss: 0.200601\n",
      "tensor(0.2244, device='cuda:0', grad_fn=<KlDivBackward>)\n",
      "tensor(0.2368, device='cuda:0', grad_fn=<KlDivBackward>)\n",
      "tensor(0.2333, device='cuda:0', grad_fn=<KlDivBackward>)\n",
      "tensor(0.1610, device='cuda:0', grad_fn=<KlDivBackward>)\n",
      "tensor(0.2085, device='cuda:0', grad_fn=<KlDivBackward>)\n",
      "Train Epoch: 97 [40/67 (56%)]\tLoss: 0.208494\n",
      "tensor(0.2037, device='cuda:0', grad_fn=<KlDivBackward>)\n",
      "tensor(0.2487, device='cuda:0', grad_fn=<KlDivBackward>)\n",
      "tensor(0.1837, device='cuda:0', grad_fn=<KlDivBackward>)\n",
      "tensor([[-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094]], device='cuda:0',\n",
      "       grad_fn=<LogSoftmaxBackward>)\n",
      "tensor([[0.0157, 0.0426, 0.0426, 0.0426, 0.8564],\n",
      "        [0.1044, 0.0141, 0.0052, 0.1044, 0.7718],\n",
      "        [0.0047, 0.0345, 0.6931, 0.0127, 0.2550],\n",
      "        [0.0161, 0.0438, 0.0438, 0.0161, 0.8801],\n",
      "        [0.6869, 0.0930, 0.0930, 0.0342, 0.0930],\n",
      "        [0.1915, 0.0705, 0.5206, 0.0259, 0.1915],\n",
      "        [0.0161, 0.0438, 0.8801, 0.0438, 0.0161],\n",
      "        [0.0317, 0.0861, 0.0117, 0.2341, 0.6364]], device='cuda:0')\n",
      "\n",
      "\n",
      "tensor([[-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094]], device='cuda:0',\n",
      "       grad_fn=<LogSoftmaxBackward>)\n",
      "tensor([[0.0146, 0.0397, 0.7979, 0.1080, 0.0397],\n",
      "        [0.0388, 0.0388, 0.7784, 0.1053, 0.0388],\n",
      "        [0.0408, 0.0150, 0.8185, 0.0150, 0.1108],\n",
      "        [0.0202, 0.1491, 0.4053, 0.0202, 0.4053],\n",
      "        [0.0117, 0.0861, 0.6364, 0.2341, 0.0317],\n",
      "        [0.1643, 0.0604, 0.4466, 0.1643, 0.1643],\n",
      "        [0.1441, 0.3917, 0.0530, 0.0195, 0.3917],\n",
      "        [0.3483, 0.3483, 0.0471, 0.1281, 0.1281]], device='cuda:0')\n",
      "\n",
      "\n",
      "\n",
      "Test set: Accuracy: 2/16 (12%)\n",
      "\n",
      "training epoch 98\n",
      "tensor(0.1639, device='cuda:0', grad_fn=<KlDivBackward>)\n",
      "Train Epoch: 98 [0/67 (0%)]\tLoss: 0.163899\n",
      "tensor(0.2341, device='cuda:0', grad_fn=<KlDivBackward>)\n",
      "tensor(0.2344, device='cuda:0', grad_fn=<KlDivBackward>)\n",
      "tensor(0.2293, device='cuda:0', grad_fn=<KlDivBackward>)\n",
      "tensor(0.2039, device='cuda:0', grad_fn=<KlDivBackward>)\n",
      "tensor(0.2156, device='cuda:0', grad_fn=<KlDivBackward>)\n",
      "Train Epoch: 98 [40/67 (56%)]\tLoss: 0.215571\n",
      "tensor(0.2287, device='cuda:0', grad_fn=<KlDivBackward>)\n",
      "tensor(0.1952, device='cuda:0', grad_fn=<KlDivBackward>)\n",
      "tensor(0.2157, device='cuda:0', grad_fn=<KlDivBackward>)\n",
      "tensor([[-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094]], device='cuda:0',\n",
      "       grad_fn=<LogSoftmaxBackward>)\n",
      "tensor([[0.1441, 0.3917, 0.0530, 0.0195, 0.3917],\n",
      "        [0.0408, 0.0150, 0.8185, 0.0150, 0.1108],\n",
      "        [0.0388, 0.0388, 0.7784, 0.1053, 0.0388],\n",
      "        [0.0202, 0.1491, 0.4053, 0.0202, 0.4053],\n",
      "        [0.1643, 0.0604, 0.4466, 0.1643, 0.1643],\n",
      "        [0.1915, 0.0705, 0.5206, 0.0259, 0.1915],\n",
      "        [0.0047, 0.0345, 0.6931, 0.0127, 0.2550],\n",
      "        [0.1044, 0.0141, 0.0052, 0.1044, 0.7718]], device='cuda:0')\n",
      "\n",
      "\n",
      "tensor([[-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094]], device='cuda:0',\n",
      "       grad_fn=<LogSoftmaxBackward>)\n",
      "tensor([[0.0161, 0.0438, 0.0438, 0.0161, 0.8801],\n",
      "        [0.6869, 0.0930, 0.0930, 0.0342, 0.0930],\n",
      "        [0.0317, 0.0861, 0.0117, 0.2341, 0.6364],\n",
      "        [0.0161, 0.0438, 0.8801, 0.0438, 0.0161],\n",
      "        [0.0117, 0.0861, 0.6364, 0.2341, 0.0317],\n",
      "        [0.0146, 0.0397, 0.7979, 0.1080, 0.0397],\n",
      "        [0.3483, 0.3483, 0.0471, 0.1281, 0.1281],\n",
      "        [0.0157, 0.0426, 0.0426, 0.0426, 0.8564]], device='cuda:0')\n",
      "\n",
      "\n",
      "\n",
      "Test set: Accuracy: 2/16 (12%)\n",
      "\n",
      "training epoch 99\n",
      "tensor(0.1560, device='cuda:0', grad_fn=<KlDivBackward>)\n",
      "Train Epoch: 99 [0/67 (0%)]\tLoss: 0.156021\n",
      "tensor(0.2023, device='cuda:0', grad_fn=<KlDivBackward>)\n",
      "tensor(0.2200, device='cuda:0', grad_fn=<KlDivBackward>)\n",
      "tensor(0.2219, device='cuda:0', grad_fn=<KlDivBackward>)\n",
      "tensor(0.2261, device='cuda:0', grad_fn=<KlDivBackward>)\n",
      "tensor(0.2266, device='cuda:0', grad_fn=<KlDivBackward>)\n",
      "Train Epoch: 99 [40/67 (56%)]\tLoss: 0.226643\n",
      "tensor(0.2186, device='cuda:0', grad_fn=<KlDivBackward>)\n",
      "tensor(0.2504, device='cuda:0', grad_fn=<KlDivBackward>)\n",
      "tensor(0.1708, device='cuda:0', grad_fn=<KlDivBackward>)\n",
      "tensor([[-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094]], device='cuda:0',\n",
      "       grad_fn=<LogSoftmaxBackward>)\n",
      "tensor([[0.1044, 0.0141, 0.0052, 0.1044, 0.7718],\n",
      "        [0.0161, 0.0438, 0.8801, 0.0438, 0.0161],\n",
      "        [0.0317, 0.0861, 0.0117, 0.2341, 0.6364],\n",
      "        [0.0408, 0.0150, 0.8185, 0.0150, 0.1108],\n",
      "        [0.0388, 0.0388, 0.7784, 0.1053, 0.0388],\n",
      "        [0.0157, 0.0426, 0.0426, 0.0426, 0.8564],\n",
      "        [0.1643, 0.0604, 0.4466, 0.1643, 0.1643],\n",
      "        [0.0117, 0.0861, 0.6364, 0.2341, 0.0317]], device='cuda:0')\n",
      "\n",
      "\n",
      "tensor([[-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094],\n",
      "        [-1.6094, -1.6094, -1.6094, -1.6094, -1.6094]], device='cuda:0',\n",
      "       grad_fn=<LogSoftmaxBackward>)\n",
      "tensor([[0.0146, 0.0397, 0.7979, 0.1080, 0.0397],\n",
      "        [0.6869, 0.0930, 0.0930, 0.0342, 0.0930],\n",
      "        [0.3483, 0.3483, 0.0471, 0.1281, 0.1281],\n",
      "        [0.0047, 0.0345, 0.6931, 0.0127, 0.2550],\n",
      "        [0.0161, 0.0438, 0.0438, 0.0161, 0.8801],\n",
      "        [0.0202, 0.1491, 0.4053, 0.0202, 0.4053],\n",
      "        [0.1441, 0.3917, 0.0530, 0.0195, 0.3917],\n",
      "        [0.1915, 0.0705, 0.5206, 0.0259, 0.1915]], device='cuda:0')\n",
      "\n",
      "\n",
      "\n",
      "Test set: Accuracy: 2/16 (12%)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "\n",
    "log_interval = 5\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "for epoch in range(1, 100):\n",
    "    print(\"training epoch \" + str(epoch))\n",
    "    if epoch == 31:\n",
    "        print(\"First round of training complete. Setting learn rate to 0.001.\")\n",
    "#     scheduler.step()\n",
    "    train(model, epoch)\n",
    "    scheduler.step()\n",
    "    test(model, epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print model's state_dict\n",
    "print(\"Model's state_dict:\")\n",
    "for param_tensor in model.state_dict():\n",
    "    print(param_tensor, \"\\t\", model.state_dict()[param_tensor].size())\n",
    "# Print optimizer's state_dict\n",
    "print(\"Optimizer's state_dict:\")\n",
    "for var_name in optimizer.state_dict():\n",
    "    print(var_name, \"\\t\", optimizer.state_dict()[var_name])\n",
    "torch.save(model.state_dict(), 'dataset_model_soundemotion.pt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NOTES\n",
    "\n",
    "below is the mfccs notes / random code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/czf/memories/venv/lib/python3.6/site-packages/librosa/core/audio.py:161: UserWarning: PySoundFile failed. Trying audioread instead.\n",
      "  warnings.warn('PySoundFile failed. Trying audioread instead.')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(40, 2586) (1323648,)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[-5.30341797e+02, -4.07741577e+02, -3.27536621e+02, ...,\n",
       "        -2.39811523e+02, -1.96744080e+02, -1.44711777e+02],\n",
       "       [ 5.81265569e-01,  1.03006027e+02,  1.29354553e+02, ...,\n",
       "         1.48707626e+02,  1.45873001e+02,  1.28202530e+02],\n",
       "       [ 4.58764762e-01,  7.53921986e+00, -1.18814125e+01, ...,\n",
       "        -2.51551704e+01, -1.92207527e+01, -1.79366188e+01],\n",
       "       ...,\n",
       "       [ 3.11299562e-01, -1.29907084e+00,  1.18818974e+00, ...,\n",
       "        -6.58579540e+00, -3.34302998e+00, -4.75482178e+00],\n",
       "       [ 2.23848164e-01, -3.19489312e+00, -2.78556681e+00, ...,\n",
       "        -1.36089420e+01, -6.40699673e+00, -5.27228928e+00],\n",
       "       [ 8.67742151e-02,  1.31472754e+00, -1.41885233e+00, ...,\n",
       "         3.34440261e-01,  1.14392626e+00, -3.62402201e-02]], dtype=float32)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "audio, sample_rate = librosa.load(\"SongEmotionDataset/1.mp3\", res_type='kaiser_fast')\n",
    "# [print(x) for x in audio]\n",
    "\n",
    "#convert audio into 2d array\n",
    "mfccs = librosa.feature.mfcc(y=audio, sr=sample_rate, n_mfcc=40)\n",
    "# mfccsscaled = np.mean(mfccs.T,axis=0)\n",
    "print(mfccs.shape, audio.shape)\n",
    "mfccs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# audio_tensor = torch.tensor(audio)\n",
    "# audio_tensor\n",
    "# audio_tensor.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for sound_file in data_path.iterdir():\n",
    "#     if \".mp3\" in str(sound_file):\n",
    "#         print(sound_file)\n",
    "#         audio, sample_rate = librosa.load(str(sound_file), res_type='kaiser_fast')\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_percentage = 0.8\n",
    "\n",
    "# min_train = min_total*train_percentage\n",
    "# min_test = min_total - min_train\n",
    "\n",
    "# train_totals = torch.zeros(len(emotions))\n",
    "\n",
    "# while \n",
    "\n",
    "\n",
    "# for i in range(1, 401):\n",
    "#     count_total = sheet.cell_value(i, 7)\n",
    "    \n",
    "#     emotions_counter = [0 for e in emotions]\n",
    "#     if i % 5 == 0:\n",
    "#         test_song.append(dataset.joinpath(\"{}.mp3\".format(i)))\n",
    "#         emotion_arr = []\n",
    "#         for j in range(5):\n",
    "#             emotion_arr.append(sheet.cell_value(i, 2 + j))\n",
    "#         test_emotion.append(torch.tensor(emotion_arr, device=device).float())\n",
    "        \n",
    "#     emotions_counter = [0 for e in emotions]\n",
    "#     else:\n",
    "#         train_song.append(dataset.joinpath(\"{}.mp3\".format(i)))\n",
    "#         emotion_arr = []\n",
    "#         for j in range(5):\n",
    "#             emotion_arr.append(sheet.cell_value(i, 2 + j))\n",
    "#         train_emotion.append(torch.tensor(emotion_arr, device=device))\n",
    "\n",
    "# print(len(train_song), len(test_song))\n",
    "# print(len(train_emotion), len(test_emotion))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "myenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
