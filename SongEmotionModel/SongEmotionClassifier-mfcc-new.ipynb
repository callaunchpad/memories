{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xlrd\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from numba import decorators\n",
    "import librosa\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import Dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)\n",
    "device = \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read in the data file\n",
    "# Give the location of the file \n",
    "\n",
    "df = pd.read_excel(r'data/data.xlsx', sheet_name='normalized')\n",
    "# print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "## LOADING IN DATASETS\n",
    "\n",
    "dataset = Path.cwd().joinpath(\"SongEmotionDataset\")\n",
    "datasheet = Path.cwd().joinpath(\"data\") # for csua\n",
    "\n",
    "#emotion labels\n",
    "label_loc = datasheet.joinpath(\"data.xlsx\")\n",
    "wb = xlrd.open_workbook(label_loc) \n",
    "sheet = wb.sheet_by_index(3)\n",
    "\n",
    "#emotion arr\n",
    "# emotions = [\"amazement\", \"calmness\", \"power\", \"joyful activation\", \"sadness\"]\n",
    "emotions = [\"solemnity\", \"tenderness\", \"nostalgia\", \"calmness\", \"power\", \"joyful activation\", \"tension\", \"sadness\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[187  54 195 137  89 119 182 191  37 203  74 162  47 127  62   6 173 155\n",
      " 166  49  30 116  28 219  68  75 174  51  23 100 150  85  11   8  83 145\n",
      " 215  10 189 128  45  16  94 157  34  53 201 185 105  41 170   3 151 161\n",
      "  57  17 114 177 223 164 130  32 141 179 206 176  43 169 212 218 198  19\n",
      "  90 111  99 184 124 199 175  46  42  55  61 225  76  96 204 178  81  20\n",
      " 186 158 211  56  73 147  48  38 193 192 217 180  63 167  77  40  29  21\n",
      "  52 134  60  67   2  87  91 190 104 143 156  39  25 202  14 188 102  66\n",
      " 163   5  24 140  95 109  35 209  86 224 110 136  36 207 139 183  78  26\n",
      " 120 103  88 101 112 129 108 142 146  50 135 200 213 165 117 138 122  15\n",
      " 133  79 160  65  71 220  59  70 148 222 115 208 131  97  18 210 106 168\n",
      " 216 171 123 153 159 194 107 172  22  80   9  93  72  82 154 113 196  64\n",
      " 214   4  27  12 197  58   7   1  31 144  13 118 221 152 132 149 121 181\n",
      " 205  44 126  84  92  33 125  98  69]\n"
     ]
    }
   ],
   "source": [
    "train_percentage = 0.8\n",
    "\n",
    "train_song = []\n",
    "test_song = []\n",
    "train_emotion = []\n",
    "test_emotion = []\n",
    "\n",
    "row_indexes = np.arange(1,226)\n",
    "np.random.shuffle(row_indexes)\n",
    "\n",
    "def get_data(indexes):\n",
    "    print(indexes)\n",
    "    song = []\n",
    "    emotion = []\n",
    "    \n",
    "    for x in indexes:\n",
    "        row = int(sheet.cell_value(x, 13) - 1)\n",
    "        trackid = int(sheet.cell_value(x, 0))\n",
    "        song.append(dataset.joinpath(\"{}.mp3\".format(trackid)))\n",
    "        emotion.append(row)\n",
    "            \n",
    "    return song, emotion\n",
    "    \n",
    "song, emotion = get_data(row_indexes)\n",
    "# print(song)\n",
    "# print(emotion)\n",
    "# test_song, test_emotion = get_data(test_indexes)\n",
    "\n",
    "train_song = [song[i] for i in range(len(song)) if i < len(song)*train_percentage]\n",
    "test_song = [song[i] for i in range(len(song)) if i >= len(song)*train_percentage]\n",
    "\n",
    "train_emotion = [emotion[i] for i in range(len(emotion)) if i < len(emotion)*train_percentage]\n",
    "test_emotion = [emotion[i] for i in range(len(emotion)) if i >= len(emotion)*train_percentage]\n",
    "\n",
    "# num_maxes = [0 for _ in emotions]\n",
    "\n",
    "# for row in emotion:\n",
    "#     i = torch.argmax(row)\n",
    "#     print(i)\n",
    "#     num_maxes[i] += 1\n",
    "    \n",
    "# print(num_maxes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_emotion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SongEmotionDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Song Emotion Dataset. Uses librosa to process mp3 files.\n",
    "    Takes first 20 seconds, and samples every 10 to get processed audio tensor.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, mp3, labels, transform=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            mp3: list of paths to mp3 files\n",
    "            labels: list of labels\n",
    "        \"\"\"\n",
    "        self.labels = labels\n",
    "        self.mp3 = mp3\n",
    "        self.cache = {}\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        if index not in self.cache.keys():\n",
    "            data, rate = librosa.load(self.mp3[index], sr=16000, duration=10)\n",
    "            mfccs = librosa.feature.mfcc(y=data, sr=rate, n_mfcc=40)\n",
    "            assert rate == 16000\n",
    "            sample_tensor = torch.tensor(mfccs, device=device)\n",
    "            downsampled_tensor = sample_tensor #[::10]\n",
    "#             print(mfccs.shape, data.shape)\n",
    "\n",
    "#             self.cache[index] = (downsampled_tensor, F.softmax(self.labels[index]))\n",
    "            self.cache[index] = (downsampled_tensor, self.labels[index])\n",
    "#             print(self.labels[index])\n",
    "\n",
    "        return self.cache[index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Net(\n",
      "  (conv1): Conv2d(1, 32, kernel_size=(2, 2), stride=(1, 1))\n",
      "  (conv2): Conv2d(32, 64, kernel_size=(2, 2), stride=(1, 1))\n",
      "  (pool1): MaxPool2d(kernel_size=(2, 2), stride=(2, 2), padding=0, dilation=1, ceil_mode=False)\n",
      "  (drop1): Dropout(p=0.25, inplace=False)\n",
      "  (flat1): Flatten()\n",
      "  (dense1): Linear(in_features=188480, out_features=128, bias=True)\n",
      "  (drop2): Dropout(p=0.5, inplace=False)\n",
      "  (dense2): Linear(in_features=128, out_features=8, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 32, (2,2))\n",
    "        self.conv2 = nn.Conv2d(32, 64, (2,2))\n",
    "        self.pool1 = nn.MaxPool2d((2,2))\n",
    "        self.drop1 = nn.Dropout(p=0.25)\n",
    "        self.flat1 = nn.Flatten()\n",
    "        self.dense1 = nn.Linear(188480, 128)\n",
    "        self.drop2 = nn.Dropout(p=0.5)\n",
    "        self.dense2 = nn.Linear(128, 8)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.conv2(x)\n",
    "        x = F.relu(x)\n",
    "        \n",
    "        x = self.pool1(x)\n",
    "        x = self.drop1(x)\n",
    "        x = self.flat1(x)\n",
    "        x = self.dense1(x)\n",
    "        x = F.relu(x)\n",
    "        \n",
    "        x = self.drop2(x)\n",
    "        x = self.dense2(x)\n",
    "        x = F.relu(x)\n",
    "\n",
    "#         return F.log_softmax(x, dim = 1)\n",
    "        return x\n",
    "\n",
    "model = Net()\n",
    "model.to(device)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train set size: 180\n",
      "Test set size: 45\n"
     ]
    }
   ],
   "source": [
    "train_set = SongEmotionDataset(train_song, train_emotion)\n",
    "test_set = SongEmotionDataset(test_song, test_emotion)\n",
    "print(\"Train set size: \" + str(len(train_set)))\n",
    "print(\"Test set size: \" + str(len(test_set)))\n",
    "\n",
    "kwargs = {'num_workers': 1, 'pin_memory': True} if device == 'cuda' else {} #needed for using datasets on gpu\n",
    "train_loader = torch.utils.data.DataLoader(train_set, batch_size = 8, shuffle = True, **kwargs)\n",
    "test_loader = torch.utils.data.DataLoader(test_set, batch_size = 8, shuffle = True, **kwargs)\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr = 0.01, weight_decay = 0.0001)\n",
    "scheduler = optim.lr_scheduler.StepLR(optimizer, step_size = 20, gamma = 0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, epoch):\n",
    "    model.train()\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        optimizer.zero_grad()\n",
    "        data.unsqueeze_(1)\n",
    "#         data = data.requires_grad_() #set requires_grad to True for training\n",
    "        output = model(data)\n",
    "        print(output.shape)\n",
    "\n",
    "#         kl = nn.KLDivLoss()\n",
    "#         loss = kl(output, target)\n",
    "        loss = F.cross_entropy(output, target)\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "#         print(target)\n",
    "#         print(\"\\n\")\n",
    "        \n",
    "        if batch_idx % log_interval == 0: #print training stats\n",
    "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                epoch, batch_idx * len(data), len(train_loader.dataset),\n",
    "                100. * batch_idx / len(train_loader), loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(model, epoch):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    for data, target in test_loader:\n",
    "        data.unsqueeze_(1)\n",
    "        output = model(data)\n",
    "        print(target)\n",
    "        \n",
    "#         print(output)\n",
    "#         output = output.permute(1, 0, 2)\n",
    "        pred = output.max(1)[1] # get the index of the max log-probability \n",
    "        print(pred)\n",
    "        correct += pred.eq(target).sum()\n",
    "        print(correct)\n",
    "#         correct += pred.eq(target.max(1)[1]).cpu().sum().item()\n",
    "    print('\\nTest set: Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
    "        correct, len(test_loader.dataset),\n",
    "        100. * correct / len(test_loader.dataset)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([3, 5, 5, 5, 7, 0, 3, 0])\n",
      "tensor([7, 7, 7, 7, 7, 7, 7, 7])\n",
      "tensor(1)\n",
      "tensor([3, 6, 6, 0, 7, 0, 5, 3])\n",
      "tensor([7, 7, 7, 7, 7, 7, 7, 7])\n",
      "tensor(2)\n",
      "tensor([2, 2, 5, 4, 6, 1, 2, 1])\n",
      "tensor([7, 7, 7, 7, 7, 7, 7, 7])\n",
      "tensor(2)\n",
      "tensor([4, 2, 4, 0, 0, 7, 4, 6])\n",
      "tensor([7, 7, 7, 7, 7, 7, 7, 7])\n",
      "tensor(3)\n",
      "tensor([4, 2, 0, 5, 1, 3, 5, 4])\n",
      "tensor([7, 7, 7, 7, 7, 7, 7, 7])\n",
      "tensor(3)\n",
      "tensor([6, 0, 2, 2, 4])\n",
      "tensor([7, 7, 7, 7, 7])\n",
      "tensor(3)\n",
      "\n",
      "Test set: Accuracy: 3/45 (7%)\n",
      "\n",
      "tensor([3, 5, 1, 0, 1, 6, 3, 4])\n",
      "tensor([7, 7, 7, 7, 7, 7, 7, 7])\n",
      "tensor(0)\n",
      "tensor([5, 3, 4, 2, 6, 4, 0, 6])\n",
      "tensor([7, 7, 7, 7, 7, 7, 7, 7])\n",
      "tensor(0)\n",
      "tensor([0, 2, 0, 3, 5, 7, 3, 5])\n",
      "tensor([7, 7, 7, 7, 7, 7, 7, 7])\n",
      "tensor(1)\n",
      "tensor([2, 4, 0, 1, 7, 6, 5, 4])\n",
      "tensor([7, 7, 7, 7, 7, 7, 7, 7])\n",
      "tensor(2)\n",
      "tensor([4, 0, 2, 5, 2, 2, 0, 5])\n",
      "tensor([7, 7, 7, 7, 7, 7, 7, 7])\n",
      "tensor(2)\n",
      "tensor([2, 0, 7, 6, 4])\n",
      "tensor([7, 7, 7, 7, 7])\n",
      "tensor(3)\n",
      "\n",
      "Test set: Accuracy: 3/45 (7%)\n",
      "\n",
      "tensor([0, 3, 2, 3, 0, 6, 1, 4])\n",
      "tensor([7, 7, 7, 7, 7, 7, 7, 7])\n",
      "tensor(0)\n",
      "tensor([1, 3, 0, 5, 6, 0, 4, 5])\n",
      "tensor([7, 7, 7, 7, 7, 7, 7, 7])\n",
      "tensor(0)\n",
      "tensor([2, 4, 2, 2, 4, 4, 4, 2])\n",
      "tensor([7, 7, 7, 7, 7, 7, 7, 7])\n",
      "tensor(0)\n",
      "tensor([4, 5, 5, 1, 3, 7, 5, 0])\n",
      "tensor([7, 7, 7, 7, 7, 7, 7, 7])\n",
      "tensor(1)\n",
      "tensor([7, 0, 6, 0, 7, 0, 6, 2])\n",
      "tensor([7, 7, 7, 7, 7, 7, 7, 7])\n",
      "tensor(3)\n",
      "tensor([2, 5, 3, 6, 5])\n",
      "tensor([7, 7, 7, 7, 7])\n",
      "tensor(3)\n",
      "\n",
      "Test set: Accuracy: 3/45 (7%)\n",
      "\n",
      "tensor([4, 6, 3, 1, 0, 5, 5, 5])\n",
      "tensor([7, 7, 7, 7, 7, 7, 7, 7])\n",
      "tensor(0)\n",
      "tensor([7, 4, 2, 6, 2, 4, 3, 5])\n",
      "tensor([7, 7, 7, 7, 7, 7, 7, 7])\n",
      "tensor(1)\n",
      "tensor([3, 4, 2, 5, 3, 1, 3, 0])\n",
      "tensor([7, 7, 7, 7, 7, 7, 7, 7])\n",
      "tensor(1)\n",
      "tensor([4, 5, 7, 0, 0, 7, 4, 0])\n",
      "tensor([7, 7, 7, 7, 7, 7, 7, 7])\n",
      "tensor(3)\n",
      "tensor([2, 1, 0, 5, 6, 2, 2, 0])\n",
      "tensor([7, 7, 7, 7, 7, 7, 7, 7])\n",
      "tensor(3)\n",
      "tensor([6, 4, 0, 6, 2])\n",
      "tensor([7, 7, 7, 7, 7])\n",
      "tensor(3)\n",
      "\n",
      "Test set: Accuracy: 3/45 (7%)\n",
      "\n",
      "tensor([3, 1, 6, 4, 0, 2, 0, 0])\n",
      "tensor([7, 7, 7, 7, 7, 7, 7, 7])\n",
      "tensor(0)\n",
      "tensor([7, 6, 0, 7, 3, 5, 4, 2])\n",
      "tensor([7, 7, 7, 7, 7, 7, 7, 7])\n",
      "tensor(2)\n",
      "tensor([3, 5, 0, 4, 2, 2, 6, 7])\n",
      "tensor([7, 7, 7, 7, 7, 7, 7, 7])\n",
      "tensor(3)\n",
      "tensor([6, 3, 6, 5, 4, 3, 5, 5])\n",
      "tensor([7, 7, 7, 7, 7, 7, 7, 7])\n",
      "tensor(3)\n",
      "tensor([1, 0, 2, 2, 4, 5, 0, 5])\n",
      "tensor([7, 7, 7, 7, 7, 7, 7, 7])\n",
      "tensor(3)\n",
      "tensor([0, 4, 1, 4, 2])\n",
      "tensor([7, 7, 7, 7, 7])\n",
      "tensor(3)\n",
      "\n",
      "Test set: Accuracy: 3/45 (7%)\n",
      "\n",
      "tensor([1, 6, 0, 2, 2, 5, 2, 0])\n",
      "tensor([7, 7, 7, 7, 7, 7, 7, 7])\n",
      "tensor(0)\n",
      "tensor([0, 2, 4, 0, 2, 7, 3, 3])\n",
      "tensor([7, 7, 7, 7, 7, 7, 7, 7])\n",
      "tensor(1)\n",
      "tensor([2, 4, 5, 2, 5, 6, 6, 1])\n",
      "tensor([7, 7, 7, 7, 7, 7, 7, 7])\n",
      "tensor(1)\n",
      "tensor([6, 3, 0, 6, 7, 3, 4, 0])\n",
      "tensor([7, 7, 7, 7, 7, 7, 7, 7])\n",
      "tensor(2)\n",
      "tensor([4, 1, 0, 3, 4, 5, 5, 4])\n",
      "tensor([7, 7, 7, 7, 7, 7, 7, 7])\n",
      "tensor(2)\n",
      "tensor([5, 4, 0, 5, 7])\n",
      "tensor([7, 7, 7, 7, 7])\n",
      "tensor(3)\n",
      "\n",
      "Test set: Accuracy: 3/45 (7%)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i in range(6):\n",
    "    test(model, i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training epoch 1\n",
      "torch.Size([8, 8])\n",
      "Train Epoch: 1 [0/180 (0%)]\tLoss: 3.095165\n",
      "torch.Size([8, 8])\n",
      "torch.Size([8, 8])\n",
      "torch.Size([8, 8])\n",
      "torch.Size([8, 8])\n",
      "torch.Size([8, 8])\n",
      "Train Epoch: 1 [40/180 (22%)]\tLoss: 2.130520\n",
      "torch.Size([8, 8])\n",
      "torch.Size([8, 8])\n",
      "torch.Size([8, 8])\n",
      "torch.Size([8, 8])\n",
      "torch.Size([8, 8])\n",
      "Train Epoch: 1 [80/180 (43%)]\tLoss: 2.079441\n",
      "torch.Size([8, 8])\n",
      "torch.Size([8, 8])\n",
      "torch.Size([8, 8])\n",
      "torch.Size([8, 8])\n",
      "torch.Size([8, 8])\n",
      "Train Epoch: 1 [120/180 (65%)]\tLoss: 2.079441\n",
      "torch.Size([8, 8])\n",
      "torch.Size([8, 8])\n",
      "torch.Size([8, 8])\n",
      "torch.Size([8, 8])\n",
      "torch.Size([8, 8])\n",
      "Train Epoch: 1 [160/180 (87%)]\tLoss: 2.079441\n",
      "torch.Size([8, 8])\n",
      "torch.Size([4, 8])\n",
      "torch.Size([8, 8])\n",
      "pred: \n",
      "tensor([7, 7, 7, 7, 7, 7, 7, 7])\n",
      "torch.Size([8])\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "Dimension out of range (expected to be in range of [-1, 0], but got 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-78-c5476e3bde80>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0mscheduler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m     \u001b[0mtest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-77-45a87133c66c>\u001b[0m in \u001b[0;36mtest\u001b[0;34m(model, epoch)\u001b[0m\n\u001b[1;32m     13\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpred\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m         \u001b[0mcorrect\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mpred\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meq\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtarget\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m     print('\\nTest set: Accuracy: {}/{} ({:.0f}%)\\n'.format(\n\u001b[1;32m     17\u001b[0m         \u001b[0mcorrect\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_loader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: Dimension out of range (expected to be in range of [-1, 0], but got 1)"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "\n",
    "log_interval = 5\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "for epoch in range(1, 100):\n",
    "    print(\"training epoch \" + str(epoch))\n",
    "    if epoch == 31:\n",
    "        print(\"First round of training complete. Setting learn rate to 0.001.\")\n",
    "#     scheduler.step()\n",
    "    train(model, epoch)\n",
    "    scheduler.step()\n",
    "    test(model, epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print model's state_dict\n",
    "print(\"Model's state_dict:\")\n",
    "for param_tensor in model.state_dict():\n",
    "    print(param_tensor, \"\\t\", model.state_dict()[param_tensor].size())\n",
    "# Print optimizer's state_dict\n",
    "print(\"Optimizer's state_dict:\")\n",
    "for var_name in optimizer.state_dict():\n",
    "    print(var_name, \"\\t\", optimizer.state_dict()[var_name])\n",
    "torch.save(model.state_dict(), 'dataset_model_soundemotion.pt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NOTES\n",
    "\n",
    "below is the mfccs notes / random code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/czf/memories/venv/lib/python3.6/site-packages/librosa/core/audio.py:161: UserWarning: PySoundFile failed. Trying audioread instead.\n",
      "  warnings.warn('PySoundFile failed. Trying audioread instead.')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(40, 2586) (1323648,)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[-5.30341797e+02, -4.07741577e+02, -3.27536621e+02, ...,\n",
       "        -2.39811523e+02, -1.96744080e+02, -1.44711777e+02],\n",
       "       [ 5.81265569e-01,  1.03006027e+02,  1.29354553e+02, ...,\n",
       "         1.48707626e+02,  1.45873001e+02,  1.28202530e+02],\n",
       "       [ 4.58764762e-01,  7.53921986e+00, -1.18814125e+01, ...,\n",
       "        -2.51551704e+01, -1.92207527e+01, -1.79366188e+01],\n",
       "       ...,\n",
       "       [ 3.11299562e-01, -1.29907084e+00,  1.18818974e+00, ...,\n",
       "        -6.58579540e+00, -3.34302998e+00, -4.75482178e+00],\n",
       "       [ 2.23848164e-01, -3.19489312e+00, -2.78556681e+00, ...,\n",
       "        -1.36089420e+01, -6.40699673e+00, -5.27228928e+00],\n",
       "       [ 8.67742151e-02,  1.31472754e+00, -1.41885233e+00, ...,\n",
       "         3.34440261e-01,  1.14392626e+00, -3.62402201e-02]], dtype=float32)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "audio, sample_rate = librosa.load(\"SongEmotionDataset/1.mp3\", res_type='kaiser_fast')\n",
    "# [print(x) for x in audio]\n",
    "\n",
    "#convert audio into 2d array\n",
    "mfccs = librosa.feature.mfcc(y=audio, sr=sample_rate, n_mfcc=40)\n",
    "# mfccsscaled = np.mean(mfccs.T,axis=0)\n",
    "print(mfccs.shape, audio.shape)\n",
    "mfccs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# audio_tensor = torch.tensor(audio)\n",
    "# audio_tensor\n",
    "# audio_tensor.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for sound_file in data_path.iterdir():\n",
    "#     if \".mp3\" in str(sound_file):\n",
    "#         print(sound_file)\n",
    "#         audio, sample_rate = librosa.load(str(sound_file), res_type='kaiser_fast')\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_percentage = 0.8\n",
    "\n",
    "# min_train = min_total*train_percentage\n",
    "# min_test = min_total - min_train\n",
    "\n",
    "# train_totals = torch.zeros(len(emotions))\n",
    "\n",
    "# while \n",
    "\n",
    "\n",
    "# for i in range(1, 401):\n",
    "#     count_total = sheet.cell_value(i, 7)\n",
    "    \n",
    "#     emotions_counter = [0 for e in emotions]\n",
    "#     if i % 5 == 0:\n",
    "#         test_song.append(dataset.joinpath(\"{}.mp3\".format(i)))\n",
    "#         emotion_arr = []\n",
    "#         for j in range(5):\n",
    "#             emotion_arr.append(sheet.cell_value(i, 2 + j))\n",
    "#         test_emotion.append(torch.tensor(emotion_arr, device=device).float())\n",
    "        \n",
    "#     emotions_counter = [0 for e in emotions]\n",
    "#     else:\n",
    "#         train_song.append(dataset.joinpath(\"{}.mp3\".format(i)))\n",
    "#         emotion_arr = []\n",
    "#         for j in range(5):\n",
    "#             emotion_arr.append(sheet.cell_value(i, 2 + j))\n",
    "#         train_emotion.append(torch.tensor(emotion_arr, device=device))\n",
    "\n",
    "# print(len(train_song), len(test_song))\n",
    "# print(len(train_emotion), len(test_emotion))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def train(model, epoch):\n",
    "#     model.train()\n",
    "#     for batch_idx, (data, target) in enumerate(train_loader):\n",
    "#         print(data)\n",
    "#         optimizer.zero_grad()\n",
    "#         data.unsqueeze_(1)\n",
    "#         data = data.requires_grad_() #set requires_grad to True for training\n",
    "#         output = model(data)\n",
    "# #         output = output.view(-1, len(emotions))\n",
    "# #         print(output.shape, target.shape)\n",
    "# #         print(output, target)\n",
    "# #         loss = F.kl_div(output, target)\n",
    "#         kl = nn.KLDivLoss()\n",
    "#         loss = kl(output, target)\n",
    "# #         loss = F.cross_entropy(output, target)\n",
    "# #         loss = nn.CrossEntropyLoss(output, target)\n",
    "# #         loss = F.nll_loss(output, target)\n",
    "#         loss.backward()\n",
    "#         print(loss)\n",
    "#         optimizer.step()\n",
    "#         print(output)\n",
    "#         print(target)\n",
    "#         print(\"\\n\")\n",
    "# #         scheduler.step()\n",
    "#         if batch_idx % log_interval == 0: #print training stats\n",
    "#             print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "#                 epoch, batch_idx * len(data), len(train_loader.dataset),\n",
    "#                 100. * batch_idx / len(train_loader), loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 11.4343, 157.9053,  55.5535, 120.1634,  54.9434], device='cuda:0')\n",
      "tensor([3.1017, 9.4848, 9.0917, 8.7741, 8.5477], device='cuda:0')\n",
      "[1, 9, 11, 10, 8]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/armaan.goel/memories/myenv/lib/python3.6/site-packages/ipykernel_launcher.py:22: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "/home/armaan.goel/memories/myenv/lib/python3.6/site-packages/ipykernel_launcher.py:34: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    }
   ],
   "source": [
    "# train_percentage = 0.8\n",
    "# allowed_exceedance = 0\n",
    "\n",
    "# train_song = []\n",
    "# test_song = []\n",
    "# train_emotion = []\n",
    "# test_emotion = []\n",
    "\n",
    "# row_indexes = np.arange(1,401)\n",
    "# np.random.shuffle(row_indexes)\n",
    "\n",
    "# # train_indexes = [row_indexes[i] for i in range(len(row_indexes)) if i < len(row_indexes)*train_percentage]\n",
    "# # test_indexes = [row_indexes[i] for i in range(len(row_indexes)) if i >= len(row_indexes)*train_percentage]\n",
    "\n",
    "# def get_data(indexes):\n",
    "#     song = []\n",
    "#     emotion = []\n",
    "    \n",
    "#     totals = torch.zeros(len(emotions), device=device).float()\n",
    "#     for x in indexes:    \n",
    "#         row = torch.tensor([sheet.cell_value(x, 2 + j) for j in range(5)], device=device).float()\n",
    "#         totals += F.softmax(row)\n",
    "\n",
    "#     min_total = torch.min(totals)\n",
    "#     print(totals)\n",
    "    \n",
    "#     totals = torch.zeros(len(emotions), device=device).float()\n",
    "#     for x in indexes:\n",
    "#         row = torch.tensor([sheet.cell_value(x, 2 + j) for j in range(5)], device=device).float()\n",
    "        \n",
    "#         if torch.max(totals + row) < min_total*(1 + allowed_exceedance):\n",
    "#             song.append(dataset.joinpath(\"{}.mp3\".format(x)))\n",
    "#             emotion.append(row)\n",
    "#             totals += F.softmax(row)\n",
    "            \n",
    "#     print(totals)\n",
    "#     return song, emotion\n",
    "    \n",
    "# song, emotion = get_data(row_indexes)\n",
    "# # test_song, test_emotion = get_data(test_indexes)\n",
    "\n",
    "# train_song = [song[i] for i in range(len(song)) if i < len(song)*train_percentage]\n",
    "# test_song = [song[i] for i in range(len(song)) if i >= len(song)*train_percentage]\n",
    "\n",
    "# train_emotion = [emotion[i] for i in range(len(emotion)) if i < len(emotion)*train_percentage]\n",
    "# test_emotion = [emotion[i] for i in range(len(emotion)) if i >= len(emotion)*train_percentage]\n",
    "\n",
    "# num_maxes = [0 for _ in emotions]\n",
    "\n",
    "# for row in emotion:\n",
    "#     i = torch.argmax(row)\n",
    "#     num_maxes[i] += 1\n",
    "    \n",
    "# print(num_maxes)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
