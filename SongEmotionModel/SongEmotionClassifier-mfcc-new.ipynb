{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xlrd\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from numba import decorators\n",
    "import librosa\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import Dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)\n",
    "# device = \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read in the data file\n",
    "# Give the location of the file \n",
    "\n",
    "df = pd.read_excel(r'data/data.xlsx', sheet_name='normalized')\n",
    "# print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "## LOADING IN DATASETS\n",
    "\n",
    "dataset = Path.cwd().joinpath(\"SongEmotionDataset\")\n",
    "datasheet = Path.cwd().joinpath(\"data\") # for csua\n",
    "\n",
    "#emotion labels\n",
    "label_loc = datasheet.joinpath(\"data.xlsx\")\n",
    "wb = xlrd.open_workbook(label_loc) \n",
    "sheet = wb.sheet_by_index(3)\n",
    "\n",
    "#emotion arr\n",
    "# emotions = [\"amazement\", \"calmness\", \"power\", \"joyful activation\", \"sadness\"]\n",
    "emotions = [\"solemnity\", \"tenderness\", \"nostalgia\", \"calmness\", \"power\", \"joyful activation\", \"tension\", \"sadness\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[202 189 119 165 143   6  98 128 192   2  63 159 213  84 148 195  86  53\n",
      " 124  15 108  45  76  26 162 214 198 175 117 155 139  99  75  39 130  14\n",
      " 177 125  36  62  34  20  60 111   8 173  73  68  44  81 205 106 200  47\n",
      " 134  67 118 174 212 104 122  12 101   1  17  61  58  38 144 207  57 147\n",
      "  52  55  48 152 204   4 133 201 225 167 218  82 115  65 194 135 166 188\n",
      "  42 131   9  78  23 199 116  30 211  80 113 149 140  79  51  56 184 105\n",
      " 141  89  46 181 224  32 209 126 114  11  33 180  95 103 142 182 178  96\n",
      " 170 107  71 222 223 210 138  22 217  66 154 123  87 110  13 220  72 153\n",
      " 171  97 183 156 151 185 169 157 146  92 161 100 136  19 219 102 206  31\n",
      " 109  49 187 221 163 145 193  37  35 203  18  28 160  83 129  90  50 158\n",
      "  40 176 196 150 132 186  27   5  69  41 215 127  91 137  88 120  54  93\n",
      "  10  74 168  85 190  29  77 191 112   7  94 172  25 216  16 179 164  24\n",
      " 197 208  70  64  43  21   3 121  59]\n"
     ]
    }
   ],
   "source": [
    "train_percentage = 0.8\n",
    "\n",
    "train_song = []\n",
    "test_song = []\n",
    "train_emotion = []\n",
    "test_emotion = []\n",
    "\n",
    "row_indexes = np.arange(1,226)\n",
    "np.random.shuffle(row_indexes)\n",
    "\n",
    "def get_data(indexes):\n",
    "    print(indexes)\n",
    "    song = []\n",
    "    emotion = []\n",
    "    \n",
    "    for x in indexes:\n",
    "        row = int(sheet.cell_value(x, 13) - 1)\n",
    "        trackid = int(sheet.cell_value(x, 0))\n",
    "        song.append(dataset.joinpath(\"{}.mp3\".format(trackid)))\n",
    "        emotion.append(row)\n",
    "            \n",
    "    return song, emotion\n",
    "    \n",
    "song, emotion = get_data(row_indexes)\n",
    "# print(song)\n",
    "# print(emotion)\n",
    "# test_song, test_emotion = get_data(test_indexes)\n",
    "\n",
    "train_song = [song[i] for i in range(len(song)) if i < len(song)*train_percentage]\n",
    "test_song = [song[i] for i in range(len(song)) if i >= len(song)*train_percentage]\n",
    "\n",
    "train_emotion = [emotion[i] for i in range(len(emotion)) if i < len(emotion)*train_percentage]\n",
    "test_emotion = [emotion[i] for i in range(len(emotion)) if i >= len(emotion)*train_percentage]\n",
    "\n",
    "# num_maxes = [0 for _ in emotions]\n",
    "\n",
    "# for row in emotion:\n",
    "#     i = torch.argmax(row)\n",
    "#     print(i)\n",
    "#     num_maxes[i] += 1\n",
    "    \n",
    "# print(num_maxes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_emotion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SongEmotionDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Song Emotion Dataset. Uses librosa to process mp3 files.\n",
    "    Takes first 20 seconds, and samples every 10 to get processed audio tensor.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, mp3, labels, transform=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            mp3: list of paths to mp3 files\n",
    "            labels: list of labels\n",
    "        \"\"\"\n",
    "        self.labels = labels\n",
    "        self.mp3 = mp3\n",
    "        self.cache = {}\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        if index not in self.cache.keys():\n",
    "            data, rate = librosa.load(self.mp3[index], sr=16000, duration=10)\n",
    "            mfccs = librosa.feature.mfcc(y=data, sr=rate, n_mfcc=40)\n",
    "            assert rate == 16000\n",
    "            sample_tensor = torch.tensor(mfccs, device=device)\n",
    "            downsampled_tensor = sample_tensor #[::10]\n",
    "#             print(mfccs.shape, data.shape)\n",
    "\n",
    "#             self.cache[index] = (downsampled_tensor, F.softmax(self.labels[index]))\n",
    "            self.cache[index] = (downsampled_tensor, self.labels[index])\n",
    "#             print(self.labels[index])\n",
    "\n",
    "        return self.cache[index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "CUDA error: device-side assert triggered",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-64-1b203f96551e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mNet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 33\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     34\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/memories/venv/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36mto\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    441\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_floating_point\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_blocking\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    442\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 443\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconvert\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    444\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    445\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_backward_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/memories/venv/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    201\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    202\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchildren\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 203\u001b[0;31m             \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    204\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    205\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mcompute_should_use_set_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtensor_applied\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/memories/venv/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    223\u001b[0m                 \u001b[0;31m# `with torch.no_grad():`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    224\u001b[0m                 \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 225\u001b[0;31m                     \u001b[0mparam_applied\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparam\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    226\u001b[0m                 \u001b[0mshould_use_set_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompute_should_use_set_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparam\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparam_applied\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    227\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mshould_use_set_data\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/memories/venv/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36mconvert\u001b[0;34m(t)\u001b[0m\n\u001b[1;32m    439\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mconvert_to_format\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m4\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    440\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_floating_point\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_blocking\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmemory_format\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconvert_to_format\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 441\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_floating_point\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_blocking\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    442\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    443\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconvert\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: CUDA error: device-side assert triggered"
     ]
    }
   ],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 32, (2,2))\n",
    "        self.conv2 = nn.Conv2d(32, 64, (2,2))\n",
    "        self.pool1 = nn.MaxPool2d((2,2))\n",
    "        self.drop1 = nn.Dropout(p=0.25)\n",
    "        self.flat1 = nn.Flatten()\n",
    "        self.dense1 = nn.Linear(188480, 128)\n",
    "        self.drop2 = nn.Dropout(p=0.5)\n",
    "        self.dense2 = nn.Linear(128, 8)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.conv2(x)\n",
    "        x = F.relu(x)\n",
    "        \n",
    "        x = self.pool1(x)\n",
    "        x = self.drop1(x)\n",
    "        x = self.flat1(x)\n",
    "        x = self.dense1(x)\n",
    "        x = F.relu(x)\n",
    "        \n",
    "        x = self.drop2(x)\n",
    "        x = self.dense2(x)\n",
    "        x = F.relu(x)\n",
    "\n",
    "#         return F.log_softmax(x, dim = 1)\n",
    "        return x\n",
    "\n",
    "model = Net()\n",
    "model.to(device)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train set size: 180\n",
      "Test set size: 45\n"
     ]
    }
   ],
   "source": [
    "train_set = SongEmotionDataset(train_song, train_emotion)\n",
    "test_set = SongEmotionDataset(test_song, test_emotion)\n",
    "print(\"Train set size: \" + str(len(train_set)))\n",
    "print(\"Test set size: \" + str(len(test_set)))\n",
    "\n",
    "kwargs = {'num_workers': 1, 'pin_memory': True} if device == 'cuda' else {} #needed for using datasets on gpu\n",
    "train_loader = torch.utils.data.DataLoader(train_set, batch_size = 8, shuffle = True, **kwargs)\n",
    "test_loader = torch.utils.data.DataLoader(test_set, batch_size = 8, shuffle = True, **kwargs)\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr = 0.01, weight_decay = 0.0001)\n",
    "scheduler = optim.lr_scheduler.StepLR(optimizer, step_size = 20, gamma = 0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, epoch):\n",
    "    model.train()\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        optimizer.zero_grad()\n",
    "        data.unsqueeze_(1)\n",
    "#         data = data.requires_grad_() #set requires_grad to True for training\n",
    "        output = model(data)\n",
    "        print(output.shape)\n",
    "\n",
    "#         kl = nn.KLDivLoss()\n",
    "#         loss = kl(output, target)\n",
    "        loss = F.cross_entropy(output, target)\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "#         print(target)\n",
    "#         print(\"\\n\")\n",
    "        \n",
    "        if batch_idx % log_interval == 0: #print training stats\n",
    "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                epoch, batch_idx * len(data), len(train_loader.dataset),\n",
    "                100. * batch_idx / len(train_loader), loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(model, epoch):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    for data, target in test_loader:\n",
    "        data.unsqueeze_(1)\n",
    "        output = model(data)\n",
    "        print(output.shape)\n",
    "        \n",
    "#         print(output)\n",
    "#         output = output.permute(1, 0, 2)\n",
    "        pred = output.max(1)[1] # get the index of the max log-probability \n",
    "        correct += pred.eq(target.max(1)[1]).cpu().sum().item()\n",
    "    print('\\nTest set: Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
    "        correct, len(test_loader.dataset),\n",
    "        100. * correct / len(test_loader.dataset)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training epoch 1\n",
      "torch.Size([8, 8])\n",
      "Train Epoch: 1 [0/180 (0%)]\tLoss: 3.449864\n",
      "torch.Size([8, 8])\n",
      "torch.Size([8, 8])\n",
      "torch.Size([8, 8])\n",
      "torch.Size([8, 8])\n",
      "torch.Size([8, 8])\n",
      "Train Epoch: 1 [40/180 (22%)]\tLoss: 2.080843\n",
      "torch.Size([8, 8])\n",
      "torch.Size([8, 8])\n",
      "torch.Size([8, 8])\n",
      "torch.Size([8, 8])\n",
      "torch.Size([8, 8])\n",
      "Train Epoch: 1 [80/180 (43%)]\tLoss: 2.079441\n",
      "torch.Size([8, 8])\n",
      "torch.Size([8, 8])\n",
      "torch.Size([8, 8])\n",
      "torch.Size([8, 8])\n",
      "torch.Size([8, 8])\n",
      "Train Epoch: 1 [120/180 (65%)]\tLoss: 2.079441\n",
      "torch.Size([8, 8])\n",
      "torch.Size([8, 8])\n",
      "torch.Size([8, 8])\n",
      "torch.Size([8, 8])\n",
      "torch.Size([8, 8])\n",
      "Train Epoch: 1 [160/180 (87%)]\tLoss: 2.079441\n",
      "torch.Size([8, 8])\n",
      "torch.Size([4, 8])\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "Dimension out of range (expected to be in range of [-1, 0], but got 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-57-c5476e3bde80>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0mscheduler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m     \u001b[0mtest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-56-383a8c1ee7ce>\u001b[0m in \u001b[0;36mtest\u001b[0;34m(model, epoch)\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;31m#         output = output.permute(1, 0, 2)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m         \u001b[0mpred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;31m# get the index of the max log-probability\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m         \u001b[0mcorrect\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mpred\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meq\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtarget\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m     print('\\nTest set: Accuracy: {}/{} ({:.0f}%)\\n'.format(\n\u001b[1;32m     12\u001b[0m         \u001b[0mcorrect\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_loader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: Dimension out of range (expected to be in range of [-1, 0], but got 1)"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "\n",
    "log_interval = 5\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "for epoch in range(1, 100):\n",
    "    print(\"training epoch \" + str(epoch))\n",
    "    if epoch == 31:\n",
    "        print(\"First round of training complete. Setting learn rate to 0.001.\")\n",
    "#     scheduler.step()\n",
    "    train(model, epoch)\n",
    "    scheduler.step()\n",
    "    test(model, epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print model's state_dict\n",
    "print(\"Model's state_dict:\")\n",
    "for param_tensor in model.state_dict():\n",
    "    print(param_tensor, \"\\t\", model.state_dict()[param_tensor].size())\n",
    "# Print optimizer's state_dict\n",
    "print(\"Optimizer's state_dict:\")\n",
    "for var_name in optimizer.state_dict():\n",
    "    print(var_name, \"\\t\", optimizer.state_dict()[var_name])\n",
    "torch.save(model.state_dict(), 'dataset_model_soundemotion.pt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NOTES\n",
    "\n",
    "below is the mfccs notes / random code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/czf/memories/venv/lib/python3.6/site-packages/librosa/core/audio.py:161: UserWarning: PySoundFile failed. Trying audioread instead.\n",
      "  warnings.warn('PySoundFile failed. Trying audioread instead.')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(40, 2586) (1323648,)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[-5.30341797e+02, -4.07741577e+02, -3.27536621e+02, ...,\n",
       "        -2.39811523e+02, -1.96744080e+02, -1.44711777e+02],\n",
       "       [ 5.81265569e-01,  1.03006027e+02,  1.29354553e+02, ...,\n",
       "         1.48707626e+02,  1.45873001e+02,  1.28202530e+02],\n",
       "       [ 4.58764762e-01,  7.53921986e+00, -1.18814125e+01, ...,\n",
       "        -2.51551704e+01, -1.92207527e+01, -1.79366188e+01],\n",
       "       ...,\n",
       "       [ 3.11299562e-01, -1.29907084e+00,  1.18818974e+00, ...,\n",
       "        -6.58579540e+00, -3.34302998e+00, -4.75482178e+00],\n",
       "       [ 2.23848164e-01, -3.19489312e+00, -2.78556681e+00, ...,\n",
       "        -1.36089420e+01, -6.40699673e+00, -5.27228928e+00],\n",
       "       [ 8.67742151e-02,  1.31472754e+00, -1.41885233e+00, ...,\n",
       "         3.34440261e-01,  1.14392626e+00, -3.62402201e-02]], dtype=float32)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "audio, sample_rate = librosa.load(\"SongEmotionDataset/1.mp3\", res_type='kaiser_fast')\n",
    "# [print(x) for x in audio]\n",
    "\n",
    "#convert audio into 2d array\n",
    "mfccs = librosa.feature.mfcc(y=audio, sr=sample_rate, n_mfcc=40)\n",
    "# mfccsscaled = np.mean(mfccs.T,axis=0)\n",
    "print(mfccs.shape, audio.shape)\n",
    "mfccs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# audio_tensor = torch.tensor(audio)\n",
    "# audio_tensor\n",
    "# audio_tensor.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for sound_file in data_path.iterdir():\n",
    "#     if \".mp3\" in str(sound_file):\n",
    "#         print(sound_file)\n",
    "#         audio, sample_rate = librosa.load(str(sound_file), res_type='kaiser_fast')\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_percentage = 0.8\n",
    "\n",
    "# min_train = min_total*train_percentage\n",
    "# min_test = min_total - min_train\n",
    "\n",
    "# train_totals = torch.zeros(len(emotions))\n",
    "\n",
    "# while \n",
    "\n",
    "\n",
    "# for i in range(1, 401):\n",
    "#     count_total = sheet.cell_value(i, 7)\n",
    "    \n",
    "#     emotions_counter = [0 for e in emotions]\n",
    "#     if i % 5 == 0:\n",
    "#         test_song.append(dataset.joinpath(\"{}.mp3\".format(i)))\n",
    "#         emotion_arr = []\n",
    "#         for j in range(5):\n",
    "#             emotion_arr.append(sheet.cell_value(i, 2 + j))\n",
    "#         test_emotion.append(torch.tensor(emotion_arr, device=device).float())\n",
    "        \n",
    "#     emotions_counter = [0 for e in emotions]\n",
    "#     else:\n",
    "#         train_song.append(dataset.joinpath(\"{}.mp3\".format(i)))\n",
    "#         emotion_arr = []\n",
    "#         for j in range(5):\n",
    "#             emotion_arr.append(sheet.cell_value(i, 2 + j))\n",
    "#         train_emotion.append(torch.tensor(emotion_arr, device=device))\n",
    "\n",
    "# print(len(train_song), len(test_song))\n",
    "# print(len(train_emotion), len(test_emotion))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def train(model, epoch):\n",
    "#     model.train()\n",
    "#     for batch_idx, (data, target) in enumerate(train_loader):\n",
    "#         print(data)\n",
    "#         optimizer.zero_grad()\n",
    "#         data.unsqueeze_(1)\n",
    "#         data = data.requires_grad_() #set requires_grad to True for training\n",
    "#         output = model(data)\n",
    "# #         output = output.view(-1, len(emotions))\n",
    "# #         print(output.shape, target.shape)\n",
    "# #         print(output, target)\n",
    "# #         loss = F.kl_div(output, target)\n",
    "#         kl = nn.KLDivLoss()\n",
    "#         loss = kl(output, target)\n",
    "# #         loss = F.cross_entropy(output, target)\n",
    "# #         loss = nn.CrossEntropyLoss(output, target)\n",
    "# #         loss = F.nll_loss(output, target)\n",
    "#         loss.backward()\n",
    "#         print(loss)\n",
    "#         optimizer.step()\n",
    "#         print(output)\n",
    "#         print(target)\n",
    "#         print(\"\\n\")\n",
    "# #         scheduler.step()\n",
    "#         if batch_idx % log_interval == 0: #print training stats\n",
    "#             print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "#                 epoch, batch_idx * len(data), len(train_loader.dataset),\n",
    "#                 100. * batch_idx / len(train_loader), loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 11.4343, 157.9053,  55.5535, 120.1634,  54.9434], device='cuda:0')\n",
      "tensor([3.1017, 9.4848, 9.0917, 8.7741, 8.5477], device='cuda:0')\n",
      "[1, 9, 11, 10, 8]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/armaan.goel/memories/myenv/lib/python3.6/site-packages/ipykernel_launcher.py:22: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "/home/armaan.goel/memories/myenv/lib/python3.6/site-packages/ipykernel_launcher.py:34: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    }
   ],
   "source": [
    "# train_percentage = 0.8\n",
    "# allowed_exceedance = 0\n",
    "\n",
    "# train_song = []\n",
    "# test_song = []\n",
    "# train_emotion = []\n",
    "# test_emotion = []\n",
    "\n",
    "# row_indexes = np.arange(1,401)\n",
    "# np.random.shuffle(row_indexes)\n",
    "\n",
    "# # train_indexes = [row_indexes[i] for i in range(len(row_indexes)) if i < len(row_indexes)*train_percentage]\n",
    "# # test_indexes = [row_indexes[i] for i in range(len(row_indexes)) if i >= len(row_indexes)*train_percentage]\n",
    "\n",
    "# def get_data(indexes):\n",
    "#     song = []\n",
    "#     emotion = []\n",
    "    \n",
    "#     totals = torch.zeros(len(emotions), device=device).float()\n",
    "#     for x in indexes:    \n",
    "#         row = torch.tensor([sheet.cell_value(x, 2 + j) for j in range(5)], device=device).float()\n",
    "#         totals += F.softmax(row)\n",
    "\n",
    "#     min_total = torch.min(totals)\n",
    "#     print(totals)\n",
    "    \n",
    "#     totals = torch.zeros(len(emotions), device=device).float()\n",
    "#     for x in indexes:\n",
    "#         row = torch.tensor([sheet.cell_value(x, 2 + j) for j in range(5)], device=device).float()\n",
    "        \n",
    "#         if torch.max(totals + row) < min_total*(1 + allowed_exceedance):\n",
    "#             song.append(dataset.joinpath(\"{}.mp3\".format(x)))\n",
    "#             emotion.append(row)\n",
    "#             totals += F.softmax(row)\n",
    "            \n",
    "#     print(totals)\n",
    "#     return song, emotion\n",
    "    \n",
    "# song, emotion = get_data(row_indexes)\n",
    "# # test_song, test_emotion = get_data(test_indexes)\n",
    "\n",
    "# train_song = [song[i] for i in range(len(song)) if i < len(song)*train_percentage]\n",
    "# test_song = [song[i] for i in range(len(song)) if i >= len(song)*train_percentage]\n",
    "\n",
    "# train_emotion = [emotion[i] for i in range(len(emotion)) if i < len(emotion)*train_percentage]\n",
    "# test_emotion = [emotion[i] for i in range(len(emotion)) if i >= len(emotion)*train_percentage]\n",
    "\n",
    "# num_maxes = [0 for _ in emotions]\n",
    "\n",
    "# for row in emotion:\n",
    "#     i = torch.argmax(row)\n",
    "#     num_maxes[i] += 1\n",
    "    \n",
    "# print(num_maxes)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
