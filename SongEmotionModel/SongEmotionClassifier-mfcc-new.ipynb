{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xlrd\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from numba import decorators\n",
    "import librosa\n",
    "\n",
    "import os\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "import torchaudio\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import Dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read in the data file\n",
    "# Give the location of the file \n",
    "\n",
    "df = pd.read_excel(r'data/data.xlsx', sheet_name='normalized')\n",
    "# print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "## LOADING IN DATASETS\n",
    "\n",
    "dataset = Path.cwd().joinpath(\"SongEmotionDataset\")\n",
    "datasheet = Path.cwd().joinpath(\"data\") # for csua\n",
    "imagepath = Path.cwd().joinpath(\"SongEmotionDatasetImages\")\n",
    "cmap = plt.get_cmap('inferno')\n",
    "\n",
    "#emotion labels\n",
    "label_loc = datasheet.joinpath(\"data.xlsx\")\n",
    "wb = xlrd.open_workbook(label_loc) \n",
    "sheet = wb.sheet_by_index(3)\n",
    "val_sheet = wb.sheet_by_index(1)\n",
    "\n",
    "#emotion arr\n",
    "# emotions = [\"amazement\", \"calmness\", \"power\", \"joyful activation\", \"sadness\"]\n",
    "emotions = [\"solemnity\", \"tenderness\", \"nostalgia\", \"calmness\", \"power\", \"joyful activation\", \"tension\", \"sadness\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jamesjiao/opt/anaconda3/lib/python3.7/site-packages/matplotlib/axes/_axes.py:7739: RuntimeWarning: divide by zero encountered in log10\n",
      "  Z = 10. * np.log10(spec)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def saveimages():\n",
    "    song = []\n",
    "    for filename in os.listdir(dataset):\n",
    "        if filename.endswith(\".mp3\"):\n",
    "            songname = dataset.joinpath(filename)\n",
    "#             y, sr = librosa.load(songname, mono=True, duration=5)\n",
    "            waveform, sr = torchaudio.load(songname)\n",
    "            mono_waveform = torch.mean(waveform, dim=0)\n",
    "            plt.specgram(mono_waveform, NFFT=2048, Fs=2, Fc=0, noverlap=128, cmap=cmap, sides='default', mode='default', scale='dB');\n",
    "            plt.axis('off');\n",
    "            plt.savefig(imagepath.joinpath(filename.replace(\".mp3\", \".png\")))\n",
    "            plt.clf()\n",
    "\n",
    "saveimages()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train_percentage = 0.8\n",
    "\n",
    "train_song = []\n",
    "test_song = []\n",
    "train_img = []\n",
    "test_img = []\n",
    "train_emotion = []\n",
    "test_emotion = []\n",
    "\n",
    "row_indexes = np.arange(1,226)\n",
    "np.random.shuffle(row_indexes)\n",
    "\n",
    "def get_data(indexes):\n",
    "    song = []\n",
    "    img = []\n",
    "    emotion = []\n",
    "    \n",
    "    for x in indexes:\n",
    "        row = int(sheet.cell_value(x, 13) - 1)\n",
    "        trackid = int(sheet.cell_value(x, 0))\n",
    "        song.append(dataset.joinpath(\"{}.mp3\".format(trackid)))\n",
    "        img.append(imagepath.joinpath(\"{}.png\".format(trackid)))\n",
    "        emotion.append(row)\n",
    "            \n",
    "    return song, img, emotion\n",
    "    \n",
    "song, img, emotion = get_data(row_indexes)\n",
    "\n",
    "train_song = [song[i] for i in range(len(song)) if i < len(song)*train_percentage]\n",
    "test_song = [song[i] for i in range(len(song)) if i >= len(song)*train_percentage]\n",
    "\n",
    "train_img = [img[i] for i in range(len(img)) if i < len(img)*train_percentage]\n",
    "test_img = [img[i] for i in range(len(img)) if i >= len(img)*train_percentage]\n",
    "\n",
    "train_emotion = [emotion[i] for i in range(len(emotion)) if i < len(emotion)*train_percentage]\n",
    "test_emotion = [emotion[i] for i in range(len(emotion)) if i >= len(emotion)*train_percentage]\n",
    "\n",
    "def get_val_data():\n",
    "    val_song = []\n",
    "    val_img = []\n",
    "    val_emotion = []\n",
    "    \n",
    "    for x in range(1, 401):\n",
    "        row = int(val_sheet.cell_value(x, 13) - 1)\n",
    "        trackid = int(val_sheet.cell_value(x, 0))\n",
    "        val_song.append(dataset.joinpath(\"{}.mp3\".format(trackid)))\n",
    "        val_img.append(imagepath.joinpath(\"{}.png\".format(trackid)))\n",
    "        val_emotion.append(row)\n",
    "            \n",
    "    return val_song, val_img, val_emotion\n",
    "\n",
    "val_song = []\n",
    "val_img = []\n",
    "val_emotion = []\n",
    "val_song, val_img, val_emotion = get_val_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_emotion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SongEmotionDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Song Emotion Dataset. Uses librosa to process mp3 files.\n",
    "    Takes first 20 seconds, and samples every 10 to get processed audio tensor.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, png, labels, transform=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            mp3: list of paths to mp3 files\n",
    "            labels: list of labels\n",
    "        \"\"\"\n",
    "        self.labels = labels\n",
    "        self.png = png\n",
    "        self.cache = {}\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        if index not in self.cache.keys():\n",
    "            transform = transforms.Compose([\n",
    "                    transforms.Resize(224),\n",
    "                    transforms.CenterCrop(224),\n",
    "                    transforms.ToTensor(),\n",
    "                    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "                ]\n",
    "            )\n",
    "\n",
    "            songimage = Image.open(self.png[index]).convert('RGB')\n",
    "            songimage = transform(songimage)\n",
    "            self.cache[index] = (songimage, torch.tensor(self.labels[index], device=device))\n",
    "            \n",
    "        return self.cache[index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ResNet(\n",
      "  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
      "  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (relu): ReLU(inplace=True)\n",
      "  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
      "  (layer1): Sequential(\n",
      "    (0): Bottleneck(\n",
      "      (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (downsample): Sequential(\n",
      "        (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): Bottleneck(\n",
      "      (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "    )\n",
      "    (2): Bottleneck(\n",
      "      (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "    )\n",
      "  )\n",
      "  (layer2): Sequential(\n",
      "    (0): Bottleneck(\n",
      "      (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (downsample): Sequential(\n",
      "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): Bottleneck(\n",
      "      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "    )\n",
      "    (2): Bottleneck(\n",
      "      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "    )\n",
      "    (3): Bottleneck(\n",
      "      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "    )\n",
      "  )\n",
      "  (layer3): Sequential(\n",
      "    (0): Bottleneck(\n",
      "      (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (downsample): Sequential(\n",
      "        (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "        (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): Bottleneck(\n",
      "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "    )\n",
      "    (2): Bottleneck(\n",
      "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "    )\n",
      "    (3): Bottleneck(\n",
      "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "    )\n",
      "    (4): Bottleneck(\n",
      "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "    )\n",
      "    (5): Bottleneck(\n",
      "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "    )\n",
      "  )\n",
      "  (layer4): Sequential(\n",
      "    (0): Bottleneck(\n",
      "      (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (downsample): Sequential(\n",
      "        (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "        (1): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): Bottleneck(\n",
      "      (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "    )\n",
      "    (2): Bottleneck(\n",
      "      (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "    )\n",
      "  )\n",
      "  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "  (fc): Linear(in_features=2048, out_features=8, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "model_conv = torchvision.models.resnet50(pretrained=True)\n",
    "for param in model_conv.parameters():\n",
    "    param.requires_grad = False\n",
    "# don't freeze model\n",
    "    \n",
    "num_ftrs = model_conv.fc.in_features\n",
    "model_conv.fc = nn.Linear(num_ftrs, len(emotions))\n",
    "# model_conv = nn.Sequential(model_conv.fc, nn.Linear(1024, len(emotions)))\n",
    "model_conv = model_conv.to(device)\n",
    "            \n",
    "model = model_conv\n",
    "model.to(device)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train set size: 180\n",
      "Test set size: 45\n",
      "Val set size: 400\n"
     ]
    }
   ],
   "source": [
    "train_set = SongEmotionDataset(train_img, train_emotion)\n",
    "test_set = SongEmotionDataset(test_img, test_emotion)\n",
    "val_set = SongEmotionDataset(val_img, val_emotion)\n",
    "print(\"Train set size: \" + str(len(train_set)))\n",
    "print(\"Test set size: \" + str(len(test_set)))\n",
    "print(\"Val set size: \" + str(len(val_set)))\n",
    "\n",
    "kwargs = {'num_workers': 2, 'pin_memory': True} if device == 'cuda' else {} #needed for using datasets on gpu\n",
    "train_loader = torch.utils.data.DataLoader(train_set, batch_size = 16, shuffle = True, **kwargs)\n",
    "test_loader = torch.utils.data.DataLoader(test_set, batch_size = 16, shuffle = True, **kwargs)\n",
    "val_loader = torch.utils.data.DataLoader(val_set, batch_size = 16, shuffle = True, **kwargs)\n",
    "\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr = 0.001)\n",
    "# scheduler = optim.lr_scheduler.StepLR(optimizer, step_size = 20, gamma = 0.1)\n",
    "scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, epoch):\n",
    "    model.train()\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        optimizer.zero_grad()\n",
    "        data = data.to(device)\n",
    "        target = target.to(device)\n",
    "        \n",
    "        output = model(data)\n",
    "#         output = output.requires_grad_() #set requires_grad to True for training\n",
    "        loss = F.cross_entropy(output, target)\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "#         print(\"target: \" + str(target))\n",
    "#         print(\"output: \" + str(output))\n",
    "#         print(\"\\n\")\n",
    "        \n",
    "        if batch_idx % log_interval == 0: #print training stats\n",
    "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                epoch, batch_idx * len(data), len(train_loader.dataset),\n",
    "                100. * batch_idx / len(train_loader), loss))\n",
    "    scheduler.step(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(model, epoch):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    for data, target in test_loader:\n",
    "        data = data.to(device)\n",
    "        target = target.to(device)\n",
    "        output = model(data)\n",
    "        \n",
    "#         print(output)\n",
    "#         output = output.permute(1, 0, 2)\n",
    "        pred = output.max(1)[1] # get the index of the max log-probability \n",
    "        correct += pred.eq(target).sum()\n",
    "#         correct += pred.eq(target.max(1)[1]).cpu().sum().item()\n",
    "    print('\\nTest set: Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
    "        correct, len(test_loader.dataset),\n",
    "        100. * correct / len(test_loader.dataset)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training epoch 1\n",
      "Train Epoch: 1 [0/180 (0%)]\tLoss: 2.132748\n",
      "Train Epoch: 1 [80/180 (42%)]\tLoss: 1.945154\n",
      "Train Epoch: 1 [160/180 (83%)]\tLoss: 2.211276\n",
      "\n",
      "Test set: Accuracy: 6/45 (13%)\n",
      "\n",
      "training epoch 2\n",
      "Train Epoch: 2 [0/180 (0%)]\tLoss: 2.315745\n",
      "Train Epoch: 2 [80/180 (42%)]\tLoss: 2.064468\n",
      "Train Epoch: 2 [160/180 (83%)]\tLoss: 2.451052\n",
      "\n",
      "Test set: Accuracy: 7/45 (16%)\n",
      "\n",
      "training epoch 3\n",
      "Train Epoch: 3 [0/180 (0%)]\tLoss: 1.825146\n",
      "Train Epoch: 3 [80/180 (42%)]\tLoss: 2.006152\n",
      "Train Epoch: 3 [160/180 (83%)]\tLoss: 1.770290\n",
      "\n",
      "Test set: Accuracy: 12/45 (27%)\n",
      "\n",
      "training epoch 4\n",
      "Train Epoch: 4 [0/180 (0%)]\tLoss: 1.721858\n",
      "Train Epoch: 4 [80/180 (42%)]\tLoss: 1.723758\n",
      "Train Epoch: 4 [160/180 (83%)]\tLoss: 1.803045\n",
      "\n",
      "Test set: Accuracy: 7/45 (16%)\n",
      "\n",
      "training epoch 5\n",
      "Train Epoch: 5 [0/180 (0%)]\tLoss: 1.884384\n",
      "Train Epoch: 5 [80/180 (42%)]\tLoss: 1.817407\n",
      "Train Epoch: 5 [160/180 (83%)]\tLoss: 1.792194\n",
      "\n",
      "Test set: Accuracy: 12/45 (27%)\n",
      "\n",
      "training epoch 6\n",
      "Train Epoch: 6 [0/180 (0%)]\tLoss: 1.809959\n",
      "Train Epoch: 6 [80/180 (42%)]\tLoss: 1.427126\n",
      "Train Epoch: 6 [160/180 (83%)]\tLoss: 1.584461\n",
      "\n",
      "Test set: Accuracy: 14/45 (31%)\n",
      "\n",
      "training epoch 7\n",
      "Train Epoch: 7 [0/180 (0%)]\tLoss: 1.542869\n",
      "Train Epoch: 7 [80/180 (42%)]\tLoss: 1.530808\n",
      "Train Epoch: 7 [160/180 (83%)]\tLoss: 1.613023\n",
      "\n",
      "Test set: Accuracy: 13/45 (29%)\n",
      "\n",
      "training epoch 8\n",
      "Train Epoch: 8 [0/180 (0%)]\tLoss: 1.580961\n",
      "Train Epoch: 8 [80/180 (42%)]\tLoss: 1.533513\n",
      "Train Epoch: 8 [160/180 (83%)]\tLoss: 1.375815\n",
      "\n",
      "Test set: Accuracy: 11/45 (24%)\n",
      "\n",
      "training epoch 9\n",
      "Train Epoch: 9 [0/180 (0%)]\tLoss: 1.457126\n",
      "Train Epoch: 9 [80/180 (42%)]\tLoss: 1.712215\n",
      "Train Epoch: 9 [160/180 (83%)]\tLoss: 1.617573\n",
      "\n",
      "Test set: Accuracy: 16/45 (36%)\n",
      "\n",
      "training epoch 10\n",
      "Train Epoch: 10 [0/180 (0%)]\tLoss: 1.447118\n",
      "Train Epoch: 10 [80/180 (42%)]\tLoss: 1.540400\n",
      "Train Epoch: 10 [160/180 (83%)]\tLoss: 1.333581\n",
      "\n",
      "Test set: Accuracy: 11/45 (24%)\n",
      "\n",
      "training epoch 11\n",
      "Train Epoch: 11 [0/180 (0%)]\tLoss: 1.296368\n",
      "Train Epoch: 11 [80/180 (42%)]\tLoss: 1.384373\n",
      "Train Epoch: 11 [160/180 (83%)]\tLoss: 1.611922\n",
      "\n",
      "Test set: Accuracy: 13/45 (29%)\n",
      "\n",
      "training epoch 12\n",
      "Train Epoch: 12 [0/180 (0%)]\tLoss: 1.265635\n",
      "Train Epoch: 12 [80/180 (42%)]\tLoss: 1.398942\n",
      "Train Epoch: 12 [160/180 (83%)]\tLoss: 1.231946\n",
      "\n",
      "Test set: Accuracy: 9/45 (20%)\n",
      "\n",
      "training epoch 13\n",
      "Train Epoch: 13 [0/180 (0%)]\tLoss: 1.340945\n",
      "Train Epoch: 13 [80/180 (42%)]\tLoss: 1.727078\n",
      "Train Epoch: 13 [160/180 (83%)]\tLoss: 1.050742\n",
      "\n",
      "Test set: Accuracy: 17/45 (38%)\n",
      "\n",
      "training epoch 14\n",
      "Train Epoch: 14 [0/180 (0%)]\tLoss: 1.100031\n",
      "Train Epoch: 14 [80/180 (42%)]\tLoss: 1.477339\n",
      "Train Epoch: 14 [160/180 (83%)]\tLoss: 1.115135\n",
      "\n",
      "Test set: Accuracy: 14/45 (31%)\n",
      "\n",
      "training epoch 15\n",
      "Train Epoch: 15 [0/180 (0%)]\tLoss: 1.195910\n",
      "Train Epoch: 15 [80/180 (42%)]\tLoss: 1.255284\n",
      "Train Epoch: 15 [160/180 (83%)]\tLoss: 1.208489\n",
      "\n",
      "Test set: Accuracy: 9/45 (20%)\n",
      "\n",
      "training epoch 16\n",
      "Train Epoch: 16 [0/180 (0%)]\tLoss: 0.835563\n",
      "Train Epoch: 16 [80/180 (42%)]\tLoss: 1.008898\n",
      "Train Epoch: 16 [160/180 (83%)]\tLoss: 1.241969\n",
      "\n",
      "Test set: Accuracy: 15/45 (33%)\n",
      "\n",
      "training epoch 17\n",
      "Train Epoch: 17 [0/180 (0%)]\tLoss: 1.105743\n",
      "Train Epoch: 17 [80/180 (42%)]\tLoss: 1.067092\n",
      "Train Epoch: 17 [160/180 (83%)]\tLoss: 1.187074\n",
      "\n",
      "Test set: Accuracy: 13/45 (29%)\n",
      "\n",
      "training epoch 18\n",
      "Train Epoch: 18 [0/180 (0%)]\tLoss: 1.049999\n",
      "Train Epoch: 18 [80/180 (42%)]\tLoss: 1.324965\n",
      "Train Epoch: 18 [160/180 (83%)]\tLoss: 0.810830\n",
      "\n",
      "Test set: Accuracy: 14/45 (31%)\n",
      "\n",
      "training epoch 19\n",
      "Train Epoch: 19 [0/180 (0%)]\tLoss: 0.961487\n",
      "Train Epoch: 19 [80/180 (42%)]\tLoss: 1.234051\n",
      "Train Epoch: 19 [160/180 (83%)]\tLoss: 0.810153\n",
      "\n",
      "Test set: Accuracy: 16/45 (36%)\n",
      "\n",
      "training epoch 20\n",
      "Train Epoch: 20 [0/180 (0%)]\tLoss: 1.027304\n",
      "Train Epoch: 20 [80/180 (42%)]\tLoss: 0.937033\n",
      "Train Epoch: 20 [160/180 (83%)]\tLoss: 1.448575\n",
      "\n",
      "Test set: Accuracy: 14/45 (31%)\n",
      "\n",
      "training epoch 21\n",
      "Train Epoch: 21 [0/180 (0%)]\tLoss: 0.929430\n",
      "Train Epoch: 21 [80/180 (42%)]\tLoss: 1.027526\n",
      "Train Epoch: 21 [160/180 (83%)]\tLoss: 0.936655\n",
      "\n",
      "Test set: Accuracy: 14/45 (31%)\n",
      "\n",
      "training epoch 22\n",
      "Train Epoch: 22 [0/180 (0%)]\tLoss: 0.946381\n",
      "Train Epoch: 22 [80/180 (42%)]\tLoss: 1.012782\n",
      "Train Epoch: 22 [160/180 (83%)]\tLoss: 1.020662\n",
      "\n",
      "Test set: Accuracy: 11/45 (24%)\n",
      "\n",
      "training epoch 23\n",
      "Train Epoch: 23 [0/180 (0%)]\tLoss: 1.069769\n",
      "Train Epoch: 23 [80/180 (42%)]\tLoss: 1.291922\n",
      "Train Epoch: 23 [160/180 (83%)]\tLoss: 1.224366\n",
      "\n",
      "Test set: Accuracy: 13/45 (29%)\n",
      "\n",
      "training epoch 24\n",
      "Train Epoch: 24 [0/180 (0%)]\tLoss: 1.337822\n",
      "Train Epoch: 24 [80/180 (42%)]\tLoss: 0.772598\n",
      "Train Epoch: 24 [160/180 (83%)]\tLoss: 0.794960\n",
      "\n",
      "Test set: Accuracy: 13/45 (29%)\n",
      "\n",
      "training epoch 25\n",
      "Train Epoch: 25 [0/180 (0%)]\tLoss: 1.146244\n",
      "Train Epoch: 25 [80/180 (42%)]\tLoss: 1.053144\n",
      "Train Epoch: 25 [160/180 (83%)]\tLoss: 0.834883\n",
      "\n",
      "Test set: Accuracy: 14/45 (31%)\n",
      "\n",
      "training epoch 26\n",
      "Train Epoch: 26 [0/180 (0%)]\tLoss: 0.772502\n",
      "Train Epoch: 26 [80/180 (42%)]\tLoss: 1.139472\n",
      "Train Epoch: 26 [160/180 (83%)]\tLoss: 1.163956\n",
      "\n",
      "Test set: Accuracy: 16/45 (36%)\n",
      "\n",
      "training epoch 27\n",
      "Train Epoch: 27 [0/180 (0%)]\tLoss: 0.908678\n",
      "Train Epoch: 27 [80/180 (42%)]\tLoss: 1.036274\n",
      "Train Epoch: 27 [160/180 (83%)]\tLoss: 0.822668\n",
      "\n",
      "Test set: Accuracy: 11/45 (24%)\n",
      "\n",
      "training epoch 28\n",
      "Train Epoch: 28 [0/180 (0%)]\tLoss: 0.892396\n",
      "Train Epoch: 28 [80/180 (42%)]\tLoss: 1.347441\n",
      "Train Epoch: 28 [160/180 (83%)]\tLoss: 0.806452\n",
      "\n",
      "Test set: Accuracy: 17/45 (38%)\n",
      "\n",
      "training epoch 29\n",
      "Train Epoch: 29 [0/180 (0%)]\tLoss: 0.763482\n",
      "Train Epoch: 29 [80/180 (42%)]\tLoss: 1.375501\n",
      "Train Epoch: 29 [160/180 (83%)]\tLoss: 0.675208\n",
      "\n",
      "Test set: Accuracy: 14/45 (31%)\n",
      "\n",
      "training epoch 30\n",
      "Train Epoch: 30 [0/180 (0%)]\tLoss: 0.763980\n",
      "Train Epoch: 30 [80/180 (42%)]\tLoss: 0.950610\n",
      "Train Epoch: 30 [160/180 (83%)]\tLoss: 1.043105\n",
      "\n",
      "Test set: Accuracy: 13/45 (29%)\n",
      "\n",
      "training epoch 31\n",
      "Train Epoch: 31 [0/180 (0%)]\tLoss: 0.702673\n",
      "Train Epoch: 31 [80/180 (42%)]\tLoss: 0.934112\n",
      "Train Epoch: 31 [160/180 (83%)]\tLoss: 0.957196\n",
      "\n",
      "Test set: Accuracy: 13/45 (29%)\n",
      "\n",
      "training epoch 32\n",
      "Train Epoch: 32 [0/180 (0%)]\tLoss: 0.792035\n",
      "Train Epoch: 32 [80/180 (42%)]\tLoss: 1.309704\n",
      "Train Epoch: 32 [160/180 (83%)]\tLoss: 0.584768\n",
      "\n",
      "Test set: Accuracy: 12/45 (27%)\n",
      "\n",
      "training epoch 33\n",
      "Train Epoch: 33 [0/180 (0%)]\tLoss: 0.607687\n",
      "Train Epoch: 33 [80/180 (42%)]\tLoss: 1.337248\n",
      "Train Epoch: 33 [160/180 (83%)]\tLoss: 0.609345\n",
      "\n",
      "Test set: Accuracy: 15/45 (33%)\n",
      "\n",
      "training epoch 34\n",
      "Train Epoch: 34 [0/180 (0%)]\tLoss: 0.914007\n",
      "Train Epoch: 34 [80/180 (42%)]\tLoss: 0.917856\n",
      "Train Epoch: 34 [160/180 (83%)]\tLoss: 0.858398\n",
      "\n",
      "Test set: Accuracy: 13/45 (29%)\n",
      "\n",
      "training epoch 35\n",
      "Train Epoch: 35 [0/180 (0%)]\tLoss: 0.764914\n",
      "Train Epoch: 35 [80/180 (42%)]\tLoss: 1.443230\n",
      "Train Epoch: 35 [160/180 (83%)]\tLoss: 0.838348\n",
      "\n",
      "Test set: Accuracy: 12/45 (27%)\n",
      "\n",
      "training epoch 36\n",
      "Train Epoch: 36 [0/180 (0%)]\tLoss: 0.749543\n",
      "Train Epoch: 36 [80/180 (42%)]\tLoss: 0.574971\n",
      "Train Epoch: 36 [160/180 (83%)]\tLoss: 0.776913\n",
      "\n",
      "Test set: Accuracy: 13/45 (29%)\n",
      "\n",
      "training epoch 37\n",
      "Train Epoch: 37 [0/180 (0%)]\tLoss: 0.630172\n",
      "Train Epoch: 37 [80/180 (42%)]\tLoss: 0.923264\n",
      "Train Epoch: 37 [160/180 (83%)]\tLoss: 0.442682\n",
      "\n",
      "Test set: Accuracy: 10/45 (22%)\n",
      "\n",
      "training epoch 38\n",
      "Train Epoch: 38 [0/180 (0%)]\tLoss: 0.583181\n",
      "Train Epoch: 38 [80/180 (42%)]\tLoss: 0.632315\n",
      "Train Epoch: 38 [160/180 (83%)]\tLoss: 0.586692\n",
      "\n",
      "Test set: Accuracy: 14/45 (31%)\n",
      "\n",
      "training epoch 39\n",
      "Train Epoch: 39 [0/180 (0%)]\tLoss: 0.665035\n",
      "Train Epoch: 39 [80/180 (42%)]\tLoss: 0.855586\n",
      "Train Epoch: 39 [160/180 (83%)]\tLoss: 0.638157\n",
      "\n",
      "Test set: Accuracy: 14/45 (31%)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "\n",
    "log_interval = 5\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "for epoch in range(1, 40):\n",
    "    print(\"training epoch \" + str(epoch))\n",
    "    train(model, epoch)\n",
    "    test(model, epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def val(model):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    for data, target in val_loader:\n",
    "        data = data.to(device)\n",
    "        target = target.to(device)\n",
    "        output = model(data)\n",
    "        \n",
    "#         print(output)\n",
    "#         output = output.permute(1, 0, 2)\n",
    "        pred = output.max(1)[1] # get the index of the max log-probability \n",
    "        correct += pred.eq(target).sum()\n",
    "#         correct += pred.eq(target.max(1)[1]).cpu().sum().item()\n",
    "    print('\\nVal set: Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
    "        correct, len(val_loader.dataset),\n",
    "        100. * correct / len(val_loader.dataset)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Val set: Accuracy: 197/400 (49%)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "val(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.fc, './song_mfcc_model_fc.pth')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.7444, -0.4518, -1.5000,  2.4692,  5.8717, -1.9363, -2.6768, -4.4650]],\n",
      "       device='cuda:0', grad_fn=<AddmmBackward>)\n"
     ]
    }
   ],
   "source": [
    "transform = transforms.Compose([\n",
    "        transforms.Resize(224),\n",
    "        transforms.CenterCrop(224),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "    ]\n",
    ")\n",
    "\n",
    "def image_loader(image_name):\n",
    "    image = Image.open(\"./SongEmotionDatasetImages/\" + image_name).convert(\"RGB\")\n",
    "    image = transform(image).to(device)\n",
    "    return torch.tensor(image, device=device).unsqueeze(0)\n",
    "    \n",
    "img = image_loader(\"228.png\")\n",
    "model.eval()\n",
    "print(model(img))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NOTES\n",
    "\n",
    "below is the mfccs notes / random code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([3, 5, 5, 5, 7, 0, 3, 0])\n",
      "tensor([7, 7, 7, 7, 7, 7, 7, 7])\n",
      "tensor(1)\n",
      "tensor([3, 6, 6, 0, 7, 0, 5, 3])\n",
      "tensor([7, 7, 7, 7, 7, 7, 7, 7])\n",
      "tensor(2)\n",
      "tensor([2, 2, 5, 4, 6, 1, 2, 1])\n",
      "tensor([7, 7, 7, 7, 7, 7, 7, 7])\n",
      "tensor(2)\n",
      "tensor([4, 2, 4, 0, 0, 7, 4, 6])\n",
      "tensor([7, 7, 7, 7, 7, 7, 7, 7])\n",
      "tensor(3)\n",
      "tensor([4, 2, 0, 5, 1, 3, 5, 4])\n",
      "tensor([7, 7, 7, 7, 7, 7, 7, 7])\n",
      "tensor(3)\n",
      "tensor([6, 0, 2, 2, 4])\n",
      "tensor([7, 7, 7, 7, 7])\n",
      "tensor(3)\n",
      "\n",
      "Test set: Accuracy: 3/45 (7%)\n",
      "\n",
      "tensor([3, 5, 1, 0, 1, 6, 3, 4])\n",
      "tensor([7, 7, 7, 7, 7, 7, 7, 7])\n",
      "tensor(0)\n",
      "tensor([5, 3, 4, 2, 6, 4, 0, 6])\n",
      "tensor([7, 7, 7, 7, 7, 7, 7, 7])\n",
      "tensor(0)\n",
      "tensor([0, 2, 0, 3, 5, 7, 3, 5])\n",
      "tensor([7, 7, 7, 7, 7, 7, 7, 7])\n",
      "tensor(1)\n",
      "tensor([2, 4, 0, 1, 7, 6, 5, 4])\n",
      "tensor([7, 7, 7, 7, 7, 7, 7, 7])\n",
      "tensor(2)\n",
      "tensor([4, 0, 2, 5, 2, 2, 0, 5])\n",
      "tensor([7, 7, 7, 7, 7, 7, 7, 7])\n",
      "tensor(2)\n",
      "tensor([2, 0, 7, 6, 4])\n",
      "tensor([7, 7, 7, 7, 7])\n",
      "tensor(3)\n",
      "\n",
      "Test set: Accuracy: 3/45 (7%)\n",
      "\n",
      "tensor([0, 3, 2, 3, 0, 6, 1, 4])\n",
      "tensor([7, 7, 7, 7, 7, 7, 7, 7])\n",
      "tensor(0)\n",
      "tensor([1, 3, 0, 5, 6, 0, 4, 5])\n",
      "tensor([7, 7, 7, 7, 7, 7, 7, 7])\n",
      "tensor(0)\n",
      "tensor([2, 4, 2, 2, 4, 4, 4, 2])\n",
      "tensor([7, 7, 7, 7, 7, 7, 7, 7])\n",
      "tensor(0)\n",
      "tensor([4, 5, 5, 1, 3, 7, 5, 0])\n",
      "tensor([7, 7, 7, 7, 7, 7, 7, 7])\n",
      "tensor(1)\n",
      "tensor([7, 0, 6, 0, 7, 0, 6, 2])\n",
      "tensor([7, 7, 7, 7, 7, 7, 7, 7])\n",
      "tensor(3)\n",
      "tensor([2, 5, 3, 6, 5])\n",
      "tensor([7, 7, 7, 7, 7])\n",
      "tensor(3)\n",
      "\n",
      "Test set: Accuracy: 3/45 (7%)\n",
      "\n",
      "tensor([4, 6, 3, 1, 0, 5, 5, 5])\n",
      "tensor([7, 7, 7, 7, 7, 7, 7, 7])\n",
      "tensor(0)\n",
      "tensor([7, 4, 2, 6, 2, 4, 3, 5])\n",
      "tensor([7, 7, 7, 7, 7, 7, 7, 7])\n",
      "tensor(1)\n",
      "tensor([3, 4, 2, 5, 3, 1, 3, 0])\n",
      "tensor([7, 7, 7, 7, 7, 7, 7, 7])\n",
      "tensor(1)\n",
      "tensor([4, 5, 7, 0, 0, 7, 4, 0])\n",
      "tensor([7, 7, 7, 7, 7, 7, 7, 7])\n",
      "tensor(3)\n",
      "tensor([2, 1, 0, 5, 6, 2, 2, 0])\n",
      "tensor([7, 7, 7, 7, 7, 7, 7, 7])\n",
      "tensor(3)\n",
      "tensor([6, 4, 0, 6, 2])\n",
      "tensor([7, 7, 7, 7, 7])\n",
      "tensor(3)\n",
      "\n",
      "Test set: Accuracy: 3/45 (7%)\n",
      "\n",
      "tensor([3, 1, 6, 4, 0, 2, 0, 0])\n",
      "tensor([7, 7, 7, 7, 7, 7, 7, 7])\n",
      "tensor(0)\n",
      "tensor([7, 6, 0, 7, 3, 5, 4, 2])\n",
      "tensor([7, 7, 7, 7, 7, 7, 7, 7])\n",
      "tensor(2)\n",
      "tensor([3, 5, 0, 4, 2, 2, 6, 7])\n",
      "tensor([7, 7, 7, 7, 7, 7, 7, 7])\n",
      "tensor(3)\n",
      "tensor([6, 3, 6, 5, 4, 3, 5, 5])\n",
      "tensor([7, 7, 7, 7, 7, 7, 7, 7])\n",
      "tensor(3)\n",
      "tensor([1, 0, 2, 2, 4, 5, 0, 5])\n",
      "tensor([7, 7, 7, 7, 7, 7, 7, 7])\n",
      "tensor(3)\n",
      "tensor([0, 4, 1, 4, 2])\n",
      "tensor([7, 7, 7, 7, 7])\n",
      "tensor(3)\n",
      "\n",
      "Test set: Accuracy: 3/45 (7%)\n",
      "\n",
      "tensor([1, 6, 0, 2, 2, 5, 2, 0])\n",
      "tensor([7, 7, 7, 7, 7, 7, 7, 7])\n",
      "tensor(0)\n",
      "tensor([0, 2, 4, 0, 2, 7, 3, 3])\n",
      "tensor([7, 7, 7, 7, 7, 7, 7, 7])\n",
      "tensor(1)\n",
      "tensor([2, 4, 5, 2, 5, 6, 6, 1])\n",
      "tensor([7, 7, 7, 7, 7, 7, 7, 7])\n",
      "tensor(1)\n",
      "tensor([6, 3, 0, 6, 7, 3, 4, 0])\n",
      "tensor([7, 7, 7, 7, 7, 7, 7, 7])\n",
      "tensor(2)\n",
      "tensor([4, 1, 0, 3, 4, 5, 5, 4])\n",
      "tensor([7, 7, 7, 7, 7, 7, 7, 7])\n",
      "tensor(2)\n",
      "tensor([5, 4, 0, 5, 7])\n",
      "tensor([7, 7, 7, 7, 7])\n",
      "tensor(3)\n",
      "\n",
      "Test set: Accuracy: 3/45 (7%)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i in range(6):\n",
    "    test(model, i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print model's state_dict\n",
    "print(\"Model's state_dict:\")\n",
    "for param_tensor in model.state_dict():\n",
    "    print(param_tensor, \"\\t\", model.state_dict()[param_tensor].size())\n",
    "# Print optimizer's state_dict\n",
    "print(\"Optimizer's state_dict:\")\n",
    "for var_name in optimizer.state_dict():\n",
    "    print(var_name, \"\\t\", optimizer.state_dict()[var_name])\n",
    "torch.save(model.state_dict(), 'dataset_model_soundemotion.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/czf/memories/venv/lib/python3.6/site-packages/librosa/core/audio.py:161: UserWarning: PySoundFile failed. Trying audioread instead.\n",
      "  warnings.warn('PySoundFile failed. Trying audioread instead.')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(40, 2586) (1323648,)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[-5.30341797e+02, -4.07741577e+02, -3.27536621e+02, ...,\n",
       "        -2.39811523e+02, -1.96744080e+02, -1.44711777e+02],\n",
       "       [ 5.81265569e-01,  1.03006027e+02,  1.29354553e+02, ...,\n",
       "         1.48707626e+02,  1.45873001e+02,  1.28202530e+02],\n",
       "       [ 4.58764762e-01,  7.53921986e+00, -1.18814125e+01, ...,\n",
       "        -2.51551704e+01, -1.92207527e+01, -1.79366188e+01],\n",
       "       ...,\n",
       "       [ 3.11299562e-01, -1.29907084e+00,  1.18818974e+00, ...,\n",
       "        -6.58579540e+00, -3.34302998e+00, -4.75482178e+00],\n",
       "       [ 2.23848164e-01, -3.19489312e+00, -2.78556681e+00, ...,\n",
       "        -1.36089420e+01, -6.40699673e+00, -5.27228928e+00],\n",
       "       [ 8.67742151e-02,  1.31472754e+00, -1.41885233e+00, ...,\n",
       "         3.34440261e-01,  1.14392626e+00, -3.62402201e-02]], dtype=float32)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "audio, sample_rate = librosa.load(\"SongEmotionDataset/1.mp3\", res_type='kaiser_fast')\n",
    "# [print(x) for x in audio]\n",
    "\n",
    "#convert audio into 2d array\n",
    "mfccs = librosa.feature.mfcc(y=audio, sr=sample_rate, n_mfcc=40)\n",
    "# mfccsscaled = np.mean(mfccs.T,axis=0)\n",
    "print(mfccs.shape, audio.shape)\n",
    "mfccs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# audio_tensor = torch.tensor(audio)\n",
    "# audio_tensor\n",
    "# audio_tensor.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for sound_file in data_path.iterdir():\n",
    "#     if \".mp3\" in str(sound_file):\n",
    "#         print(sound_file)\n",
    "#         audio, sample_rate = librosa.load(str(sound_file), res_type='kaiser_fast')\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_percentage = 0.8\n",
    "\n",
    "# min_train = min_total*train_percentage\n",
    "# min_test = min_total - min_train\n",
    "\n",
    "# train_totals = torch.zeros(len(emotions))\n",
    "\n",
    "# while \n",
    "\n",
    "\n",
    "# for i in range(1, 401):\n",
    "#     count_total = sheet.cell_value(i, 7)\n",
    "    \n",
    "#     emotions_counter = [0 for e in emotions]\n",
    "#     if i % 5 == 0:\n",
    "#         test_song.append(dataset.joinpath(\"{}.mp3\".format(i)))\n",
    "#         emotion_arr = []\n",
    "#         for j in range(5):\n",
    "#             emotion_arr.append(sheet.cell_value(i, 2 + j))\n",
    "#         test_emotion.append(torch.tensor(emotion_arr, device=device).float())\n",
    "        \n",
    "#     emotions_counter = [0 for e in emotions]\n",
    "#     else:\n",
    "#         train_song.append(dataset.joinpath(\"{}.mp3\".format(i)))\n",
    "#         emotion_arr = []\n",
    "#         for j in range(5):\n",
    "#             emotion_arr.append(sheet.cell_value(i, 2 + j))\n",
    "#         train_emotion.append(torch.tensor(emotion_arr, device=device))\n",
    "\n",
    "# print(len(train_song), len(test_song))\n",
    "# print(len(train_emotion), len(test_emotion))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def train(model, epoch):\n",
    "#     model.train()\n",
    "#     for batch_idx, (data, target) in enumerate(train_loader):\n",
    "#         print(data)\n",
    "#         optimizer.zero_grad()\n",
    "#         data.unsqueeze_(1)\n",
    "#         data = data.requires_grad_() #set requires_grad to True for training\n",
    "#         output = model(data)\n",
    "# #         output = output.view(-1, len(emotions))\n",
    "# #         print(output.shape, target.shape)\n",
    "# #         print(output, target)\n",
    "# #         loss = F.kl_div(output, target)\n",
    "#         kl = nn.KLDivLoss()\n",
    "#         loss = kl(output, target)\n",
    "# #         loss = F.cross_entropy(output, target)\n",
    "# #         loss = nn.CrossEntropyLoss(output, target)\n",
    "# #         loss = F.nll_loss(output, target)\n",
    "#         loss.backward()\n",
    "#         print(loss)\n",
    "#         optimizer.step()\n",
    "#         print(output)\n",
    "#         print(target)\n",
    "#         print(\"\\n\")\n",
    "# #         scheduler.step()\n",
    "#         if batch_idx % log_interval == 0: #print training stats\n",
    "#             print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "#                 epoch, batch_idx * len(data), len(train_loader.dataset),\n",
    "#                 100. * batch_idx / len(train_loader), loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 11.4343, 157.9053,  55.5535, 120.1634,  54.9434], device='cuda:0')\n",
      "tensor([3.1017, 9.4848, 9.0917, 8.7741, 8.5477], device='cuda:0')\n",
      "[1, 9, 11, 10, 8]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/armaan.goel/memories/myenv/lib/python3.6/site-packages/ipykernel_launcher.py:22: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "/home/armaan.goel/memories/myenv/lib/python3.6/site-packages/ipykernel_launcher.py:34: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    }
   ],
   "source": [
    "# train_percentage = 0.8\n",
    "# allowed_exceedance = 0\n",
    "\n",
    "# train_song = []\n",
    "# test_song = []\n",
    "# train_emotion = []\n",
    "# test_emotion = []\n",
    "\n",
    "# row_indexes = np.arange(1,401)\n",
    "# np.random.shuffle(row_indexes)\n",
    "\n",
    "# # train_indexes = [row_indexes[i] for i in range(len(row_indexes)) if i < len(row_indexes)*train_percentage]\n",
    "# # test_indexes = [row_indexes[i] for i in range(len(row_indexes)) if i >= len(row_indexes)*train_percentage]\n",
    "\n",
    "# def get_data(indexes):\n",
    "#     song = []\n",
    "#     emotion = []\n",
    "    \n",
    "#     totals = torch.zeros(len(emotions), device=device).float()\n",
    "#     for x in indexes:    \n",
    "#         row = torch.tensor([sheet.cell_value(x, 2 + j) for j in range(5)], device=device).float()\n",
    "#         totals += F.softmax(row)\n",
    "\n",
    "#     min_total = torch.min(totals)\n",
    "#     print(totals)\n",
    "    \n",
    "#     totals = torch.zeros(len(emotions), device=device).float()\n",
    "#     for x in indexes:\n",
    "#         row = torch.tensor([sheet.cell_value(x, 2 + j) for j in range(5)], device=device).float()\n",
    "        \n",
    "#         if torch.max(totals + row) < min_total*(1 + allowed_exceedance):\n",
    "#             song.append(dataset.joinpath(\"{}.mp3\".format(x)))\n",
    "#             emotion.append(row)\n",
    "#             totals += F.softmax(row)\n",
    "            \n",
    "#     print(totals)\n",
    "#     return song, emotion\n",
    "    \n",
    "# song, emotion = get_data(row_indexes)\n",
    "# # test_song, test_emotion = get_data(test_indexes)\n",
    "\n",
    "# train_song = [song[i] for i in range(len(song)) if i < len(song)*train_percentage]\n",
    "# test_song = [song[i] for i in range(len(song)) if i >= len(song)*train_percentage]\n",
    "\n",
    "# train_emotion = [emotion[i] for i in range(len(emotion)) if i < len(emotion)*train_percentage]\n",
    "# test_emotion = [emotion[i] for i in range(len(emotion)) if i >= len(emotion)*train_percentage]\n",
    "\n",
    "# num_maxes = [0 for _ in emotions]\n",
    "\n",
    "# for row in emotion:\n",
    "#     i = torch.argmax(row)\n",
    "#     num_maxes[i] += 1\n",
    "    \n",
    "# print(num_maxes)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
